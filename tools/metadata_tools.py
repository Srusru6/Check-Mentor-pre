"""
å…ƒæ•°æ®å·¥å…·è„šæœ¬

æä¾›å‘½ä»¤è¡Œå·¥å…·ç”¨äºï¼š
1. éªŒè¯å…ƒæ•°æ®æ–‡ä»¶çš„æ ¼å¼
2. ç”Ÿæˆå…ƒæ•°æ®æ¨¡æ¿
3. æŸ¥çœ‹å…ƒæ•°æ®ç»Ÿè®¡
4. æµ‹è¯•å…ƒæ•°æ®åŠŸèƒ½
"""
import json
import argparse
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# Ensure core modules can be imported when running as a script
sys.path.append(str(Path(__file__).resolve().parents[1]))

from core.metadata_manager import MetadataManager, PaperMetadata


def validate_metadata_file(file_path: Path) -> tuple[bool, list[str]]:
    """
    éªŒè¯å…ƒæ•°æ®æ–‡ä»¶çš„æ ¼å¼
    
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. æ–°æ ¼å¼ï¼ˆitemsæ•°ç»„ï¼‰ï¼š{"items": [{title, doi, authors, published: {year, month}}]}
    2. æ—§æ ¼å¼ï¼ˆæ–‡ä»¶åæ˜ å°„ï¼‰ï¼š{"filename.md": {doi, authors, published: {year, month}}}
    
    Returns:
        (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯åˆ—è¡¨)
    """
    errors = []
    
    if not file_path.exists():
        errors.append(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False, errors
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        errors.append(f"JSONæ ¼å¼é”™è¯¯: {e}")
        return False, errors
    except Exception as e:
        errors.append(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return False, errors
    
    if not isinstance(data, dict):
        errors.append("æ ¹èŠ‚ç‚¹å¿…é¡»æ˜¯å­—å…¸")
        return False, errors
    
    # æ£€æµ‹æ ¼å¼ç±»å‹
    if "items" in data:
        # æ–°æ ¼å¼ï¼šitems æ•°ç»„
        items = data.get("items", [])
        if not isinstance(items, list):
            errors.append("items å¿…é¡»æ˜¯æ•°ç»„")
            return False, errors
        
        for i, item in enumerate(items):
            prefix = f"[item {i}]"
            
            if not isinstance(item, dict):
                errors.append(f"{prefix} å¿…é¡»æ˜¯å­—å…¸")
                continue
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ["title", "doi", "authors"]
            for field in required_fields:
                if field not in item:
                    errors.append(f"{prefix} ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
            
            # éªŒè¯å­—æ®µç±»å‹
            if "title" in item and not isinstance(item["title"], str):
                errors.append(f"{prefix} title å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
            
            if "doi" in item and not isinstance(item["doi"], str):
                errors.append(f"{prefix} doi å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
            
            if "authors" in item:
                if not isinstance(item["authors"], list):
                    errors.append(f"{prefix} authors å¿…é¡»æ˜¯åˆ—è¡¨")
                elif not all(isinstance(a, str) for a in item["authors"]):
                    errors.append(f"{prefix} authors åˆ—è¡¨ä¸­çš„æ‰€æœ‰å…ƒç´ å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
            
            # éªŒè¯ published å­—æ®µï¼ˆå¯é€‰ï¼‰
            if "published" in item:
                if not isinstance(item["published"], dict):
                    errors.append(f"{prefix} published å¿…é¡»æ˜¯å­—å…¸")
                else:
                    if "year" in item["published"]:
                        if not isinstance(item["published"]["year"], int):
                            errors.append(f"{prefix} published.year å¿…é¡»æ˜¯æ•´æ•°")
                    if "month" in item["published"]:
                        month = item["published"]["month"]
                        if not isinstance(month, int) or not (1 <= month <= 12):
                            errors.append(f"{prefix} published.month å¿…é¡»æ˜¯1-12ä¹‹é—´çš„æ•´æ•°")
    
    else:
        # æ—§æ ¼å¼ï¼šæ–‡ä»¶åæ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
        for filename, metadata in data.items():
            prefix = f"[{filename}]"
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ["doi", "authors"]
            for field in required_fields:
                if field not in metadata:
                    errors.append(f"{prefix} ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
            
            # éªŒè¯å­—æ®µç±»å‹
            if "doi" in metadata and not isinstance(metadata["doi"], str):
                errors.append(f"{prefix} doi å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
            
            if "authors" in metadata:
                if not isinstance(metadata["authors"], list):
                    errors.append(f"{prefix} authors å¿…é¡»æ˜¯åˆ—è¡¨")
                elif not all(isinstance(a, str) for a in metadata["authors"]):
                    errors.append(f"{prefix} authors åˆ—è¡¨ä¸­çš„æ‰€æœ‰å…ƒç´ å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
            
            # éªŒè¯ published å­—æ®µï¼ˆå¯é€‰ï¼‰
            if "published" in metadata:
                if not isinstance(metadata["published"], dict):
                    errors.append(f"{prefix} published å¿…é¡»æ˜¯å­—å…¸")
                else:
                    if "year" in metadata["published"]:
                        if not isinstance(metadata["published"]["year"], int):
                            errors.append(f"{prefix} published.year å¿…é¡»æ˜¯æ•´æ•°")
                    if "month" in metadata["published"]:
                        month = metadata["published"]["month"]
                        if not isinstance(month, int) or not (1 <= month <= 12):
                            errors.append(f"{prefix} published.month å¿…é¡»æ˜¯1-12ä¹‹é—´çš„æ•´æ•°")
    
    return len(errors) == 0, errors


def generate_template(output_path: Path, paper_directory: Path = None):
    """
    ç”Ÿæˆå…ƒæ•°æ®æ¨¡æ¿
    
    Args:
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        paper_directory: è®ºæ–‡ç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œå°†ä¸ºæ‰€æœ‰ .md æ–‡ä»¶ç”Ÿæˆæ¡ç›®ï¼‰
    """
    if paper_directory and paper_directory.exists():
        # ä¸ºç›®å½•ä¸­çš„æ‰€æœ‰ .md æ–‡ä»¶ç”Ÿæˆæ¨¡æ¿ï¼ˆæ–°æ ¼å¼ï¼‰
        md_files = sorted(paper_directory.glob("*.md"))
        items = []
        for md_file in md_files:
            # å»æ‰ .md æ‰©å±•åä½œä¸º title
            title = md_file.stem
            items.append({
                "title": title,
                "doi": "",
                "authors": [],
                "published": {
                    "year": 2024,
                    "month": 1
                }
            })
        template = {"items": items}
        print(f"âœ“ ä¸º {len(md_files)} ç¯‡è®ºæ–‡ç”Ÿæˆäº†æ¨¡æ¿æ¡ç›®")
    else:
        # ç”Ÿæˆç¤ºä¾‹æ¨¡æ¿ï¼ˆæ–°æ ¼å¼ï¼‰
        template = {
            "items": [
                {
                    "title": "ç¤ºä¾‹è®ºæ–‡æ ‡é¢˜",
                    "doi": "10.xxxx/xxxxx",
                    "authors": ["ä½œè€…1", "ä½œè€…2", "ä½œè€…3"],
                    "published": {
                        "year": 2024,
                        "month": 1
                    }
                }
            ]
        }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(template, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ æ¨¡æ¿å·²ç”Ÿæˆ: {output_path}")


def merge_metadata_files(input_files: list[Path], output_file: Path):
    """
    åˆå¹¶å¤šä¸ªå…ƒæ•°æ®æ–‡ä»¶
    
    Args:
        input_files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    merged_items = []
    seen_dois = set()
    
    print(f"æ­£åœ¨åˆå¹¶ {len(input_files)} ä¸ªæ–‡ä»¶...")
    
    for file_path in input_files:
        if not file_path.exists():
            print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {file_path}")
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            items = []
            if isinstance(data, list):
                items = data
            elif isinstance(data, dict):
                if "items" in data and isinstance(data["items"], list):
                    items = data["items"]
                else:
                    # åˆ¤æ–­æ˜¯"æ—§æ ¼å¼(æ–‡ä»¶å->å…ƒæ•°æ®)"è¿˜æ˜¯"å•æ¡å…ƒæ•°æ®"
                    # å¦‚æœæ‰€æœ‰å€¼éƒ½æ˜¯å­—å…¸ï¼Œåˆ™è®¤ä¸ºæ˜¯æ—§æ ¼å¼æ˜ å°„ï¼›å¦åˆ™è®¤ä¸ºæ˜¯å•æ¡è®°å½•
                    is_map_of_items = True
                    if not data:
                        is_map_of_items = False
                    else:
                        for v in data.values():
                            if not isinstance(v, dict):
                                is_map_of_items = False
                                break
                    
                    if is_map_of_items:
                        # æ—§æ ¼å¼è½¬æ¢
                        for filename, meta in data.items():
                            item = meta.copy()
                            # å¦‚æœæ²¡æœ‰ titleï¼Œå°è¯•ä»æ–‡ä»¶åè·å–
                            if "title" not in item:
                                item["title"] = Path(filename).stem
                            items.append(item)
                    else:
                        # å•æ¡è®°å½•æ ¼å¼
                        item = data.copy()
                        if "title" not in item:
                            item["title"] = file_path.stem
                        items.append(item)
            
            count = 0
            for item in items:
                # ç®€å•çš„å»é‡ç­–ç•¥ï¼šåŸºäº DOI
                doi = item.get("doi")
                if doi:
                    # è§„èŒƒåŒ– DOI (ç®€å•å»é™¤ç©ºç™½)
                    doi = doi.strip()
                    if doi in seen_dois:
                        continue
                    seen_dois.add(doi)
                
                merged_items.append(item)
                count += 1
            
            print(f"  + ä» {file_path.name} æ·»åŠ äº† {count} æ¡è®°å½•")
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    result = {"items": merged_items}
    
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆå¹¶å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³: {output_file}")
        print(f"   æ€»è®°å½•æ•°: {len(merged_items)}")
    except Exception as e:
        print(f"âŒ ä¿å­˜è¾“å‡ºæ–‡ä»¶æ—¶å‡ºé”™: {e}")


def show_statistics(file_path: Path):
    """æ˜¾ç¤ºå…ƒæ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
    manager = MetadataManager()
    
    if manager.load_metadata_file(file_path):
        print("\n" + "="*60)
        manager.print_statistics()
        print("="*60)
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        print("\nğŸ“ è¯¦ç»†ä¿¡æ¯:")
        for filename, metadata in sorted(manager.metadata_cache.items()):
            print(f"\n  ğŸ“„ {filename}")
            print(f"     DOI: {metadata.doi}")
            print(f"     ä½œè€…: {', '.join(metadata.authors) if metadata.authors else 'æ— '}")
            print(f"     å‘å¸ƒæ—¥æœŸ: {metadata.get_publish_date_str()}")
            print(f"     æ—¶æ•ˆæ€§å¾—åˆ†: {metadata.get_recency_score():.3f}")


def test_metadata_features():
    """æµ‹è¯•å…ƒæ•°æ®åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•å…ƒæ•°æ®åŠŸèƒ½...\n")
    
    # æµ‹è¯•1: åˆ›å»ºå…ƒæ•°æ®å¯¹è±¡
    print("æµ‹è¯• 1: åˆ›å»ºå…ƒæ•°æ®å¯¹è±¡")
    metadata = PaperMetadata(
        doi="10.1038/test",
        authors=["Alice", "Bob", "Charlie"],
        publish_year=2024,
        publish_month=1
    )
    print(f"  âœ“ åˆ›å»ºæˆåŠŸ")
    print(f"    - å‘å¸ƒå¹´ä»½: {metadata.get_publish_year()}")
    print(f"    - å‘å¸ƒæ—¥æœŸå­—ç¬¦ä¸²: {metadata.get_publish_date_str()}")
    print(f"    - æ—¶æ•ˆæ€§å¾—åˆ†: {metadata.get_recency_score():.3f}")
    
    # æµ‹è¯•2: æ—¶æ•ˆæ€§è®¡ç®—
    print("\næµ‹è¯• 2: ä¸åŒå¹´ä»½çš„æ—¶æ•ˆæ€§å¾—åˆ†")
    test_years = [2024, 2020, 2015, 2010, 2000]
    current_year = datetime.now().year
    for year in test_years:
        m = PaperMetadata(publish_year=year)
        score = m.get_recency_score()
        age = current_year - year
        print(f"  {year} ({age}å¹´å‰): {score:.3f}")
    
    # æµ‹è¯•3: å­—å…¸è½¬æ¢ï¼ˆæ–°æ ¼å¼ï¼‰
    print("\næµ‹è¯• 3: å­—å…¸åºåˆ—åŒ–å’Œååºåˆ—åŒ–ï¼ˆæ–°æ ¼å¼ï¼‰")
    original = PaperMetadata(
        doi="10.1234/test",
        authors=["Author1", "Author2"],
        publish_year=2023,
        publish_month=6
    )
    dict_data = original.to_dict()
    restored = PaperMetadata.from_dict(dict_data)
    print(f"  âœ“ åºåˆ—åŒ–æˆåŠŸ: {json.dumps(dict_data, ensure_ascii=False)}")
    print(f"  âœ“ ååºåˆ—åŒ–æˆåŠŸ: DOI={restored.doi}, ä½œè€…æ•°={len(restored.authors)}")
    
    # æµ‹è¯•4: å…ƒæ•°æ®ç®¡ç†å™¨
    print("\næµ‹è¯• 4: å…ƒæ•°æ®ç®¡ç†å™¨")
    manager = MetadataManager()
    manager.add_metadata("test1.md", metadata)
    retrieved = manager.get_metadata("test1.md")
    print(f"  âœ“ æ·»åŠ å’Œæ£€ç´¢æˆåŠŸ")
    print(f"  âœ“ æ£€ç´¢åˆ°çš„ DOI: {retrieved.doi}")
    
    # æµ‹è¯•5: ä»itemsæ•°ç»„åŠ è½½
    print("\næµ‹è¯• 5: ä»itemsæ•°ç»„æ ¼å¼åŠ è½½")
    test_data = {
        "items": [
            {
                "title": "Test Paper 1",
                "doi": "10.1234/test1",
                "authors": ["Author A", "Author B"],
                "published": {"year": 2023, "month": 5}
            },
            {
                "title": "Test Paper 2",
                "doi": "10.1234/test2",
                "authors": ["Author C"],
                "published": {"year": 2024}
            }
        ]
    }
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
        temp_file = f.name
    
    manager2 = MetadataManager()
    success = manager2.load_metadata_file(Path(temp_file))
    print(f"  âœ“ åŠ è½½æˆåŠŸ: {success}")
    print(f"  âœ“ åŠ è½½äº† {len(manager2.metadata_cache)} æ¡è®°å½•")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    import os
    os.unlink(temp_file)
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")


def main():
    parser = argparse.ArgumentParser(
        description="å…ƒæ•°æ®å·¥å…· - éªŒè¯ã€ç”Ÿæˆå’Œæµ‹è¯•è®ºæ–‡å…ƒæ•°æ®"
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤')
    
    # validate å‘½ä»¤
    validate_parser = subparsers.add_parser('validate', help='éªŒè¯å…ƒæ•°æ®æ–‡ä»¶')
    validate_parser.add_argument('file', type=str, help='å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    
    # generate å‘½ä»¤
    generate_parser = subparsers.add_parser('generate', help='ç”Ÿæˆå…ƒæ•°æ®æ¨¡æ¿')
    generate_parser.add_argument('output', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    generate_parser.add_argument(
        '--paper-dir',
        type=str,
        help='è®ºæ–‡ç›®å½•ï¼ˆå¯é€‰ï¼Œä¸ºç›®å½•ä¸­çš„æ‰€æœ‰ .md æ–‡ä»¶ç”Ÿæˆæ¡ç›®ï¼‰'
    )
    
    # stats å‘½ä»¤
    stats_parser = subparsers.add_parser('stats', help='æ˜¾ç¤ºå…ƒæ•°æ®ç»Ÿè®¡')
    stats_parser.add_argument('file', type=str, help='å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„')
    
    # merge å‘½ä»¤
    merge_parser = subparsers.add_parser('merge', help='åˆå¹¶å¤šä¸ªå…ƒæ•°æ®æ–‡ä»¶')
    merge_parser.add_argument('inputs', nargs='+', help='è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨')
    merge_parser.add_argument('-o', '--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')

    # test å‘½ä»¤
    test_parser = subparsers.add_parser('test', help='æµ‹è¯•å…ƒæ•°æ®åŠŸèƒ½')
    
    args = parser.parse_args()
    
    if args.command == 'validate':
        file_path = Path(args.file)
        print(f"ğŸ” éªŒè¯å…ƒæ•°æ®æ–‡ä»¶: {file_path}\n")
        is_valid, errors = validate_metadata_file(file_path)
        
        if is_valid:
            print("âœ… å…ƒæ•°æ®æ–‡ä»¶æœ‰æ•ˆ!")
        else:
            print("âŒ å…ƒæ•°æ®æ–‡ä»¶æ— æ•ˆ:\n")
            for error in errors:
                print(f"  â€¢ {error}")
            return 1
    
    elif args.command == 'generate':
        output_path = Path(args.output)
        paper_dir = Path(args.paper_dir) if args.paper_dir else None
        
        print(f"ğŸ“ ç”Ÿæˆå…ƒæ•°æ®æ¨¡æ¿: {output_path}\n")
        generate_template(output_path, paper_dir)
    
    elif args.command == 'stats':
        file_path = Path(args.file)
        print(f"ğŸ“Š åˆ†æå…ƒæ•°æ®æ–‡ä»¶: {file_path}\n")
        show_statistics(file_path)

    elif args.command == 'merge':
        input_files = [Path(p) for p in args.inputs]
        output_file = Path(args.output)
        merge_metadata_files(input_files, output_file)
    
    elif args.command == 'test':
        test_metadata_features()
    
    else:
        parser.print_help()
    
    return 0


if __name__ == "__main__":
    exit(main())
