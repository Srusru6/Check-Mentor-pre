"""
Workflow 1: åˆ†ææ•™æˆçš„æ ¸å¿ƒè´¡çŒ®

è¯¥å·¥ä½œæµçš„ç›®æ ‡æ˜¯å›ç­”ç¬¬ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š
"è€å¸ˆå¯¹å“ªäº›æ–¹å‘æ„Ÿå…´è¶£ï¼Œä»–å¯¹æ­¤æœ‰å“ªäº›è´¡çŒ®ï¼Ÿ"

å®ƒé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š
1. æ¥æ”¶æ•™æˆçš„ä»£è¡¨ä½œåˆ—è¡¨ï¼ˆmain_papersï¼‰ä½œä¸ºè¾“å…¥ã€‚
2. å¯¹æ¯ä¸€ç¯‡ä»£è¡¨ä½œè¿›è¡Œå†…å®¹æ€»ç»“ï¼Œæå–å…¶æ ¸å¿ƒç ”ç©¶æ–¹å‘å’Œè´¡çŒ®ã€‚
3. å°†æ‰€æœ‰è®ºæ–‡çš„åˆ†æç»“æœè¿›è¡Œç»¼åˆï¼Œå½¢æˆä¸€ä¸ªå…³äºæ•™æˆæ•´ä½“ç ”ç©¶æ–¹å‘å’Œè´¡çŒ®çš„ç»“æ„åŒ–æŠ¥å‘Šã€‚
"""
import json
import time
import re
from typing import List, Dict, Any
from tqdm import tqdm

from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatTongyi
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException

from .. import config
from .cache_manager import CacheManager

class ContributionWorkflow:
    """
    åˆ†ææ•™æˆæ ¸å¿ƒè´¡çŒ®çš„å·¥ä½œæµã€‚
    """
    def __init__(self, main_llm, fallback_llm=None):
        """
        åˆå§‹åŒ–å·¥ä½œæµï¼Œæ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„LLMå®ä¾‹ã€‚
        """
        print("  -> ContributionWorkflow initialized.")
        self.llm = main_llm
        self.fallback_llm = fallback_llm
        self.cache = None

    def _load_paper_content(self, file_path: str) -> str:
        """åŠ è½½æŒ‡å®šè·¯å¾„çš„ Markdown æ–‡ä»¶å†…å®¹ã€‚"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"    âš ï¸ Error loading file {file_path}: {e}")
            return ""

    def _invoke_llm_with_fallback(self, chain, paper_content):
        """
        è°ƒç”¨LLMï¼Œå¦‚æœä¸»LLMå¤±è´¥ï¼Œåˆ™å°è¯•å¤‡ç”¨LLMã€‚
        è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„è°ƒç”¨é€»è¾‘ï¼Œé€‚ç”¨äºæ‰€æœ‰å•ç¯‡è®ºæ–‡åˆ†æã€‚
        """
        try:
            # å°è¯•ä¸»LLM (åœ¨tqdmæ¨¡å¼ä¸‹ä¿æŒé™é»˜)
            result = chain.invoke({"paper_content": paper_content[:12000]})
            return result
        except (OutputParserException, json.JSONDecodeError):
            # ç¬¬ä¸€æ¬¡è§£æå¤±è´¥ï¼Œé™é»˜é‡è¯•ä¸€æ¬¡
            try:
                result = chain.invoke({"paper_content": paper_content[:12000]})
                return result
            except Exception:
                # é‡è¯•å¤±è´¥ï¼Œäº¤ç”±å¤‡ç”¨æ¨¡å‹å¤„ç†
                pass
        except Exception:
            # å…¶ä»–ä¸»LLMé”™è¯¯ï¼Œäº¤ç”±å¤‡ç”¨æ¨¡å‹å¤„ç†
            pass

        # å¦‚æœä¸»LLMå¤±è´¥ï¼Œä¸”å¤‡ç”¨LLMå·²é…ç½®ï¼Œåˆ™å°è¯•å¤‡ç”¨LLM
        if self.fallback_llm:
            try:
                fallback_chain = chain.with_components(llm=self.fallback_llm)
                result = fallback_chain.invoke({"paper_content": paper_content[:12000]})
                return result
            except Exception:
                # å¤‡ç”¨æ¨¡å‹ä¹Ÿå¤±è´¥äº†
                pass
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œåˆ™è¿”å›ä¸€ä¸ªé”™è¯¯æ ‡è®°
        return {"error": "Both main and fallback LLMs failed."}


    def _analyze_single_paper(self, paper_content: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM åˆ†æå•ç¯‡è®ºæ–‡çš„å†…å®¹ï¼Œæå–ç ”ç©¶é¢†åŸŸå’Œæ ¸å¿ƒè´¡çŒ®ã€‚
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert academic analyst. Your task is to extract the key information from a research paper.
Provide a JSON response with the following structure:
{{
  "research_area": "<The primary research area or sub-field of this paper, e.g., 'Quantum Computing', 'Photonic Integrated Circuits'>",
  "core_contribution": "<A concise, one-sentence summary of the paper's main contribution.>"
}}"""),
            ("user", "Please analyze the following paper content and provide the structured JSON output:\n\n---\n{paper_content}\n---")
        ])
        
        parser = JsonOutputParser()
        chain = prompt | self.llm | parser

        analysis_result = self._invoke_llm_with_fallback(chain, paper_content)
        # åœ¨è¿›åº¦æ¡æ¨¡å¼ä¸‹ï¼Œå•ä¸ªè¯·æ±‚çš„å»¶è¿Ÿå¯ä»¥é€‚å½“ç¼©çŸ­æˆ–ç§»é™¤ï¼Œ
        # å› ä¸ºæ€»ä½“é€Ÿç‡ç”±å¾ªç¯æ§åˆ¶
        # time.sleep(1) 
        
        return analysis_result

    def _synthesize_results(self, all_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å°†æ‰€æœ‰è®ºæ–‡çš„åˆ†æç»“æœç»¼åˆæˆä¸€ä¸ªæ€»çš„ã€å¯¹æœ¬ç§‘ç”Ÿå‹å¥½çš„æŠ¥å‘Šã€‚
        """
        # è¿‡æ»¤æ‰åˆ†æå¤±è´¥çš„è®ºæ–‡
        valid_analyses = [analysis for analysis in all_analyses if "error" not in analysis]
        if not valid_analyses:
            return {
                "research_directions": ["No valid analysis results to synthesize."],
                "contribution_summary": "Could not generate a summary due to lack of valid data."
            }

        # å°†æ‰€æœ‰åˆ†æç»“æœæ‰“åŒ…æˆä¸€ä¸ªå­—ç¬¦ä¸²
        analysis_text = "\n\n".join([
            f"Paper {i+1}:\n- Research Area: {analysis.get('research_area', 'N/A')}\n- Core Contribution: {analysis.get('core_contribution', 'N/A')}"
            for i, analysis in enumerate(valid_analyses)
        ])

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a senior science writer and mentor, tasked with writing a summary of a professor's research for a bright, motivated undergraduate student. The student is exploring research opportunities and needs to understand the professor's work: what it is, why it's important, and what its impact has been.

**Your Goal:** Transform a list of individual paper analyses into a compelling, clear, and honest narrative. Avoid overly simplistic analogies, but strive for clarity.

**Your Audience:** A smart undergraduate who is familiar with basic physics/engineering concepts but is not an expert in this specific sub-field.

**Key Instructions:**

1.  **Structure and Tone:**
    *   **Role-play:** Write as a knowledgeable and encouraging mentor. Your tone should be professional yet accessible and engaging.
    *   **Narrative, not a List:** Do not just list the findings. Weave them into a coherent story about the professor's research journey and goals.
    *   **Word Count:** Aim for a comprehensive summary of around 400 words. You have enough space to be thorough.

2.  **Content - Section 1: Research Directions (What they do):**
    *   Identify 3-5 primary, distinct research themes from the provided list.
    *   For each theme, write a short, clear paragraph. Start with the key concept, then briefly explain its goal.
    *   **"Prudent Explanation" Rule:**
        *   Identify key technical terms (e.g., "topological photonics," "quantum entanglement").
        *   If you are highly confident in your knowledge, provide a concise, parenthetical explanation `(like this)`.
        *   **Crucially:** If you encounter a highly specialized term and are NOT confident in explaining it, **DO NOT GUESS**. Simply use the term as is. This signals to the student that it's a specific concept to look up. Honesty is better than being wrong.

3.  **Content - Section 2: Contribution Summary (Why it matters & its Impact):**
    *   Synthesize the individual contributions into a big-picture overview.
    *   Explain the **"Why"**: What is the grand challenge or fundamental question this professor's work is trying to address? (e.g., "making quantum computers scalable," "pushing the limits of optical communication").
    *   Explain the **"Impact"**: How has their work advanced the field? Mention breakthroughs, pioneering work, or how they connect different ideas.
    *   Conclude with a powerful summary sentence that captures the essence of their research's significance.

**Output Format:**
You MUST provide a JSON response with a `research_directions` key (a list of strings) and a `contribution_summary` key (a single string).

{{
  "research_directions": [
    "<Paragraph for Direction 1>",
    "<Paragraph for Direction 2>",
    ...
  ],
  "contribution_summary": "<The overall summary paragraph, approximately 400 words.>"
}}"""),
            ("user", "Based on the following analyses of the professor's papers, please generate the structured summary:\n\n---\n{analysis_text}\n---")
        ])

        # åˆ†ç¦»LLMè°ƒç”¨å’Œè§£æ
        llm_chain = prompt | self.llm
        parser = JsonOutputParser()

        try:
            # æ­¥éª¤1: è°ƒç”¨LLMå¹¶è·å–åŸå§‹è¾“å‡º
            raw_output_obj = llm_chain.invoke({"analysis_text": analysis_text})
            raw_output = raw_output_obj.content if hasattr(raw_output_obj, 'content') else str(raw_output_obj)

            # æ­¥éª¤2: æ¸…ç†å¹¶æå–çº¯å‡€çš„JSONå­—ç¬¦ä¸²
            match = re.search(r"```json\s*([\s\S]*?)\s*```", raw_output)
            if match:
                cleaned_output = match.group(1)
            else:
                cleaned_output = raw_output

            # æ­¥éª¤3: å°è¯•è§£ææ¸…ç†åçš„è¾“å‡º
            synthesis_result = parser.parse(cleaned_output)
            
            # æ­¥éª¤4: éªŒè¯è§£æåçš„ç»“æ„
            if not isinstance(synthesis_result, dict) or "contribution_summary" not in synthesis_result:
                print(f"    âš ï¸ Synthesis output is not in the expected format: {synthesis_result}")
                return {
                    "research_directions": ["Synthesis failed: Unexpected output format."],
                    "contribution_summary": f"LLM returned an unexpected data structure. Raw output: {json.dumps(synthesis_result, indent=2, ensure_ascii=False)}"
                }
            return synthesis_result
            
        except OutputParserException as e:
            print(f"    ğŸ”´ Critical Error: Failed to parse LLM output during synthesis. Reason: {e}")
            return {
                "research_directions": ["Synthesis failed: OutputParserException."],
                "contribution_summary": f"The language model's response was not valid JSON and could not be parsed. Raw output snippet: {e.llm_output[:200]}..."
            }
        except Exception as e:
            print(f"    ğŸ”´ Critical Error: An unexpected error occurred during synthesis: {e}")
            return {
                "research_directions": ["Synthesis failed: Unexpected Exception."],
                "contribution_summary": f"An unexpected error occurred. Reason: {str(e)}"
            }

    def run(self, professor_name: str, main_papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰§è¡Œåˆ†ææ•™æˆæ ¸å¿ƒè´¡çŒ®çš„å®Œæ•´æµç¨‹ã€‚
        """
        self.cache = CacheManager(professor_name, "contribution_analysis")
        print(f"  -> Running ContributionWorkflow on {len(main_papers)} main papers.")

        if not main_papers:
            print("  -> No main papers provided. Skipping workflow.")
            return {
                "summary": "æ²¡æœ‰æä¾›æ•™æˆçš„ä»£è¡¨ä½œï¼Œæ— æ³•åˆ†æå…¶æ ¸å¿ƒè´¡çŒ®ã€‚",
                "research_areas": [],
                "key_contributions": []
            }

        # æ­¥éª¤1: å¯¹æ¯ç¯‡è®ºæ–‡è¿›è¡Œå•ç‹¬åˆ†æ
        all_single_analyses = []
        
        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        for paper in tqdm(main_papers, desc="  -> Analyzing contributions"):
            paper_id = paper['id']
            # æ£€æŸ¥ç¼“å­˜ï¼Œå¹¶è¦æ±‚æ‰€æœ‰å…³é”®å­—æ®µéƒ½å­˜åœ¨
            cached_result = self.cache.get(
                paper_id, 
                required_keys=["paper_id", "title", "research_area", "core_contribution"]
            )

            if cached_result:
                single_analysis = cached_result
            else:
                content = self._load_paper_content(paper['md_filename'])
                if not content:
                    continue
                
                analysis_result = self._analyze_single_paper(content)
                
                # æ£€æŸ¥LLMè°ƒç”¨æ˜¯å¦å‡ºé”™
                if analysis_result.get("error"):
                    tqdm.write(f"    âš ï¸ Skipped paper '{paper['title']}' due to LLM failure.")
                    continue

                # æ„å»ºå®Œæ•´çš„åˆ†æå¯¹è±¡
                single_analysis = {
                    **analysis_result,
                    'paper_id': paper_id,
                    'title': paper['title']
                }
                
                # ç¼“å­˜å®Œæ•´çš„å¯¹è±¡
                self.cache.set(paper_id, single_analysis)
                time.sleep(1) # åœ¨æˆåŠŸè°ƒç”¨åä¿ç•™å»¶è¿Ÿï¼Œé¿å…APIè¶…é€Ÿ

            all_single_analyses.append(single_analysis)

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è®ºæ–‡è¢«æˆåŠŸåˆ†æ
        if not all_single_analyses:
            print("  -> No papers were successfully analyzed. Skipping synthesis.")
            return {
                "summary": "æ‰€æœ‰è®ºæ–‡å‡æœªèƒ½æˆåŠŸåˆ†æï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚",
                "research_areas": [],
                "key_contributions": []
            }

        # æ­¥éª¤2: ç»¼åˆæ‰€æœ‰åˆ†æç»“æœ
        print("\n    -> Synthesizing all results...")
        final_summary = self._synthesize_results(all_single_analyses)

        # æ­¥éª¤3: æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡º
        final_result = {
            "research_directions": final_summary.get("research_directions", []),
            "contribution_summary": final_summary.get("contribution_summary", "æœªèƒ½ç”Ÿæˆæ ¸å¿ƒè´¡çŒ®æ€»ç»“ã€‚"),
            "analyzed_papers": [p["title"] for p in all_single_analyses],
            "key_contributions": [
                {
                    "paper_id": analysis['paper_id'],
                    "title": analysis['title'],
                    "contribution": analysis.get('core_contribution', 'N/A')
                }
                for analysis in all_single_analyses
            ]
        }

        return final_result
