"""
Workflow 1: åˆ†ææ•™æˆçš„æ ¸å¿ƒè´¡çŒ®

è¯¥å·¥ä½œæµçš„ç›®æ ‡æ˜¯å›ç­”ç¬¬ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š
"è€å¸ˆå¯¹å“ªäº›æ–¹å‘æ„Ÿå…´è¶£ï¼Œä»–å¯¹æ­¤æœ‰å“ªäº›è´¡çŒ®ï¼Ÿ"

å®ƒé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š
1. æ¥æ”¶æ•™æˆçš„ä»£è¡¨ä½œåˆ—è¡¨ï¼ˆmain_papersï¼‰ä½œä¸ºè¾“å…¥ã€‚
2. å¯¹æ¯ä¸€ç¯‡ä»£è¡¨ä½œè¿›è¡Œå†…å®¹æ€»ç»“ï¼Œæå–å…¶æ ¸å¿ƒç ”ç©¶æ–¹å‘å’Œè´¡çŒ®ã€‚
3. å°†æ‰€æœ‰è®ºæ–‡çš„åˆ†æç»“æœè¿›è¡Œç»¼åˆï¼Œå½¢æˆä¸€ä¸ªå…³äºæ•™æˆæ•´ä½“ç ”ç©¶æ–¹å‘å’Œè´¡çŒ®çš„ç»“æ„åŒ–æŠ¥å‘Šã€‚
"""
import json
import time
import re
from typing import List, Dict, Any
from tqdm import tqdm

from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatTongyi
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException

from .. import config
from .cache_manager import CacheManager

class ContributionWorkflow:
    """
    åˆ†ææ•™æˆæ ¸å¿ƒè´¡çŒ®çš„å·¥ä½œæµã€‚
    """
    def __init__(self, main_llm, fallback_llm=None):
        """
        åˆå§‹åŒ–å·¥ä½œæµï¼Œæ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„LLMå®ä¾‹ã€‚
        """
        print("  -> ContributionWorkflow initialized.")
        self.llm = main_llm
        self.fallback_llm = fallback_llm
        self.cache = None

    def _load_paper_content(self, file_path: str) -> str:
        """åŠ è½½æŒ‡å®šè·¯å¾„çš„ Markdown æ–‡ä»¶å†…å®¹ã€‚"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"    âš ï¸ Error loading file {file_path}: {e}")
            return ""

    def _invoke_llm_with_fallback(self, chain, paper_content, metadata_context=""):
        """
        è°ƒç”¨LLMï¼Œå¦‚æœä¸»LLMå¤±è´¥ï¼Œåˆ™å°è¯•å¤‡ç”¨LLMã€‚
        è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„è°ƒç”¨é€»è¾‘ï¼Œé€‚ç”¨äºæ‰€æœ‰å•ç¯‡è®ºæ–‡åˆ†æã€‚
        
        Args:
            chain: LangChainé“¾
            paper_content: è®ºæ–‡å†…å®¹
            metadata_context: å…ƒæ•°æ®ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        try:
            # å°è¯•ä¸»LLM (åœ¨tqdmæ¨¡å¼ä¸‹ä¿æŒé™é»˜)
            result = chain.invoke({
                "paper_content": paper_content[:12000],
                "metadata_context": metadata_context
            })
            return result
        except (OutputParserException, json.JSONDecodeError):
            # ç¬¬ä¸€æ¬¡è§£æå¤±è´¥ï¼Œé™é»˜é‡è¯•ä¸€æ¬¡
            try:
                result = chain.invoke({
                    "paper_content": paper_content[:12000],
                    "metadata_context": metadata_context
                })
                return result
            except Exception:
                # é‡è¯•å¤±è´¥ï¼Œäº¤ç”±å¤‡ç”¨æ¨¡å‹å¤„ç†
                pass
        except Exception:
            # å…¶ä»–ä¸»LLMé”™è¯¯ï¼Œäº¤ç”±å¤‡ç”¨æ¨¡å‹å¤„ç†
            pass

        # å¦‚æœä¸»LLMå¤±è´¥ï¼Œä¸”å¤‡ç”¨LLMå·²é…ç½®ï¼Œåˆ™å°è¯•å¤‡ç”¨LLM
        if self.fallback_llm:
            try:
                fallback_chain = chain.with_components(llm=self.fallback_llm)
                result = fallback_chain.invoke({
                    "paper_content": paper_content[:12000],
                    "metadata_context": metadata_context
                })
                return result
            except Exception:
                # å¤‡ç”¨æ¨¡å‹ä¹Ÿå¤±è´¥äº†
                pass
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œåˆ™è¿”å›ä¸€ä¸ªé”™è¯¯æ ‡è®°
        return {"error": "Both main and fallback LLMs failed."}


    def _analyze_single_paper(self, paper_content: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM åˆ†æå•ç¯‡è®ºæ–‡çš„å†…å®¹ï¼Œæå–ç ”ç©¶é¢†åŸŸå’Œæ ¸å¿ƒè´¡çŒ®ã€‚
        
        Args:
            paper_content: è®ºæ–‡å†…å®¹
            metadata: è®ºæ–‡å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰ï¼ŒåŒ…å«å‘å¸ƒæ—¶é—´ã€ä½œè€…ç­‰ä¿¡æ¯
        """
        # æ„å»ºå…ƒæ•°æ®ä¸Šä¸‹æ–‡ä¿¡æ¯
        metadata_context = ""
        if metadata:
            metadata_context = "\n\n**Paper Metadata:**"
            if metadata.get("publish_date"):
                metadata_context += f"\n- Publication Date: {metadata['publish_date']}"
            if metadata.get("authors"):
                metadata_context += f"\n- Authors: {', '.join(metadata['authors'][:5])}"  # åªæ˜¾ç¤ºå‰5ä½ä½œè€…
                if len(metadata['authors']) > 5:
                    metadata_context += f" (and {len(metadata['authors']) - 5} more)"
            if metadata.get("doi"):
                metadata_context += f"\n- DOI: {metadata['doi']}"
            
            # å¼ºè°ƒæ–°è®ºæ–‡çš„é‡è¦æ€§
            recency_note = ""
            if metadata.get("publish_date"):
                try:
                    from datetime import datetime
                    pub_year = int(metadata['publish_date'][:4])
                    current_year = datetime.now().year
                    age = current_year - pub_year
                    
                    if age <= 2:
                        recency_note = "\n\n**NOTE**: This is a very recent paper (published within the last 2 years). Recent papers often represent the cutting-edge of the field and should be given special attention when identifying current research directions."
                    elif age <= 5:
                        recency_note = "\n\n**NOTE**: This is a relatively recent paper. Consider how it reflects current trends in the field."
                except:
                    pass
            
            metadata_context += recency_note
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert academic analyst. Your task is to extract the key information from a research paper.
Provide a JSON response with the following structure:
{{
  "research_area": "<The primary research area or sub-field of this paper, e.g., 'Quantum Computing', 'Photonic Integrated Circuits'>",
  "core_contribution": "<A concise, one-sentence summary of the paper's main contribution.>"
}}"""),
            ("user", "Please analyze the following paper content and provide the structured JSON output:{metadata_context}\n\n---\n{paper_content}\n---")
        ])
        
        parser = JsonOutputParser()
        chain = prompt | self.llm | parser

        analysis_result = self._invoke_llm_with_fallback(
            chain, 
            paper_content,
            metadata_context=metadata_context
        )
        # åœ¨è¿›åº¦æ¡æ¨¡å¼ä¸‹ï¼Œå•ä¸ªè¯·æ±‚çš„å»¶è¿Ÿå¯ä»¥é€‚å½“ç¼©çŸ­æˆ–ç§»é™¤ï¼Œ
        # å› ä¸ºæ€»ä½“é€Ÿç‡ç”±å¾ªç¯æ§åˆ¶
        # time.sleep(1) 
        
        return analysis_result

    def _synthesize_results(self, all_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å°†æ‰€æœ‰è®ºæ–‡çš„åˆ†æç»“æœç»¼åˆæˆä¸€ä¸ªæ€»çš„ã€å¯¹æœ¬ç§‘ç”Ÿå‹å¥½çš„æŠ¥å‘Šã€‚
        
        ç°åœ¨ä¼šè€ƒè™‘è®ºæ–‡çš„æ—¶æ•ˆæ€§æƒé‡ï¼šæ›´æ–°çš„è®ºæ–‡åœ¨ç»¼åˆæ—¶ä¼šè¢«ç»™äºˆæ›´å¤šå…³æ³¨ã€‚
        """
        # è¿‡æ»¤æ‰åˆ†æå¤±è´¥çš„è®ºæ–‡
        valid_analyses = [analysis for analysis in all_analyses if "error" not in analysis]
        if not valid_analyses:
            return {
                "research_directions": ["No valid analysis results to synthesize."],
                "contribution_summary": "Could not generate a summary due to lack of valid data."
            }

        # æŒ‰æ—¶æ•ˆæ€§å¾—åˆ†æ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼Œå¹¶æ„å»ºåˆ†ææ–‡æœ¬
        # é«˜å¾—åˆ†ï¼ˆæ›´æ–°ï¼‰çš„è®ºæ–‡ä¼šè¢«æ”¾åœ¨å‰é¢ï¼Œåœ¨æç¤ºè¯ä¸­ä¼šè¢«LLMä¼˜å…ˆè€ƒè™‘
        def get_recency_score(analysis):
            return analysis.get('recency_score', 0.5)
        
        sorted_analyses = sorted(valid_analyses, key=get_recency_score, reverse=True)
        
        # æ„å»ºåˆ†ææ–‡æœ¬ï¼ŒåŒ…å«æ—¶æ•ˆæ€§ä¿¡æ¯
        analysis_parts = []
        for i, analysis in enumerate(sorted_analyses):
            paper_text = f"Paper {i+1}:\n- Research Area: {analysis.get('research_area', 'N/A')}\n- Core Contribution: {analysis.get('core_contribution', 'N/A')}"
            
            # å¦‚æœæœ‰æ—¶æ•ˆæ€§ä¿¡æ¯ï¼Œæ·»åŠ æƒé‡æç¤º
            recency_score = analysis.get('recency_score')
            if recency_score is not None and recency_score != 0.5:  # 0.5æ˜¯é»˜è®¤å€¼
                if recency_score > 0.8:
                    paper_text += f"\n- **Recency Weight: HIGH** (published recently, represents cutting-edge work)"
                elif recency_score > 0.6:
                    paper_text += f"\n- **Recency Weight: MEDIUM-HIGH** (relatively recent work)"
                elif recency_score < 0.3:
                    paper_text += f"\n- **Recency Weight: LOW** (older foundational work)"
            
            analysis_parts.append(paper_text)
        
        analysis_text = "\n\n".join(analysis_parts)

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a senior science writer and mentor, tasked with writing a summary of a professor's research for a bright, motivated undergraduate student. The student is exploring research opportunities and needs to understand the professor's work: what it is, why it's important, and what its impact has been.

**Your Goal:** Transform a list of individual paper analyses into a compelling, clear, and honest narrative. Avoid overly simplistic analogies, but strive for clarity.

**Your Audience:** A smart undergraduate who is familiar with basic physics/engineering concepts but is not an expert in this specific sub-field.

**IMPORTANT - Temporal Weighting**: The papers are listed with recency weights. Papers marked as "HIGH" or "MEDIUM-HIGH" recency represent more recent work and should be given MORE ATTENTION in your synthesis, as they reflect the professor's current research directions and the field's cutting-edge. Older papers (marked as "LOW" recency) provide historical context but should not dominate the narrative unless they are clearly foundational breakthroughs.

**Key Instructions:**

1.  **Structure and Tone:**
    *   **Role-play:** Write as a knowledgeable and encouraging mentor. Your tone should be professional yet accessible and engaging.
    *   **Narrative, not a List:** Do not just list the findings. Weave them into a coherent story about the professor's research journey and goals.
    *   **Word Count:** Aim for a comprehensive summary of around 400 words. You have enough space to be thorough.

2.  **Content - Section 1: Research Directions (What they do):**
    *   Identify 3-5 primary, distinct research themes from the provided list.
    *   **Prioritize recent work**: Focus more on themes evident in papers with HIGH recency weight.
    *   For each theme, write a short, clear paragraph. Start with the key concept, then briefly explain its goal.
    *   **"Prudent Explanation" Rule:**
        *   Identify key technical terms (e.g., "topological photonics," "quantum entanglement").
        *   If you are highly confident in your knowledge, provide a concise, parenthetical explanation `(like this)`.
        *   **Crucially:** If you encounter a highly specialized term and are NOT confident in explaining it, **DO NOT GUESS**. Simply use the term as is. This signals to the student that it's a specific concept to look up. Honesty is better than being wrong.

3.  **Content - Section 2: Contribution Summary (Why it matters & its Impact):**
    *   Synthesize the individual contributions into a big-picture overview.
    *   **Emphasize recent contributions**: The summary should reflect the professor's current focus and recent achievements.
    *   Explain the **"Why"**: What is the grand challenge or fundamental question this professor's work is trying to address? (e.g., "making quantum computers scalable," "pushing the limits of optical communication").
    *   Explain the **"Impact"**: How has their work advanced the field? Mention breakthroughs, pioneering work, or how they connect different ideas.
    *   Explain the **"Impact"**: How has their work advanced the field? Mention breakthroughs, pioneering work, or how they connect different ideas.
    *   Conclude with a powerful summary sentence that captures the essence of their research's significance.

**Output Format:**
You MUST provide a JSON response with a `research_directions` key (a list of strings) and a `contribution_summary` key (a single string).

{{
  "research_directions": [
    "<Paragraph for Direction 1>",
    "<Paragraph for Direction 2>",
    ...
  ],
  "contribution_summary": "<The overall summary paragraph, approximately 400 words.>"
}}"""),
            ("user", "Based on the following analyses of the professor's papers, please generate the structured summary:\n\n---\n{analysis_text}\n---")
        ])

        # åˆ†ç¦»LLMè°ƒç”¨å’Œè§£æ
        llm_chain = prompt | self.llm
        parser = JsonOutputParser()

        try:
            # æ­¥éª¤1: è°ƒç”¨LLMå¹¶è·å–åŸå§‹è¾“å‡º
            raw_output_obj = llm_chain.invoke({"analysis_text": analysis_text})
            raw_output = raw_output_obj.content if hasattr(raw_output_obj, 'content') else str(raw_output_obj)

            # æ­¥éª¤2: æ¸…ç†å¹¶æå–çº¯å‡€çš„JSONå­—ç¬¦ä¸²
            match = re.search(r"```json\s*([\s\S]*?)\s*```", raw_output)
            if match:
                cleaned_output = match.group(1)
            else:
                cleaned_output = raw_output

            # æ­¥éª¤3: å°è¯•è§£ææ¸…ç†åçš„è¾“å‡º
            synthesis_result = parser.parse(cleaned_output)
            
            # æ­¥éª¤4: éªŒè¯è§£æåçš„ç»“æ„
            if not isinstance(synthesis_result, dict) or "contribution_summary" not in synthesis_result:
                print(f"    âš ï¸ Synthesis output is not in the expected format: {synthesis_result}")
                return {
                    "research_directions": ["Synthesis failed: Unexpected output format."],
                    "contribution_summary": f"LLM returned an unexpected data structure. Raw output: {json.dumps(synthesis_result, indent=2, ensure_ascii=False)}"
                }
            return synthesis_result
            
        except OutputParserException as e:
            print(f"    ğŸ”´ Critical Error: Failed to parse LLM output during synthesis. Reason: {e}")
            return {
                "research_directions": ["Synthesis failed: OutputParserException."],
                "contribution_summary": f"The language model's response was not valid JSON and could not be parsed. Raw output snippet: {e.llm_output[:200]}..."
            }
        except Exception as e:
            print(f"    ğŸ”´ Critical Error: An unexpected error occurred during synthesis: {e}")
            return {
                "research_directions": ["Synthesis failed: Unexpected Exception."],
                "contribution_summary": f"An unexpected error occurred. Reason: {str(e)}"
            }

    def run(self, professor_name: str, main_papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰§è¡Œåˆ†ææ•™æˆæ ¸å¿ƒè´¡çŒ®çš„å®Œæ•´æµç¨‹ã€‚
        """
        self.cache = CacheManager(professor_name, "contribution_analysis")
        print(f"  -> Running ContributionWorkflow on {len(main_papers)} main papers.")

        if not main_papers:
            print("  -> No main papers provided. Skipping workflow.")
            return {
                "summary": "æ²¡æœ‰æä¾›æ•™æˆçš„ä»£è¡¨ä½œï¼Œæ— æ³•åˆ†æå…¶æ ¸å¿ƒè´¡çŒ®ã€‚",
                "research_areas": [],
                "key_contributions": []
            }

        # æ­¥éª¤1: å¯¹æ¯ç¯‡è®ºæ–‡è¿›è¡Œå•ç‹¬åˆ†æ
        all_single_analyses = []
        
        # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
        for paper in tqdm(main_papers, desc="  -> Analyzing contributions"):
            paper_id = paper['id']
            # æ£€æŸ¥ç¼“å­˜ï¼Œå¹¶è¦æ±‚æ‰€æœ‰å…³é”®å­—æ®µéƒ½å­˜åœ¨
            cached_result = self.cache.get(
                paper_id, 
                required_keys=["paper_id", "title", "research_area", "core_contribution"]
            )

            if cached_result:
                single_analysis = cached_result
                # ç¡®ä¿ç¼“å­˜çš„ç»“æœä¹ŸåŒ…å«æ—¶æ•ˆæ€§å¾—åˆ†ï¼ˆå¦‚æœè®ºæ–‡æœ‰è¿™ä¸ªå­—æ®µï¼‰
                if 'recency_score' not in single_analysis and 'recency_score' in paper:
                    single_analysis['recency_score'] = paper['recency_score']
            else:
                content = self._load_paper_content(paper['md_filename'])
                if not content:
                    continue
                
                # æå–å…ƒæ•°æ®
                paper_metadata = paper.get('metadata')
                
                analysis_result = self._analyze_single_paper(content, paper_metadata)
                
                # æ£€æŸ¥LLMè°ƒç”¨æ˜¯å¦å‡ºé”™
                if analysis_result.get("error"):
                    tqdm.write(f"    âš ï¸ Skipped paper '{paper['title']}' due to LLM failure.")
                    continue

                # æ„å»ºå®Œæ•´çš„åˆ†æå¯¹è±¡ï¼ŒåŒ…å«æ—¶æ•ˆæ€§å¾—åˆ†
                single_analysis = {
                    **analysis_result,
                    'paper_id': paper_id,
                    'title': paper['title']
                }
                
                # å¦‚æœè®ºæ–‡æœ‰æ—¶æ•ˆæ€§å¾—åˆ†ï¼Œä¹ŸåŒ…å«è¿›å»
                if 'recency_score' in paper:
                    single_analysis['recency_score'] = paper['recency_score']
                
                # ç¼“å­˜å®Œæ•´çš„å¯¹è±¡
                self.cache.set(paper_id, single_analysis)
                time.sleep(1) # åœ¨æˆåŠŸè°ƒç”¨åä¿ç•™å»¶è¿Ÿï¼Œé¿å…APIè¶…é€Ÿ

            all_single_analyses.append(single_analysis)

        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è®ºæ–‡è¢«æˆåŠŸåˆ†æ
        if not all_single_analyses:
            print("  -> No papers were successfully analyzed. Skipping synthesis.")
            return {
                "summary": "æ‰€æœ‰è®ºæ–‡å‡æœªèƒ½æˆåŠŸåˆ†æï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚",
                "research_areas": [],
                "key_contributions": []
            }

        # æ­¥éª¤2: ç»¼åˆæ‰€æœ‰åˆ†æç»“æœ
        print("\n    -> Synthesizing all results...")
        final_summary = self._synthesize_results(all_single_analyses)

        # æ­¥éª¤3: æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡º
        final_result = {
            "research_directions": final_summary.get("research_directions", []),
            "contribution_summary": final_summary.get("contribution_summary", "æœªèƒ½ç”Ÿæˆæ ¸å¿ƒè´¡çŒ®æ€»ç»“ã€‚"),
            "analyzed_papers": [p["title"] for p in all_single_analyses],
            "key_contributions": [
                {
                    "paper_id": analysis['paper_id'],
                    "title": analysis['title'],
                    "contribution": analysis.get('core_contribution', 'N/A')
                }
                for analysis in all_single_analyses
            ]
        }

        return final_result
