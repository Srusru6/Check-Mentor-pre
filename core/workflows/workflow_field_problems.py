"""
Workflow 2: åˆ†æé¢†åŸŸçš„çƒ­ç‚¹é—®é¢˜

è¯¥å·¥ä½œæµçš„ç›®æ ‡æ˜¯å›ç­”ç¬¬äºŒä¸ªæ ¸å¿ƒé—®é¢˜ï¼š
"è¿™ä¸ªé¢†åŸŸä¸»è¦åœ¨å…³å¿ƒä»€ä¹ˆé—®é¢˜ï¼Ÿ"

å®ƒé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°ï¼š
1. æ¥æ”¶æ•™æˆçš„ä»£è¡¨ä½œï¼ˆmain_papersï¼‰å’Œå…³é”®å‚è€ƒæ–‡çŒ®ï¼ˆref1_papersï¼‰ä½œä¸ºè¾“å…¥ã€‚
2. è®¾è®¡ä¸€ä¸ªAIè¯„åˆ†æœºåˆ¶ï¼Œè¯„ä¼°æ¯ç¯‡è®ºæ–‡å¯¹äºâ€œé¢†åŸŸé—®é¢˜ä»£è¡¨æ€§â€çš„å¾—åˆ†ã€‚
3. åŸºäºå¾—åˆ†è¾ƒé«˜çš„è®ºæ–‡ï¼Œç»¼åˆåˆ†æå‡ºè¯¥é¢†åŸŸå½“å‰å…³æ³¨çš„æ ¸å¿ƒé—®é¢˜å’ŒæŠ€æœ¯è¶‹åŠ¿ã€‚
"""
import json
import time
from typing import List, Dict, Any
from tqdm import tqdm

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.exceptions import OutputParserException

from .cache_manager import CacheManager

class FieldProblemsWorkflow:
    """
    åˆ†æé¢†åŸŸçƒ­ç‚¹é—®é¢˜çš„å·¥ä½œæµã€‚
    """
    def __init__(self, main_llm, fallback_llm=None):
        """
        åˆå§‹åŒ–å·¥ä½œæµï¼Œæ¥æ”¶å¤–éƒ¨ä¼ å…¥çš„LLMå®ä¾‹ã€‚
        """
        print("  -> FieldProblemsWorkflow initialized.")
        self.llm = main_llm
        self.fallback_llm = fallback_llm
        self.cache = None

    def _print_section_header(self, title: str, level: int = 1):
        """æ‰“å°å¸¦æ ·å¼çš„ç« èŠ‚æ ‡é¢˜"""
        if level == 1:
            print(f"\n{'='*70}\nğŸ“ {title}\n{'='*70}")
        elif level == 2:
            print(f"\n--- {title} ---")

    def _load_paper_content(self, file_path: str) -> str:
        """åŠ è½½æŒ‡å®šè·¯å¾„çš„ Markdown æ–‡ä»¶å†…å®¹ã€‚"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"    âš ï¸ Error loading file {file_path}: {e}")
            return ""

    def _invoke_llm_with_fallback(self, chain, paper_content):
        """
        è°ƒç”¨LLMï¼Œå¦‚æœä¸»LLMå¤±è´¥ï¼Œåˆ™å°è¯•å¤‡ç”¨LLMã€‚
        """
        try:
            result = chain.invoke({"paper_content": paper_content[:12000]})
            return result
        except (OutputParserException, json.JSONDecodeError):
            try:
                result = chain.invoke({"paper_content": paper_content[:12000]})
                return result
            except Exception:
                pass
        except Exception:
            pass

        if self.fallback_llm:
            try:
                fallback_chain = chain.with_llm(self.fallback_llm)
                result = fallback_chain.invoke({"paper_content": paper_content[:12000]})
                return result
            except Exception:
                pass
        
        return {"error": "Both main and fallback LLMs failed."}

    def _rate_single_paper(self, paper_content: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ LLM è¯„ä¼°å•ç¯‡è®ºæ–‡ï¼ŒåŸºäºå››ç»´æ¨¡å‹è¿›è¡Œæ‰“åˆ†ã€‚
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a top-tier, discerning academic reviewer, known for your critical judgment and ability to differentiate between good and truly exceptional work. Your reputation depends on it. **You must avoid grade inflation.** Use the entire 0.0 to 1.0 scale meaningfully. A score of 0.9 or higher should be reserved only for papers that are genuine landmarks in their field.

You will provide a multi-dimensional evaluation based on the following four criteria. Each score MUST be a float between 0.0 and 1.0.

**Evaluation Criteria & Weights:**

1.  **Significance (Weight: 30%)**: How important is the core problem this paper addresses?
    - 1.0: A foundational, field-defining problem (e.g., demonstrating quantum supremacy, solving a major open question).
    - 0.7: A known, important sub-problem that unlocks significant progress.
    - 0.4: A niche or incremental issue with limited impact.
    - 0.1: A minor or solved problem.

2.  **Novelty (Weight: 10%)**: How original is the paper's proposed SOLUTION or METHOD?
    - 1.0: A completely new paradigm or technique that changes how the field works.
    - 0.7: A clever and non-obvious combination or improvement of existing methods.
    - 0.4: A standard, incremental improvement.
    - 0.1: A routine application of well-known methods.
    - **Note for Review Papers**: For a review, this score is expected to be low. Do not penalize it for this.

3.  **Clarity (Weight: 30%)**: How clearly is the problem, its context, and the proposed solution/synthesis presented?
    - 1.0: A masterclass in scientific communication. A new student could grasp the field's landscape from this paper alone. The structure is flawless.
    - 0.7: Well-written and structured, understandable by experts with some effort.
    - 0.4: Generally understandable, but key parts are confusing, poorly structured, or buried in jargon.
    - 0.1: Poorly written, illogical, and hard to understand.
    - **Note for Review Papers**: A well-organized review that clarifies the state of the art and provides a coherent narrative should score very highly here. This is a primary metric for a good review.

4.  **Potential (Weight: 30%)**: How likely is this work to inspire or enable significant future research?
    - 1.0: Opens up entirely new research avenues or provides a critical enabling tool that will be widely adopted.
    - 0.7: Likely to lead to a flurry of direct follow-up studies and citations.
    - 0.4: May be cited or lead to a few incremental studies.
    - 0.1: Unlikely to have a significant impact on the field.
    - **Note for Review Papers**: A comprehensive and insightful review that successfully maps out future challenges and opportunities should score very highly here. This is a primary metric for a good review.

You MUST provide a JSON response with the following structure:
{{
  "significance_score": <float between 0.0 and 1.0>,
  "novelty_score": <float between 0.0 and 1.0>,
  "clarity_score": <float between 0.0 and 1.0>,
  "potential_score": <float between 0.0 and 1.0>,
  "justification": "<A concise, critical justification for your scores, referencing the criteria above. Explain WHY you gave these specific scores.>",
  "identified_problem": "<A short, precise phrase identifying the core problem the paper tackles.>"
}}"""),
            ("user", "Please analyze the following paper content and provide the structured JSON output:\n\n---\n{paper_content}\n---")
        ])
        
        parser = JsonOutputParser()
        chain = prompt | self.llm | parser

        analysis_result = self._invoke_llm_with_fallback(chain, paper_content)
        
        if "error" in analysis_result:
            return analysis_result
            
        required_keys = ["significance_score", "novelty_score", "clarity_score", "potential_score"]
        if not all(key in analysis_result for key in required_keys):
            return {"error": "LLM response was missing one or more required score keys."}

        return analysis_result

    def _summarize_hot_topics(self, synthesis_context: str) -> Dict[str, Any]:
        """
        (New) Synthesizes hot topics directly from a list of top-rated papers.
        This replaces the more complex map-reduce logic for simplicity and robustness.
        """
        print("    -> Synthesizing hot topics from top-rated papers...")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a senior research analyst. You have been given a list of top-rated papers from a specific field. Your task is to synthesize this information into a coherent overview of the field's hot topics.

The papers are provided below, each with a title, the core problem it addresses, and a justification for its high rating.
---
{synthesis_context}
---

Your final output MUST be a JSON object with the following structure:
{{
  "summary": "<A brief, coherent narrative (about 200-250 words) explaining what these hot topics are, how they relate to each other, and why they are important for the field. Integrate insights from the paper justifications, preserving any mentioned novel or unique points.>",
  "hot_topics": [
    {{
      "topic_name": "<The name of the first main hot topic, derived from the papers>",
      "challenge": "<A brief description of the core challenge or question for this topic.>",
      "related_papers": ["<Title of paper 1>", "<Title of paper 2>"]
    }}
  ]
}}

Instructions:
1.  Read through all the provided paper details.
2.  Identify 2-4 overarching themes or "hot topics" that emerge from the collective 'Identified Problem' fields.
3.  For each topic, formulate a concise `topic_name` and `challenge`.
4.  Group the paper titles under the most relevant `hot_topics` they belong to. A paper can be listed under multiple topics if it's relevant.
5.  Write the final `summary` narrative based on all the information.
"""),
            ("user", "The context containing the top-rated papers is provided above. Please generate the JSON output.")
        ])

        parser = JsonOutputParser()
        chain = prompt | self.llm | parser

        try:
            synthesis_result = chain.invoke({
                "synthesis_context": synthesis_context
            })
            return synthesis_result
        except Exception as e:
            print(f"    âš ï¸ Error during final synthesis: {e}")
            return {
                "summary": "Failed to synthesize final summary due to an error.",
                "hot_topics": []
            }

    def _cluster_papers_by_llm(self, papers: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """
        (New) Uses an LLM to perform semantic clustering on a list of papers.
        """
        print("    -> High-score paper count exceeds threshold. Performing LLM-based semantic clustering...")

        paper_info = [
            {"title": p.get("title", "N/A"), "identified_problem": p.get("identified_problem", "N/A")}
            for p in papers
        ]

        prompt = ChatPromptTemplate.from_messages([
            ("system", """You are a senior research analyst. You have been given a list of papers, each with its title and the core problem it addresses. Your task is to group these papers into 3-5 high-level research themes based on the **semantic similarity** of their core problems.

Your task is **clustering, not summarization**.

The output MUST be a valid JSON object where:
- Each KEY is a concise, descriptive name for a research theme you identified (e.g., "On-Chip Quantum Light Sources").
- Each VALUE is a list of paper titles that belong to that theme.

Example Input:
[
  {{"title": "Paper A", "identified_problem": "scalable quantum entanglement"}},
  {{"title": "Paper B", "identified_problem": "generating multi-photon entangled states"}},
  {{"title": "Paper C", "identified_problem": "topological protection on photonic chips"}}
]

Example Output:
{{
  "Scalable Quantum Entanglement": ["Paper A", "Paper B"],
  "Topological Photonics": ["Paper C"]
}}"""),
            ("user", "Here is the list of papers to cluster:\n\n{paper_info_json}")
        ])

        parser = JsonOutputParser()
        chain = prompt | self.llm | parser

        try:
            paper_info_json = json.dumps(paper_info, indent=2, ensure_ascii=False)
            cluster_result = chain.invoke({"paper_info_json": paper_info_json})
            return cluster_result
        except Exception as e:
            print(f"    âš ï¸ Error during LLM clustering: {e}")
            # Fallback: if clustering fails, return a single group to avoid crashing
            return {"All High-Score Papers": [p["title"] for p in papers]}

    def run(self, main_papers: List[Dict[str, Any]], ref1_papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå·¥ä½œæµï¼Œåˆ†ææ‰€æœ‰è®ºæ–‡å¹¶è¯†åˆ«é¢†åŸŸçƒ­ç‚¹é—®é¢˜ã€‚
        """
        self._print_section_header("Workflow 2: Analyzing Field Problems", level=2)
        
        if not main_papers:
             print("  -> No main papers provided. Cannot determine professor name for cache.")
             # Fallback to a default name if no papers are provided
             professor_name = "default_professor"
        else:
            professor_name = main_papers[0]['authors'][0]

        self.cache = CacheManager(professor_name, "field_problems_analysis")
        print(f"  -> Cache file set for professor '{professor_name}' in workflow 'field_problems_analysis'.")

        all_papers = main_papers + ref1_papers
        
        if not all_papers:
            print("  -> No papers provided. Skipping workflow.")
            return {
                "summary": "æ²¡æœ‰æä¾›ä»»ä½•è®ºæ–‡ï¼Œæ— æ³•åˆ†æé¢†åŸŸçƒ­ç‚¹é—®é¢˜ã€‚",
                "hot_topics": [],
                "analyzed_papers": [],
            }

        # 1. è¯„ä¼°æ‰€æœ‰è®ºæ–‡çš„â€œé¢†åŸŸé—®é¢˜ä»£è¡¨æ€§â€
        print(f"\n  [Step 1/2] Evaluating {len(all_papers)} papers for field relevance...")
        
        rated_papers = []
        
        with tqdm(total=len(all_papers), desc="  Rating papers") as pbar:
            for paper in all_papers:
                paper_id = paper["id"]
                
                cached_result = self.cache.get(paper_id)
                if cached_result:
                    pbar.update(1)
                    pbar.set_postfix_str("Cached")
                    if all(k in cached_result for k in ['significance_score', 'novelty_score', 'clarity_score', 'potential_score']):
                        # å°†ç¼“å­˜ç»“æœä¸å…ƒæ•°æ®åˆå¹¶
                        full_cached_result = {**cached_result, 'paper_id': paper_id, 'title': paper['title']}
                        rated_papers.append(full_cached_result)
                        continue
                    else:
                        pbar.set_postfix_str("Invalid Cache, Re-analyzing")

                paper_content = self._load_paper_content(paper["md_filename"])
                if not paper_content:
                    pbar.update(1)
                    pbar.set_postfix_str("Skipped (no content)")
                    continue

                analysis_result = self._rate_single_paper(paper_content)
                
                if "error" in analysis_result:
                    pbar.update(1)
                    pbar.set_postfix_str(f"Error ({analysis_result['error']})")
                    continue

                s_score = analysis_result.get('significance_score', 0)
                n_score = analysis_result.get('novelty_score', 0)
                c_score = analysis_result.get('clarity_score', 0)
                p_score = analysis_result.get('potential_score', 0)
                
                weighted_score = (s_score * 0.3) + (n_score * 0.1) + (c_score * 0.3) + (p_score * 0.3)
                analysis_result["weighted_score"] = round(weighted_score, 2)

                full_result = {**analysis_result, "paper_id": paper_id, "title": paper["title"]}
                
                rated_papers.append(full_result)
                self.cache.set(paper_id, analysis_result)
                
                pbar.update(1)
                pbar.set_postfix_str(f"Score: {weighted_score:.2f}")
                time.sleep(0.1)
        
        if not rated_papers:
            print("\n  -> No papers were successfully rated. Skipping synthesis.")
            return {
                "summary": "æ‰€æœ‰è®ºæ–‡å‡æœªèƒ½æˆåŠŸè¯„åˆ†ï¼Œæ— æ³•åˆ†æé¢†åŸŸçƒ­ç‚¹é—®é¢˜ã€‚",
                "hot_topics": [],
                "analyzed_papers": [],
            }

        # 2. åŸºäºé«˜åˆ†è®ºæ–‡ï¼Œç»¼åˆåˆ†æçƒ­ç‚¹é—®é¢˜
        print("\n  [Step 2/2] Synthesizing hot topics from top-rated papers...")
        
        high_score_threshold = 0.6
        high_score_papers = [p for p in rated_papers if p.get("weighted_score", 0.0) >= high_score_threshold]
        print(f"  -> Found {len(high_score_papers)} papers with score >= {high_score_threshold}.")

        # å¦‚æœé«˜è´¨é‡è®ºæ–‡è¿‡å¤šï¼Œåˆ™è¿›è¡Œèšç±»å’Œä»£è¡¨æ€§é‡‡æ ·
        if len(high_score_papers) > 10:
            # 1. LLMè¯­ä¹‰èšç±»
            clusters = self._cluster_papers_by_llm(high_score_papers)
            
            # 2. ä»£è¡¨æ€§æå–
            representative_papers = []
            paper_map = {p["title"]: p for p in high_score_papers}
            
            for theme, titles in clusters.items():
                if not titles: continue
                
                # æ‰¾åˆ°è¯¥ä¸»é¢˜ä¸‹åˆ†æ•°æœ€é«˜çš„è®ºæ–‡
                theme_papers = [paper_map[title] for title in titles if title in paper_map]
                if not theme_papers: continue
                
                best_paper_in_theme = max(theme_papers, key=lambda p: p.get("weighted_score", 0))
                representative_papers.append(best_paper_in_theme)
            
            # å»é‡ï¼Œå› ä¸ºä¸€ç¯‡è®ºæ–‡å¯èƒ½å±äºå¤šä¸ªèšç±»
            # ä½¿ç”¨å­—å…¸æ¥å»é‡ï¼Œä¿æŒé¡ºåº
            final_papers_for_synthesis = list({p["paper_id"]: p for p in representative_papers}.values())
            print(f"  -> Clustered into {len(clusters)} themes. Selected {len(final_papers_for_synthesis)} representative papers for final synthesis.")
            top_papers = final_papers_for_synthesis
        else:
            # å¦‚æœè®ºæ–‡æ•°é‡ä¸å¤šï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰é«˜åˆ†è®ºæ–‡
            print("  -> Number of high-score papers is manageable. Using all for synthesis.")
            top_papers = high_score_papers

        if not top_papers:
            print("    âš ï¸ No high-score papers found. Cannot synthesize hot topics.")
            return {
                "summary": "æœªèƒ½æˆåŠŸè¯„ä¼°ä»»ä½•è®ºæ–‡ï¼Œæ— æ³•ç”Ÿæˆé¢†åŸŸçƒ­ç‚¹é—®é¢˜æ€»ç»“ã€‚",
                "hot_topics": [],
                "analyzed_papers": []
            }

        synthesis_context = "Here are the top-rated papers identified as most representative of the field's key problems:\n\n"
        for i, paper in enumerate(top_papers, 1):
            synthesis_context += f"--- Paper {i} ---\n"
            synthesis_context += f"Title: {paper.get('title', 'N/A')}\n"
            synthesis_context += f"Identified Problem: {paper.get('identified_problem', 'N/A')}\n"
            synthesis_context += f"Justification: {paper.get('justification', 'N/A')}\n"
            synthesis_context += f"Weighted Score: {paper.get('weighted_score', 0):.2f}\n\n"

        summary_result = self._summarize_hot_topics(synthesis_context)

        final_result = {
            "summary": summary_result.get("summary", "Could not generate summary."),
            "hot_topics": summary_result.get("hot_topics", []),
            "analyzed_papers": [p["title"] for p in top_papers],
        }

        print("âœ… Workflow 2 completed successfully.")
        return final_result
