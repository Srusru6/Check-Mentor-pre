"""
RAG å¤„ç†å™¨
å®ç°ä»»åŠ¡ 1-5ï¼šè®ºæ–‡åŠ è½½ã€åˆ†å‰²ã€å‘é‡åŒ–ã€å­˜å‚¨å’Œç›¸å…³æ€§åˆ†æ
"""
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

from . import config
from .core_questions import CORE_QUESTIONS, get_question, get_question_weight


class PaperRAGProcessor:
    """è®ºæ–‡ RAG å¤„ç†å™¨"""
    
    def __init__(self, persist_directory: str = None):
        """
        åˆå§‹åŒ– RAG å¤„ç†å™¨
        
        Args:
            persist_directory: Chroma æŒä¹…åŒ–ç›®å½•
        """
        self.persist_directory = persist_directory or config.CHROMA_PERSIST_DIRECTORY
        
        # åˆå§‹åŒ– Embeddings
        # ä½¿ç”¨è‡ªå®šä¹‰embeddingsç±»ï¼Œå› ä¸ºå­¦æ ¡çš„APIä¸langchainä¸å…¼å®¹
        from langchain_core.embeddings import Embeddings
        class CustomEmbeddings(Embeddings):
            def __init__(self, api_key, base_url, model):
                self.api_key = api_key
                self.base_url = base_url
                self.model = model
                from openai import OpenAI
                self.client = OpenAI(api_key=api_key, base_url=base_url)
            
            def embed_documents(self, texts):
                # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¯·æ±‚ä½“è¿‡å¤§ï¼ŒAPIé™åˆ¶æ‰¹æ¬¡å¤§å°ä¸è¶…è¿‡10
                batch_size = 10  # æ¯æ‰¹æœ€å¤š10ä¸ªæ–‡æœ¬
                all_embeddings = []
                
                for i in range(0, len(texts), batch_size):
                    batch_texts = texts[i:i + batch_size]
                    response = self.client.embeddings.create(input=batch_texts, model=self.model)
                    all_embeddings.extend([data.embedding for data in response.data])
                
                return all_embeddings
            
            def embed_query(self, text):
                response = self.client.embeddings.create(input=[text], model=self.model)
                return response.data[0].embedding
        
        self.embeddings = CustomEmbeddings(
            api_key=config.EMBEDDING_API_KEY,
            base_url=config.EMBEDDING_API_BASE,
            model=config.EMBEDDING_MODEL
        )
        
        # åˆå§‹åŒ– LLM
        self.llm = ChatOpenAI(
            model=config.LLM_MODEL,
            openai_api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_API_BASE,
            temperature=0.3
        )
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        
        # å‘é‡æ•°æ®åº“ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.vectorstore: Optional[Chroma] = None
        
    def load_papers_from_directory(self, directory: str, file_type: str = "pdf") -> List[Document]:
        """
        ä»»åŠ¡ 1: ä»ç›®å½•åŠ è½½è®ºæ–‡
        
        Args:
            directory: è®ºæ–‡æ‰€åœ¨ç›®å½•
            file_type: æ–‡ä»¶ç±»å‹ï¼Œæ”¯æŒ "pdf" æˆ– "md" (markdown)
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        print(f"ğŸ“ Loading papers from: {directory}")
        
        if file_type == "md":
            # åŠ è½½ Markdown æ–‡ä»¶
            loader = DirectoryLoader(
                directory,
                glob="**/*.md",
                loader_cls=UnstructuredMarkdownLoader,
                show_progress=True,
                use_multithreading=True
            )
        else:
            # é»˜è®¤åŠ è½½ PDF æ–‡ä»¶
            loader = DirectoryLoader(
                directory,
                glob="**/*.pdf",
                loader_cls=PyPDFLoader,
                show_progress=True,
                use_multithreading=True
            )
        
        documents = loader.load()
        print(f"âœ“ Loaded {len(documents)} pages from {file_type.upper()} papers")
        
        return documents
    
    def load_single_paper(self, paper_path: str, paper_metadata: Dict[str, Any]) -> List[Document]:
        """
        åŠ è½½å•ç¯‡è®ºæ–‡å¹¶æ·»åŠ å…ƒæ•°æ®ï¼ˆè‡ªåŠ¨è¯†åˆ« PDF æˆ– Markdownï¼‰
        
        Args:
            paper_path: è®ºæ–‡æ–‡ä»¶è·¯å¾„ï¼ˆPDF æˆ– Markdownï¼‰
            paper_metadata: è®ºæ–‡å…ƒæ•°æ®ï¼ˆid, title, authors, yearç­‰ï¼‰
            
        Returns:
            å¸¦å…ƒæ•°æ®çš„æ–‡æ¡£åˆ—è¡¨
        """
        print(f"ğŸ“„ Loading paper: {paper_metadata.get('title', 'Unknown')}")
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½å™¨
        if paper_path.lower().endswith('.md'):
            loader = UnstructuredMarkdownLoader(paper_path)
        else:
            loader = PyPDFLoader(paper_path)
        
        documents = loader.load()
        
        # ä¸ºæ¯ä¸ªé¡µé¢æ·»åŠ è®ºæ–‡å…ƒæ•°æ®
        for doc in documents:
            doc.metadata.update(paper_metadata)
        
        print(f"âœ“ Loaded {len(documents)} pages/sections")
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        ä»»åŠ¡ 2: å°†æ–‡æ¡£åˆ†å‰²æˆå—
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            åˆ†å‰²åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        print(f"âœ‚ï¸  Splitting {len(documents)} documents into chunks...")
        
        chunks = self.text_splitter.split_documents(documents)
        
        print(f"âœ“ Created {len(chunks)} chunks")
        return chunks
    
    def create_vectorstore(self, chunks: List[Document]) -> Chroma:
        """
        ä»»åŠ¡ 3-4: å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° Chroma
        
        Args:
            chunks: æ–‡æ¡£å—åˆ—è¡¨
            
        Returns:
            Chroma å‘é‡æ•°æ®åº“å®ä¾‹
        """
        print(f"ğŸ”¢ Creating vector embeddings and storing in Chroma...")
        print(f"   Persist directory: {self.persist_directory}")
        
        # æ¸…ç†æ–‡æ¡£å†…å®¹ï¼šç§»é™¤ç©ºæ–‡æœ¬å’Œç‰¹æ®Šå­—ç¬¦
        cleaned_chunks = []
        for chunk in chunks:
            # ç¡®ä¿ page_content æ˜¯å­—ç¬¦ä¸²ä¸”ä¸ä¸ºç©º
            if hasattr(chunk, 'page_content') and isinstance(chunk.page_content, str):
                content = chunk.page_content.strip()
                if content and len(content) > 0:
                    # ç§»é™¤å¯èƒ½å¯¼è‡´é—®é¢˜çš„å­—ç¬¦
                    content = content.replace('\x00', '')  # ç§»é™¤ç©ºå­—ç¬¦
                    if content:
                        # æ‰‹åŠ¨è¿‡æ»¤å¤æ‚çš„metadata
                        metadata = chunk.metadata if hasattr(chunk, 'metadata') else {}
                        filtered_metadata = {}
                        for key, value in metadata.items():
                            if isinstance(value, (str, int, float, bool)) or value is None:
                                filtered_metadata[key] = value
                            # è·³è¿‡åˆ—è¡¨å’Œå…¶ä»–å¤æ‚ç±»å‹
                        
                        # åˆ›å»ºæ–°çš„ Document å¯¹è±¡with cleaned content
                        from langchain_core.documents import Document
                        cleaned_chunk = Document(
                            page_content=content,
                            metadata=filtered_metadata
                        )
                        cleaned_chunks.append(cleaned_chunk)
                else:
                    print(f"âš ï¸  Skipping chunk with empty content")
            else:
                print(f"âš ï¸  Skipping chunk with invalid page_content: type={type(chunk.page_content)}")
        
        print(f"   Cleaned: {len(cleaned_chunks)}/{len(chunks)} valid chunks")
        
        if not cleaned_chunks:
            raise ValueError("No valid chunks after cleaning!")
        
        # åˆ›å»ºæˆ–åŠ è½½å‘é‡æ•°æ®åº“
        self.vectorstore = Chroma.from_documents(
            documents=cleaned_chunks,
            embedding=self.embeddings,
            collection_name=config.CHROMA_COLLECTION_NAME,
            persist_directory=self.persist_directory
        )
        
        print(f"âœ“ Vector store created with {len(cleaned_chunks)} embeddings")
        return self.vectorstore
    
    def load_vectorstore(self) -> Chroma:
        """
        åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
        
        Returns:
            Chroma å‘é‡æ•°æ®åº“å®ä¾‹
        """
        print(f"ğŸ“š Loading existing vector store from: {self.persist_directory}")
        
        self.vectorstore = Chroma(
            collection_name=config.CHROMA_COLLECTION_NAME,
            embedding_function=self.embeddings,
            persist_directory=self.persist_directory
        )
        
        print(f"âœ“ Vector store loaded")
        return self.vectorstore
    
    def add_papers_to_vectorstore(self, chunks: List[Document]):
        """
        å‘ç°æœ‰å‘é‡åº“æ·»åŠ æ–°è®ºæ–‡
        
        Args:
            chunks: æ–°çš„æ–‡æ¡£å—åˆ—è¡¨
        """
        if self.vectorstore is None:
            self.load_vectorstore()
        
        print(f"â• Adding {len(chunks)} new chunks to vector store...")
        self.vectorstore.add_documents(chunks)
        print(f"âœ“ Added successfully")
    
    def summarize_paper(self, paper_id: str, paper_text: str = None) -> Dict[str, Any]:
        """
        ä»»åŠ¡ 1 (æ‰©å±•): æ€»ç»“å•ç¯‡è®ºæ–‡
        
        Args:
            paper_id: è®ºæ–‡ ID
            paper_text: è®ºæ–‡å…¨æ–‡ï¼ˆå¯é€‰ï¼Œå¦‚ä¸æä¾›åˆ™ä»å‘é‡åº“æ£€ç´¢ï¼‰
            
        Returns:
            åŒ…å«æ€»ç»“çš„å­—å…¸
        """
        print(f"ğŸ“ Summarizing paper: {paper_id}")
        
        # å¦‚æœæ²¡æœ‰æä¾›è®ºæ–‡å…¨æ–‡ï¼Œä»å‘é‡åº“æ£€ç´¢ç›¸å…³å†…å®¹
        if paper_text is None and self.vectorstore is not None:
            # ä¼˜å…ˆæ£€ç´¢è®ºæ–‡ä¸»è¦å†…å®¹ï¼ˆabstract, introductionç­‰ï¼‰ï¼Œé¿å…ä½œè€…ç®€ä»‹
            # å…ˆå°è¯•è·å–é«˜è´¨é‡å—ï¼ˆæ’é™¤bioç›¸å…³å†…å®¹ï¼‰
            main_content_results = self.vectorstore.similarity_search(
                query="research methods findings results conclusion abstract introduction",
                k=20,
                filter={"id": paper_id}  # ä½¿ç”¨æ­£ç¡®çš„å…ƒæ•°æ®å­—æ®µ
            )
            
            # è¿‡æ»¤æ‰å¯èƒ½åŒ…å«ä½œè€…ç®€ä»‹çš„å—
            filtered_chunks = []
            for doc in main_content_results:
                content = doc.page_content.lower()
                # æ’é™¤åŒ…å«ä½œè€…ä¿¡æ¯ã€è‡´è°¢ã€å‚è€ƒæ–‡çŒ®ç­‰éæ ¸å¿ƒå†…å®¹çš„å—
                if not any(keyword in content for keyword in [
                    'author', 'biography', 'bio:', 'acknowledgment', 'reference', 'citation',
                    'corresponding author', 'email:', 'affiliation', 'department'
                ]):
                    filtered_chunks.append(doc)
            
            # å¦‚æœè¿‡æ»¤åå—å¤ªå°‘ï¼Œè¡¥å……ä¸€äº›å—ä½†æ ‡è®°ä¸ºä½è´¨é‡
            if len(filtered_chunks) < 5:
                additional_results = self.vectorstore.similarity_search(
                    query="",
                    k=30,
                    filter={"id": paper_id}  # ä½¿ç”¨æ­£ç¡®çš„å…ƒæ•°æ®å­—æ®µ
                )
                # æ·»åŠ æœªè¢«è¿‡æ»¤æ‰çš„å—
                for doc in additional_results:
                    if doc not in filtered_chunks:
                        filtered_chunks.append(doc)
                        if len(filtered_chunks) >= 10:
                            break
            
            paper_text = "\n\n".join([doc.page_content for doc in filtered_chunks[:15]])  # é™åˆ¶å—æ•°é‡
        
        # å¦‚æœä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œè¿”å›å¤±è´¥çŠ¶æ€
        if not paper_text or len(paper_text.strip()) < 100:
            return {
                "paper_id": paper_id,
                "summary": "Unable to generate summary: insufficient paper content available",
                "generated_at": datetime.now().isoformat(),
                "status": "failed",
                "reason": "insufficient_content"
            }
        
        # åˆ›å»ºæ€»ç»“ prompt
        summary_prompt = ChatPromptTemplate.from_messages([
            ("system", "You are an expert academic paper summarizer. Provide concise and accurate summaries. If the content is insufficient or appears to be about author biography rather than research content, clearly state this limitation."),
            ("user", """Please summarize the following research paper in English. Include:
1. Main research question/objective
2. Key methods and approaches
3. Main findings and contributions
4. Significance and impact

Paper content:
{paper_content}

Provide a structured summary in 200-300 words. If this appears to be author biography or insufficient research content, please note this clearly.""")
        ])
        
        # ç”Ÿæˆæ€»ç»“
        chain = summary_prompt | self.llm
        response = chain.invoke({"paper_content": paper_text[:6000]})  # å¢åŠ é•¿åº¦é™åˆ¶
        
        summary_text = response.content.strip()
        
        # æ£€æµ‹å ä½æ–‡æœ¬æˆ–ä½è´¨é‡æ€»ç»“
        placeholder_indicators = [
            "please provide", "i need more", "insufficient", "cannot summarize",
            "not enough information", "unable to", "content appears to be"
        ]
        
        is_placeholder = any(indicator in summary_text.lower() for indicator in placeholder_indicators)
        
        # å¦‚æœæ£€æµ‹åˆ°å ä½æ–‡æœ¬ï¼Œå°è¯•é‡è¯•ä¸€æ¬¡ä½¿ç”¨æ›´å¤šä¸Šä¸‹æ–‡
        if is_placeholder and len(filtered_chunks) > 15:
            print("âš ï¸ Detected placeholder summary, retrying with more context...")
            extended_text = "\n\n".join([doc.page_content for doc in filtered_chunks[:25]])
            response = chain.invoke({"paper_content": extended_text[:8000]})
            summary_text = response.content.strip()
            
            # å†æ¬¡æ£€æŸ¥
            is_placeholder = any(indicator in summary_text.lower() for indicator in placeholder_indicators)
        
        summary = {
            "paper_id": paper_id,
            "summary": summary_text,
            "generated_at": datetime.now().isoformat(),
            "status": "success" if not is_placeholder else "warning",
            "chunks_used": len(filtered_chunks) if 'filtered_chunks' in locals() else 0,
            "content_quality": "high" if not is_placeholder else "low"
        }
        
        print(f"âœ“ Summary generated (status: {summary['status']})")
        return summary
    
    def analyze_paper_relevance(
        self, 
        paper_id: str, 
        question_key: str,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        ä»»åŠ¡ 5: åˆ†æè®ºæ–‡ä¸ç‰¹å®šé—®é¢˜çš„ç›¸å…³æ€§
        
        Args:
            paper_id: è®ºæ–‡ ID
            question_key: æ ¸å¿ƒé—®é¢˜çš„é”®å€¼
            top_k: æ£€ç´¢çš„æ–‡æœ¬å—æ•°é‡
            
        Returns:
            ç›¸å…³æ€§åˆ†æç»“æœ
        """
        if self.vectorstore is None:
            raise ValueError("Vector store not initialized. Please load or create it first.")
        
        # è·å–é—®é¢˜æ–‡æœ¬
        question = get_question(question_key, language="en")
        question_weight = get_question_weight(question_key)
        
        print(f"ğŸ” Analyzing relevance for question: {question_key}")
        
        # æ£€ç´¢ç›¸å…³æ–‡æœ¬å—ï¼ˆä»…é™è¯¥è®ºæ–‡ï¼‰- æ”¹è¿›æŸ¥è¯¢ä»¥ä¼˜å…ˆè·å–ç ”ç©¶å†…å®¹
        # ä½¿ç”¨æ›´å…·ä½“çš„æŸ¥è¯¢ï¼Œé¿å…åŒ¹é…å‚è€ƒæ–‡çŒ®
        retriever = self.vectorstore.as_retriever(
            search_kwargs={
                "k": top_k,
                "filter": {"id": paper_id}  # ä½¿ç”¨æ­£ç¡®çš„å…ƒæ•°æ®å­—æ®µ
            }
        )
        
        research_queries = [
            f"{question} research methods results analysis",
            f"{question} experiment theory findings",
            f"{question} abstract introduction conclusion"
        ]
        
        all_relevant_chunks = []
        seen_contents = set()
        
        for query in research_queries:
            try:
                chunks = retriever.invoke(query)
                for chunk in chunks:
                    content = chunk.page_content.strip()
                    # å»é‡å¹¶è¿‡æ»¤æ˜æ˜¾æ˜¯å‚è€ƒæ–‡çŒ®çš„å†…å®¹
                    if content not in seen_contents and len(content) > 50:
                        # è¿‡æ»¤æ‰æ˜æ˜¾çš„å‚è€ƒæ–‡çŒ®æ¡ç›®
                        if not (content.count('et al.') > 2 or content.startswith(('1.', '2.', '3.', '['))):
                            all_relevant_chunks.append(chunk)
                            seen_contents.add(content)
                            if len(all_relevant_chunks) >= top_k:
                                break
                if len(all_relevant_chunks) >= top_k:
                    break
            except Exception as e:
                print(f"âš ï¸ Query failed: {query}, error: {e}")
                continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ç ”ç©¶å†…å®¹ï¼Œå›é€€åˆ°åŸå§‹æŸ¥è¯¢
        if len(all_relevant_chunks) < 3:
            print("âš ï¸ Insufficient research content found, falling back to general query")
            fallback_chunks = retriever.invoke(question)
            for chunk in fallback_chunks:
                content = chunk.page_content.strip()
                if content not in seen_contents and len(content) > 50:
                    all_relevant_chunks.append(chunk)
                    seen_contents.add(content)
                    if len(all_relevant_chunks) >= top_k:
                        break
        
        relevant_chunks = all_relevant_chunks[:top_k]
        
        # åŒæ—¶è·å–ç›¸ä¼¼åº¦åˆ†æ•°
        results_with_scores = self.vectorstore.similarity_search_with_score(
            query=question,
            k=top_k,
            filter={"id": paper_id}  # ä½¿ç”¨æ­£ç¡®çš„å…ƒæ•°æ®å­—æ®µ
        )
        
        # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆChroma è¿”å›çš„æ˜¯è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼ï¼‰
        if results_with_scores:
            avg_distance = sum(score for _, score in results_with_scores) / len(results_with_scores)
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (0-1)ï¼Œè·ç¦»è¶Šå°åˆ†æ•°è¶Šé«˜
            similarity_score = max(0, min(1, 1 - avg_distance / 2))
        else:
            similarity_score = 0.0
        
        # åˆ†æå—çš„æ¥æºç±»å‹ï¼Œç”¨äºç¡®å®šæ¨ç†æ°´å¹³
        chunk_sources = []
        for doc in relevant_chunks:
            content = doc.page_content.lower()
            # æ”¹è¿›å…³é”®è¯åŒ¹é…ï¼Œä¼˜å…ˆè¯†åˆ«ç ”ç©¶å†…å®¹
            has_research_keywords = any(keyword in content for keyword in [
                'abstract', 'introduction', 'method', 'methods', 'result', 'results', 
                'conclusion', 'conclusions', 'experiment', 'experimental', 'theory',
                'analysis', 'discussion', 'figure', 'fig.', 'table', 'algorithm'
            ])
            has_bio_keywords = any(keyword in content for keyword in [
                'author', 'biography', 'bio:', 'acknowledgment', 'acknowledgements',
                'affiliation', 'department', 'corresponding author', 'email:', 'funding'
            ])
            has_reference_keywords = any(keyword in content for keyword in [
                'et al.', 'phys. rev.', 'nature', 'science', 'arxiv:', 'doi:', 'vol.', 'pp.'
            ])
            
            if has_research_keywords:
                chunk_sources.append("research_content")
            elif has_bio_keywords:
                chunk_sources.append("author_bio")
            elif has_reference_keywords:
                chunk_sources.append("references")
            else:
                chunk_sources.append("other")
        
        # ç¡®å®šæ¨ç†æ°´å¹³ - æ”¹è¿›é€»è¾‘
        research_count = chunk_sources.count("research_content")
        bio_count = chunk_sources.count("author_bio")
        ref_count = chunk_sources.count("references")
        
        if research_count > 0:
            inference_level = "direct_evidence"
        elif bio_count > 0 and research_count == 0:
            inference_level = "inferred_from_bio"
        elif ref_count > 0 and research_count == 0 and bio_count == 0:
            inference_level = "inferred_from_references"
        else:
            inference_level = "weak_inference"
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        context = "\n\n---\n\n".join([doc.page_content for doc in relevant_chunks[:3]])
        
        # æ„å»ºå»é‡çš„chunks_usedåˆ—è¡¨
        seen_contents = set()
        unique_chunks_used = []
        for i, doc in enumerate(relevant_chunks[:3]):
            content = doc.page_content
            if content not in seen_contents:
                seen_contents.add(content)
                score = next((score for d, score in results_with_scores if d.page_content == content), None)
                unique_chunks_used.append({
                    "preview": content[:200] + "..." if len(content) > 200 else content,
                    "similarity_score": score,
                    "source_type": chunk_sources[i] if i < len(chunk_sources) else "unknown"
                })
        
        # é™åˆ¶ä¸ºæœ€å¤š3ä¸ªå”¯ä¸€å—
        unique_chunks_used = unique_chunks_used[:3]
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """You are an expert academic reviewer specializing in analyzing research papers and professor profiles. 
Your task is to analyze how well a research paper contributes to understanding a professor's research interests and the broader field.

Provide a comprehensive but accessible analysis that is scientifically rigorous yet easy to understand. 
Focus on key insights, practical implications, and educational value.

If the content appears to be from author biography rather than research content, you may still make reasonable inferences about potential research directions, but clearly indicate this uncertainty.

Provide a JSON response with the following structure:
{{
  "score": <float 0-1, how relevant this paper is to the question>,
  "confidence": <float 0-1, your confidence in this assessment>,
  "evidence": "<key evidence from the paper, explained clearly>",
  "reasoning": "<comprehensive explanation that is simple but scientifically accurate>",
  "inference_level": "<direct_evidence|inferred_from_bio|weak_inference>"
}}"""),
            ("user", """Question: {question}

Relevant paper content:
{context}

Based on the above content, evaluate how well this paper helps answer the question. 
Consider the similarity score from vector search: {similarity_score:.3f}

Content source analysis: {inference_level_desc}

Provide your analysis in JSON format. Make your explanation comprehensive yet accessible, scientifically rigorous but not overly technical. If using biographical information, clearly note the inferential nature of your analysis.""")
        ])
        
        # åˆ›å»º JSON è§£æå™¨
        json_parser = JsonOutputParser()
        
        # åˆ›å»ºé“¾
        chain = analysis_prompt | self.llm | json_parser
        
        # å‡†å¤‡æ¨ç†æ°´å¹³æè¿°
        inference_descriptions = {
            "direct_evidence": "Content appears to be from research sections (abstract, methods, results, etc.)",
            "inferred_from_bio": "Content appears to be from author biography - analysis is inferential",
            "weak_inference": "Content source unclear - analysis has higher uncertainty"
        }
        inference_level_desc = inference_descriptions.get(inference_level, "Content source analysis unavailable")
        
        # æ‰§è¡Œåˆ†æ
        try:
            analysis = chain.invoke({
                "question": question,
                "context": context,
                "similarity_score": similarity_score,
                "inference_level_desc": inference_level_desc
            })
            
            # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
            result = {
                "score": float(analysis.get("score", 0)),
                "confidence": float(analysis.get("confidence", 0)),
                "evidence": analysis.get("evidence", ""),
                "reasoning": analysis.get("reasoning", ""),
                "inference_level": analysis.get("inference_level", inference_level),
                "similarity_score": similarity_score,
                "question_weight": question_weight,
                "chunks_analyzed": len(relevant_chunks),
                "chunks_used": unique_chunks_used
            }
            
        except Exception as e:
            print(f"âš ï¸  Error in LLM analysis: {e}")
            # API è°ƒç”¨å¤±è´¥ï¼Œè¿”å›æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
            result = {
                "score": 0,
                "confidence": 0,
                "evidence": "N/A",
                "reasoning": f"APIError: LLM analysis failed. Error: {str(e)}",
                "inference_level": "error",
                "similarity_score": similarity_score,
                "question_weight": question_weight,
                "chunks_analyzed": len(relevant_chunks),
                "chunks_used": unique_chunks_used
            }
        
        print(f"âœ“ Relevance score: {result['score']:.3f}, Confidence: {result['confidence']:.3f}, Inference: {result['inference_level']}")
        return result
    
    def analyze_all_questions_for_paper(self, paper_id: str) -> Dict[str, Dict[str, Any]]:
        """
        åˆ†æè®ºæ–‡å¯¹æ‰€æœ‰æ ¸å¿ƒé—®é¢˜çš„ç›¸å…³æ€§
        
        Args:
            paper_id: è®ºæ–‡ ID
            
        Returns:
            å®Œæ•´çš„ç›¸å…³æ€§åˆ†æç»“æœ
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Analyzing paper {paper_id} for all core questions")
        print(f"{'='*70}\n")
        
        relevance_analysis = {}
        
        for question_key in CORE_QUESTIONS.keys():
            analysis = self.analyze_paper_relevance(paper_id, question_key)
            relevance_analysis[question_key] = analysis
        
        print(f"\nâœ“ Complete analysis for paper {paper_id}")
        return relevance_analysis
    
    def process_papers_batch(
        self, 
        papers_info: List[Dict[str, Any]], 
        file_type: str = "pdf"
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†è®ºæ–‡ï¼ˆå®Œæ•´æµç¨‹ï¼šåŠ è½½ -> åˆ†å‰² -> å‘é‡åŒ– -> åˆ†æï¼‰
        
        Args:
            papers_info: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« id, title, å’Œä¸€ä¸ªåŒ…å«ç»å¯¹è·¯å¾„çš„ 'md_filename'
            file_type: æ–‡ä»¶ç±»å‹ï¼Œæ”¯æŒ "pdf" æˆ– "md" (markdown)
            
        Returns:
            åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        all_chunks = []
        summaries = {}
        analysis_results = {}
        
        # ç¡®å®šæ–‡ä»¶åé”®
        filename_key = "md_filename" if file_type == "md" else "pdf_filename"
        
        # æ­¥éª¤ 1-2: åŠ è½½å¹¶åˆ†å‰²æ‰€æœ‰è®ºæ–‡
        for paper_info in papers_info:
            # ç›´æ¥ä» paper_info è·å–ç»å¯¹è·¯å¾„
            paper_path_str = paper_info.get(filename_key)
            if not paper_path_str:
                print(f"âš ï¸  Skipping paper {paper_info.get('id')} due to missing file path.")
                continue

            paper_path = Path(paper_path_str)
            if not paper_path.exists():
                print(f"âš ï¸  File not found: {paper_path}")
                continue
            
            # åŠ è½½è®ºæ–‡
            documents = self.load_single_paper(str(paper_path), paper_info)
            
            # åˆ†å‰²æ–‡æ¡£
            chunks = self.split_documents(documents)
            all_chunks.extend(chunks)
        
        # æ­¥éª¤ 3-4: åˆ›å»ºå‘é‡åº“
        self.create_vectorstore(all_chunks)
        
        # æ­¥éª¤ 5: å¯¹æ¯ç¯‡è®ºæ–‡è¿›è¡Œç›¸å…³æ€§åˆ†æ
        for paper_info in papers_info:
            paper_id = paper_info['id']
            
            # ç”Ÿæˆæ€»ç»“
            try:
                summary = self.summarize_paper(paper_id)
                summaries[paper_id] = summary
            except Exception as e:
                print(f"âš ï¸  Error summarizing paper {paper_id}: {e}")
                summaries[paper_id] = {"error": str(e)}
            
            # åˆ†æç›¸å…³æ€§
            try:
                analysis = self.analyze_all_questions_for_paper(paper_id)
                analysis_results[paper_id] = analysis
            except Exception as e:
                print(f"âš ï¸  Error analyzing paper {paper_id}: {e}")
                analysis_results[paper_id] = {"error": str(e)}
        
        return {
            "summaries": summaries,
            "analysis_results": analysis_results,
            "total_papers": len(papers_info),
            "total_chunks": len(all_chunks)
        }


# æ¨¡å—æä¾› PaperRAGProcessor ç±»ï¼›å°¾éƒ¨æµ‹è¯•ä»£ç å·²ç§»é™¤ä»¥é¿å…å¯¼å…¥æ—¶å‰¯ä½œç”¨
