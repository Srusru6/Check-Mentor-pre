# Strong Loophole-Free Test of Local Realism

Lynden K. Shalm, $^{1,\dagger}$  Evan Meyer-Scott, $^{2}$  Bradley G. Christensen, $^{3}$  Peter Bierhorst, $^{1}$  Michael A. Wayne, $^{3,4}$  Martin J. Stevens, $^{1}$  Thomas Gerrits, $^{1}$  Scott Glancy, $^{1}$  Deny R. Hamel, $^{5}$  Michael S. Allman, $^{1}$  Kevin J. Coakley, $^{1}$  Shellee D. Dyer, $^{1}$  Carson Hodge, $^{1}$  Adriana E. Lita, $^{1}$  Varun B. Verma, $^{1}$  Camilla Lambrocco, $^{1}$  Edward Tortorici, $^{1}$  Alan L. Migdall, $^{4,6}$  Yanbao Zhang, $^{2}$  Daniel R. Kumor, $^{3}$  William H. Farr, $^{7}$  Francesco Marsili, $^{7}$  Matthew D. Shaw, $^{7}$  Jeffrey A. Stern, $^{7}$  Carlos Abellán, $^{8}$  Waldimar Amaya, $^{8}$  Valerio Pruneri, $^{8,9}$  Thomas Jennewein, $^{2,10}$  Morgan W. Mitchell, $^{8,9}$  Paul G. Kwiat, $^{3}$  Joshua C. Bienfang, $^{4,6}$  Richard P. Mirin, $^{1}$  Emanuel Knill, $^{1}$  and Sae Woo Nam $^{1,‡}$ $^{1}$ National Institute of Standards and Technology, 325 Broadway, Boulder, Colorado 80305, USA  
 $^{2}$ Institute for Quantum Computing and Department of Physics and Astronomy, University of Waterloo, 200 University Avenue West, Waterloo, Ontario, Canada, N2L 3G1  
 $^{3}$ Department of Physics, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801, USA  
 $^{4}$ National Institute of Standards and Technology, 100 Bureau Drive, Gaithersburg, Maryland 20899, USA  
 $^{5}$ Département de Physique et d'Astronomie, Université de Moncton, Moncton, New Brunswick E1A 3E9, Canada  
 $^{6}$ Joint Quantum Institute, National Institute of Standards and Technology and University of Maryland, 100 Bureau Drive, Gaithersburg, Maryland 20899, USA  
 $^{7}$ Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, California 91109, USA  
 $^{8}$ ICFO-Institut de Ciencies Fotoniques, The Barcelona Institute of Science and Technology, 08860 Castelldefels (Barcelona), Spain  
 $^{9}$ ICREA-Institución Catalana de Recerca i Estudis Avançats, 08015 Barcelona, Spain  
 $^{10}$ Quantum Information Science Program, Canadian Institute for Advanced Research, Toronto, Ontario, Canada (Received 10 November 2015; published 16 December 2015)

We present a loophole-free violation of local realism using entangled photon pairs. We ensure that all relevant events in our Bell test are spacelike separated by placing the parties far enough apart and by using fast random number generators and high-speed polarization measurements. A high-quality polarization-entangled source of photons, combined with high-efficiency, low-noise, single-photon detectors, allows us to make measurements without requiring any fair-sampling assumptions. Using a hypothesis test, we compute  $p$  values as small as  $5.9 \times 10^{-9}$  for our Bell violation while maintaining the spacelike separation of our events. We estimate the degree to which a local realistic system could predict our measurement choices. Accounting for this predictability, our smallest adjusted  $p$  value is  $2.3 \times 10^{-7}$ . We therefore reject the hypothesis that local realism governs our experiment.

DOI: 10.1103/PhysRevLett.115.250402

PACS numbers: 03.65.Ud, 42.50.Xa, 42.65.Lm

But if [a hidden variable theory] is local it will not agree with quantum mechanics, and if it agrees with quantum mechanics it will not be local. This is what the theorem says. -JOHN STEWART BELL [1].

Quantum mechanics at its heart is a statistical theory. It cannot, with certainty, predict the outcome of all single events, but instead it predicts probabilities of outcomes. This probabilistic nature of quantum theory is at odds with the determinism inherent in Newtonian physics and relativity, where outcomes can be exactly predicted given sufficient knowledge of a system. Einstein and others felt that quantum mechanics was incomplete. Perhaps quantum

systems are controlled by variables, possibly hidden from us [2], that determine the outcomes of measurements. If we had direct access to these hidden variables, then the outcomes of all measurements performed on quantum systems could be predicted with certainty. The 1927 pilot-wave theory of de Broglie was a first attempt at formulating a hidden variable theory of quantum physics [3]; it was completed in 1952 by Bohm [4,5]. While the pilot-wave theory can reproduce all of the predictions of quantum mechanics, it has the curious feature that hidden variables in one location can instantly change values because of events happening in distant locations. This seemingly violates the locality principle from relativity, which says that objects cannot signal one another faster than the speed of light. In 1935 the nonlocal feature of quantum systems was popularized by Einstein, Podolsky, and Rosen [6], and is something Einstein later referred to as "spooky actions at a distance" [7]. But in 1964 Bell showed that it is impossible to construct a hidden variable theory that obeys locality and simultaneously reproduces all of the predictions of quantum mechanics [8]. Bell's theorem

fundamentally changed our understanding of quantum theory and today stands as a cornerstone of modern quantum information science.

Bell's theorem does not prove the validity of quantum mechanics, but it does allow us to test the hypothesis that nature is governed by local realism. The principle of realism says that any system has preexisting values for all possible measurements of the system. In local realistic theories, these preexisting values depend only on events in the past light cone of the system. Local hidden-variable theories obey this principle of local realism. Local realism places constraints on the behavior of systems of multiple particles—constraints that do not apply to entangled quantum particles. This leads to different predictions that can be tested in an experiment known as a Bell test. In a typical two-party Bell test, a source generates particles and sends them to two distant parties, Alice and Bob. Alice and Bob independently and randomly choose properties of their individual particles to measure. Later, they compare the results of their measurements. Local realism constrains the joint probability distribution of their choices and measurements. The basis of a Bell test is an inequality that is obeyed by local realistic probability distributions but can be violated by the probability distributions of certain entangled quantum particles [8]. A few years after Bell derived his inequality, new forms were introduced by Clauser, Horne, Shimony, and Holt [9], and Clauser and Horne [10] that are simpler to test experimentally.

In a series of landmark experiments, Freedman and Clauser [11] and Aspect, Grangier, Dalibard, and Roger [12-14] demonstrated experimental violations of Bell inequalities using pairs of polarization-entangled photons generated by an atomic cascade. However, due to technological constraints, these Bell tests and those that followed (see [15] for a review) were forced to make additional assumptions to show that local realism was incompatible with their experimental results. A significant violation of Bell's inequality implies that either local realism is false or that one or more of the assumptions made about the experiment are not true; thus, every assumption in an experiment opens a "loophole." No experiment can be absolutely free of all loopholes, but in Ref. [16] a minimal set of assumptions is described that an experiment must make to be considered "loophole free." Here we report a significant, loophole-free, experimental violation of local realism using entangled photon pairs. We use the definition of loophole free as defined in Ref. [16]. In our experiment the only assumptions that remain are those that can never—even in principle—be removed. We present physical arguments and evidence that these remaining assumptions are either true or untestable.

Bell's proof requires that the measurement choice at Alice cannot influence the outcome at Bob (and vice versa). If a signal traveling from Alice cannot reach Bob in the time between Alice's choice and the completion of Bob's

measurement, then there is no way for a local hidden variable constrained by special relativity at Alice to change Bob's outcomes. In this case we say that Alice and Bob are spacelike separated from one another. If an experiment does not have this spacelike separation, then an assumption must be made that local hidden variables cannot signal one another, leading to the "locality" loophole.

Another requirement in a Bell test is that Alice and Bob must be free to make random measurement choices that are physically independent of one another and of any properties of the particles. If this is not true, then a hidden variable could predict the chosen settings in advance and use that information to produce measurement outcomes that violate a Bell inequality. Not fulfilling this requirement opens the "freedom-of-choice" loophole. While this loophole can never, in principle, be closed, the set of hidden variable models that are able to predict the choices can be constrained using spacelike separation. In particular, in experiments that use processes such as cascade emission or parametric down-conversion to create entangled particles, spacelike separation of the measurement choices from the creation event eliminates the possibility that the particles, or any other signal emanating from the creation event, influence the settings. To satisfy this condition, Alice and Bob must choose measurement settings based on fast random events that occur in the short time before a signal traveling at the speed of light from the entangled-photon creation would be able to reach them. But it is fundamentally impossible to conclusively prove that Alice's and Bob's random number generators are independent without making additional assumptions, since their backward light cones necessarily intersect. Instead, it is possible to justify the assumption of measurement independence through a detailed characterization of the physical properties of the random number generators (such as the examination described in Refs. [17,18]).

In any experiment, imperfections could lead to loss, and not all particles will be detected. To violate a Bell inequality in an experiment with two parties, each free to choose between two settings, Eberhard showed that at least  $2/3$  of the particles must be detected [19] if nonmaximally entangled states are used. If the loss exceeds this threshold, then one may observe a violation by discarding events in which at least one party does not detect a particle. This is valid under the assumption that particles were lost in an unbiased manner. However, relying on this assumption opens the "detector" or "fair-sampling" loophole. While the locality and fair-sampling loopholes have been closed individually in different systems [20-24], it has only recently been possible to close all loopholes simultaneously using nitrogen vacancy centers in diamonds [25] and now with entangled photons in our experiment and in the work reported in Ref. [26]. These three experiments also address the freedom-of-choice loophole by spacelike separation.

Fundamentally, a Bell inequality is a constraint on probabilities that are estimated from random data. Determining whether a data set shows violation is a statistical hypothesis-testing problem. It is critical that the statistical analysis does not introduce unnecessary assumptions that create loopholes. A Bell test is divided into a series of trials. In our experiment, during each trial Alice and Bob randomly choose between one of two measurement settings (denoted  $\{a,a^{\prime}\}$  for Alice and  $\{b,b^{\prime}\}$  for Bob) and record either  $a+$  if they observe any detection events or a 0 otherwise. Alice and Bob must define when a trial is happening using only locally available information; otherwise, additional loopholes are introduced. At the end of the experiment, Alice and Bob compare the results they obtained on a trial-by-trial basis.

Our Bell test uses a version of the Clauser-Horne inequality [10,19,27] where, according to local realism,

$$
P (+ + | a b) \leq P (+ 0 | a b ^ {\prime}) + P (0 + | a ^ {\prime} b) + P (+ + | a ^ {\prime} b ^ {\prime}). \tag {1}
$$

The terms  $P(+ + |ab)$  and  $P(+ + |a'b')$  correspond to the probability that both Alice and Bob record detection events  $(++)$  when they choose the measurement settings  $ab$  or

$a^\prime b^\prime$ , respectively. Similarly, the terms  $P(+0|ab^{\prime})$  and  $P(0 + |a^{\prime}b)$  are the probabilities that only Alice or Bob records an event for settings  $ab^{\prime}$  and  $a^\prime b$ , respectively. A local realistic model can saturate this inequality; however, the probability distributions of entangled quantum particles can violate it.

To quantify our Bell violation, we construct a hypothesis test based on the inequality in Eq. (1). The null hypothesis we test is that the measured probability distributions in our experiment are constrained by local realism. Our evidence against this null hypothesis of local realism is quantified in a  $p$  value that we compute from our measured data using a test statistic. Our test statistic takes all of the measured data from Alice's and Bob's trials and summarizes them into a single number (see Supplemental Material [28] for further details). The  $p$  value is then the maximum probability that our experiment, if it is governed by local realism, could have produced a value of the test statistic that is at least as large as the observed value [38]. Smaller  $p$  values can be interpreted as stronger evidence against this hypothesis. These  $p$  values can also be used as certificates for cryptographic applications, such as random number generation, that rely on a Bell test [24,39]. We use a martingale binomial technique from Ref. [27] for computing the  $p$

![](images/6e184945d016532bff33a319ebd44d5a1344e8a3784ceaf126112f30187eafd7.jpg)  
FIG. 1 (color online). Schematic of the entangled photon source. A pulsed 775-nm-wavelength Ti:sapphire picosecond mode-locked laser running at a 79.3-MHz repetition rate is used as both a clock and a pump in our setup. A fast photodiode (FPD) and divider circuit are used to generate the synchronization signal that is distributed to Alice and Bob. A polarization-maintaining single-mode fiber (SMF) then acts as a spatial filter for the pump. After exiting the SMF, a polarizer and half-wave plate (HWP) set the pump polarization. To generate entanglement, a periodically poled potassium titanyl phosphate (PPKTP) crystal designed for type-II phase matching is placed in a polarization-based Mach-Zehnder interferometer formed using a series of HwPs and three beam displacers (BD). At BD1 the pump beam is split into two paths (1 and 2): The horizontal  $(H)$  component of polarization of the pump translates laterally in the  $x$  direction, while the vertical  $(V)$  component of polarization passes straight through. Tilting BD1 sets the phase,  $\phi$ , of the interferometer to 0. After BD1 the pump state is  $(\cos(16^{\circ})|H_1\rangle + \sin(16^{\circ})|V_2\rangle)$ . To address the polarization of the paths individually, semicircular wave plates are used. A HWP in path 2 rotates the polarization of the pump from vertical to horizontal. A second HWP at  $0^{\circ}$  is inserted into path 1 to keep the path lengths of the interferometer balanced. The pump is focused at two spots in the crystal, and photon pairs at a wavelength of  $1550~\mathrm{nm}$  are generated in either path 1 or 2 through the process of spontaneous parametric down-conversion. After the crystal, BD2 walks the  $V$ -polarized signal photons down in the  $y$  direction  $(V_{1a}$  and  $V_{2a})$ , while the  $H$ -polarized idler photons pass straight through  $(H_{1b}$  and  $H_{2b})$ . The  $x-y$  view shows the resulting locations of the four beam paths. HwPs at  $45^{\circ}$  correct the polarization, while HwPs at  $0^{\circ}$  provide temporal compensation. BD3 then completes the interferometer by recombining paths 1 and 2 for the signal and idler photons. The two down-conversion processes interfere with one another, creating the entangled state in Eq. (2). A high-purity silicon wafer with an antireflection coating is used to filter out the remaining pump light. The idler (signal) photons are coupled into a SMF and sent to Alice (Bob).

value that makes no assumptions about the distribution of events and does not require that the data be independent and identically distributed [40] as long as appropriate stopping criteria are determined in advance.

In our experiment, the source creates polarization-entangled pairs of photons and distributes them to Alice and Bob, located in distant labs. At the source location, a mode-locked Ti:sapphire laser running at a repetition rate of approximately  $79.3\mathrm{MHz}$  produces picosecond pulses centered at a wavelength of  $775\mathrm{nm}$  as shown in Fig. 1. These laser pulses pump an apodized periodically poled potassium titanyl phosphate (PPKTP) crystal to produce photon pairs at a wavelength of  $1550\mathrm{nm}$  via the process of spontaneous parametric down-conversion [41]. The downconversion system was designed using the tools available in Ref. [42]. The PPKTP crystal is embedded in the middle of a polarization-based Mach-Zehnder interferometer that enables high-quality polarization-entangled states to be generated [43]. Rotating the polarization analyzer angles at Alice and Bob, we measure the visibility of coincidence detections for a maximally entangled state to be  $0.999 \pm 0.001$  in the horizontal (vertical) polarization basis and  $0.996 \pm 0.001$  in the diagonal (antidiagonal) polarization basis (see Ref. [44] for information about the reported uncertainties). The entangled photons are then coupled into separate single-mode optical fibers with one photon sent to Alice and the other to Bob. Alice, Bob, and the source are positioned at the vertices of a nearly right-angle triangle. Due to constraints in the building layout, the photons travel to Alice and Bob in fiber optic cables that are not positioned along their direct lines of sight. While the photons are in flight toward Alice and Bob, their random number generators each choose a measurement setting. Each choice is completed before information about the entangled state, generated at the PPKTP crystal, could possibly reach the random number generators. When the photons arrive at Alice and Bob, they are launched into free space, and each photon passes through a Pockels cell and polarizer that perform the polarization measurement chosen by the random number generators as shown in Fig. 2. After the polarizer, the photons are coupled back into a single-mode fiber and sent to superconducting nanowire single-photon detectors, each with a detection efficiency of  $91 \pm 2\%$  [45]. The detector signal is then amplified and sent to a time tagger where the arrival time is recorded. We assume the measurement outcome is fixed when it is recorded by the time tagger, which happens before information about the other party's setting choice could possibly arrive, as shown in Fig. 3(b).

Alice and Bob have system detection efficiencies of  $74.7 \pm 0.3\%$  and  $75.6 \pm 0.3\%$ , respectively. We measure this system efficiency using the method outlined by Klyshko [46]. Background counts from blackbody radiation and room lights reduce our observed violation of the Bell inequality. Every time a background count is observed,

![](images/4b756d818bb99f7782bb21f9b708d39ca8a178e00c6ed0a00f25e2083e0b557d.jpg)  
FIG. 2 (color online). Receiver station setup for Alice and Bob. A photon arrives from the source. Two half-wave plates (HWP), a quarter-wave plate (QWP), a Pockels cell (PC), and two plate polarizers together act to measure the polarization state of the incoming photon. The polarization projection is determined by a random bit from applying an XOR operation to the outputs of two random number generators (RNG1 and RNG2) with predetermined pseudorandom bits (RNG3). If the random bit is 0, corresponding to measurement setting  $a$  ( $b$ ) for Alice (Bob), the Pockels cell remains off. If the random bit is 1, corresponding to measurement setting  $a'$  ( $b'$ ) for Alice (Bob), then a voltage is applied to the Pockels cell that rotates the polarization of the photons using a fast electro-optic effect. The two plate polarizers have a combined contrast ratio  $>7000:1$ . The photons are coupled back into a single-mode fiber (SMF) and detected using a superconducting nanowire single-photon detector (SNSPD). The signal is amplified and sent to a time-tagging unit where the arrival time of the event is recorded. The time tagger also records the measurement setting, the synchronization signal, and a one pulse-per-second signal from a global positioning system (GPS). The pulse-per-second signal provides an external time reference that helps align the time tags Alice and Bob record. A 10-MHz oscillator synchronizes the internal clocks on Alice's and Bob's time taggers. The synchronization pulse from the source is used to trigger the measurement basis choice.

it counts as a detection event for only one party. These background counts increase the heralding efficiency required to close the detector loophole above  $2/3$  [19]. To reduce the number of background counts, the only detection events considered are those that occur within a window of approximately 625 ps at Alice and 781 ps at Bob, centered around the expected arrival times of photons from the source. The probability of observing a background count during a single window is  $8.9 \times 10^{-7}$  for Alice and  $3.2 \times 10^{-7}$  for Bob, while the probability that a single pump pulse down-converts into a photon pair is  $\approx 5 \times 10^{-4}$ . These background counts in our system raise the efficiency needed to violate a Bell inequality from  $2/3$  to  $72.5\%$ . Given our system detection efficiencies, our entangled photon production rates, entanglement visibility, and the number of background counts, we numerically determine the entangled state and measurement settings for Alice and Bob that should give the largest Bell violation for our setup. The optimal state is not maximally entangled [19] and is given by

$$
| \psi \rangle = 0. 9 6 1 | H _ {A} H _ {B} \rangle + 0. 2 7 6 | V _ {A} V _ {B} \rangle , \tag {2}
$$

![](images/d268889413c5377cb2a6f789de246052fb47530d3e89ba2b6f582f32518de568.jpg)

![](images/f477d81de04c52a8da0a790b0f930aff0ec66f1ed058a28ac224027b1921ed13.jpg)  
FIG. 3 (color online). Minkowski diagrams for the spacetime events related to Alice (A) and the source (S) and Bob (B) and the source (a), and Alice and Bob (b). All light cones are shaded blue. Due to the geometry of Alice, Bob, and the source, more than one spacetime diagram is required. In (a) the random number generators (RNGs) at Alice and Bob must finish picking a setting outside the light cone of the birth of an entangled photon pair. A total of 15 pump pulses have a chance of down-converting into an entangled pair of photons each time the Pockels cells are on. The events related to pulses 1 through 11 are spacelike separated. As shown in (b), pulses 12 through 15 are not spacelike separated as the measurement is finished by Alice and Bob after information about the other party's measurement setting could have arrived. In our experiment, the events related to pulse 6 are the furthest outside of all relevant light cones.

where  $H(V)$  denotes horizontal (vertical) polarization, and  $A$  and  $B$  correspond to Alice's and Bob's photons, respectively. From the simulation we also determine that Alice's optimal polarization measurement angles, relative to a vertical polarizer, are  $\{a = 4.2^{\circ}, a' = -25.9^{\circ}\}$ , while Bob's are  $\{b = -4.2^{\circ}, b' = 25.9^{\circ}\}$ .

Synchronization signals enable Alice and Bob to define trials based only on local information. The synchronization

signal runs at a frequency of  $99.1\mathrm{kHz}$ , allowing Alice and Bob to perform 99,100 trials/s (79.3 MHz/800). This trial frequency is limited by the rate at which the Pockels cells can be stably driven. When the Pockels cells are triggered, they stay on for  $\approx 200$  ns. This is more than 15 times longer than the 12.6-ns pulse-to-pulse separation of the pump laser. Therefore, photons generated by the source can arrive in one of 15 slots while both Alice's and Bob's Pockels cells are on. Since the majority of the photon pulses arriving in these 15 slots satisfy the spacelike separation constraints, it is possible to aggregate multiple adjacent pulses to increase the event rate and statistical significance of the Bell violation. However, including too many pulses will cause one or more of the spacelike separation constraints to be violated. Because the probability per pulse of generating an entangled photon pair is so low, given that one photon has already arrived, the chance of getting a second event in the same Pockels cell window is negligible ( $< 1\%$ ).

Alice and Bob each have three different sources of random bits that undergo an XOR operation together to produce their random measurement decisions (for more information see Supplemental Material [28]). The first source is based on measuring optical phase diffusion in a gain-switched laser that is driven above and below the lasing threshold. A new bit is produced every 5 ns by comparing adjacent laser pulses [17]. Each bit is then processed through an XOR gate with all past bits that have been produced (for more details see Supplemental Material [28]). The second source is based on sampling the amplitude of an optical pulse at the single-photon level in a short temporal interval. This source produces a bit on demand and is triggered by the synchronization signal. Finally, Alice and Bob each have a different predetermined pseudorandom source that is composed of various popular culture movies and TV shows, as well as the digits of  $\pi$ , processed together through an XOR gate. Suppose that a local-realistic system, with the goal of producing violation of the Bell inequality, was able to manipulate the properties of the photons emitted by the entanglement source before each trial. Provided that the randomness sources correctly extract their bits from the underlying processes of phase diffusion, optical amplitude sampling, and the production of cultural artifacts (such as the movie Back to the Future), this powerful local realistic system would be required to predict the outcomes of all of these processes well in advance of the beginning of each trial to achieve its goal. Such a model would have elements of superdeterminism—the fundamentally untestable idea that all events in the Universe are preordained.

Over the course of two days, we took a total of six data runs with differing configurations of the experimental setup. Here we report the results from the final data set that recorded data for 30 minutes (see Supplemental Material [28] for descriptions and results from all data

TABLE I. The  $p$ -value results for different numbers of aggregate pulses. Here  $N(+|ab)$  refers to the number of times Alice and Bob both detect a photon with settings  $a$  and  $b$ , respectively. Before analyzing the data, a stopping criterion  $N_{\mathrm{stop}}$  was chosen. This stopping criterion refers to the total number of events considered that have the settings and outcomes specified by the terms in Eq. (1),  $N_{\mathrm{stop}} = N(+|ab) + N(+0|ab') + N(0 + |a'b) + N(+|a'b')$ . After this number of trials, the  $p$  value is computed and the remaining trials discarded. Such predetermined stopping criteria are necessary for the hypothesis test we use (see Supplemental Material [28] for more details). The total trials include all trials up to the stopping criteria regardless of whether a photon is detected. The adjusted  $p$  value accounts for the excess predictability we estimate from measurements of one of our random number generators. As discussed in the text, the time difference between Bob finishing his measurement and the earliest time at which information about Alice's measurement choice could arrive at Bob sets the margin of timing error that can be tolerated and still have all events guaranteed to be spacelike separated. We also give the minimum distance between each party and its boundary line [shown in Fig. 4(a)] that guarantees satisfaction of the spacelike separation constraints. In Ref. [28] the frequencies of each combination of setting choice for five aggregate pulses is reported.

<table><tr><td>Aggregate pulses</td><td>N(++|ab)</td><td>NSTop</td><td>Total trials</td><td>p value</td><td>Adjusted p value</td><td>Timing margin (ns)</td><td>Minimum distance (m)</td></tr><tr><td>1</td><td>1257</td><td>2376</td><td>175,654,992</td><td>2.5 × 10-3</td><td>5.9 × 10-3</td><td>63.5 ± 3.7</td><td>9.2</td></tr><tr><td>3</td><td>3800</td><td>7211</td><td>175,744,824</td><td>2.4 × 10-6</td><td>2.4 × 10-5</td><td>50.9 ± 3.7</td><td>7.3</td></tr><tr><td>5</td><td>6378</td><td>12127</td><td>177,358,351</td><td>5.9 × 10-9</td><td>2.3 × 10-7</td><td>38.3 ± 3.7</td><td>5.4</td></tr><tr><td>7</td><td>8820</td><td>16979</td><td>177,797,650</td><td>2.0 × 10-7</td><td>9.2 × 10-6</td><td>25.7 ± 3.7</td><td>3.5</td></tr></table>

sets). This is the data set where the experiment was most stable and best aligned; small changes in coupling efficiency and the stability of the Pockels cells can lead to large changes in the observed violation. The events corresponding to the sixth pulse out of the 15 possible pulses per trial are the farthest outside all the relevant light cones. Thus, we say these events are the most spacelike separated. To increase our data rate, we aggregate multiple pulses centered around pulse number 6. We consider different Bell tests using a single pulse (number 6), three pulses (pulses 5, 6, and 7), five pulses (pulses 4 through 8), and seven pulses (pulses 3 through 9). The joint measurement outcomes and corresponding  $p$  values for these combinations are shown in Table I. For a single pulse we measure a  $p$  value  $= 2.5 \times 10^{-3}$ , for three pulses a  $p$  value  $= 2.4 \times 10^{-6}$ , for five pulses a  $p$  value  $= 5.8 \times 10^{-9}$ , and for seven pulses a  $p$  value  $= 2.0 \times 10^{-7}$ , corresponding to a strong violation of local realism.

If, trial by trial, a conspiratorial hidden variable (or attacker in cryptographic scenarios) has some measure of control over or knowledge about the setting choices at Alice and Bob, then they could manipulate the outcomes to observe a violation of a Bell inequality. Even if we weaken our assumption that Alice's and Bob's setting choices are physically independent from the source, we can still compute valid  $p$  values against the hypothesis of local realism. We characterize the lack of physical independence with predictability of our random number generators. The "predictability,"  $\mathcal{P}$ , of a random number generator is the probability with which an adversary or local realistic system could guess a given setting choice. We use the parameter  $\epsilon$ , the "excess predictability," to place an upper bound on the actual predictability of our random number generators:

$$
\mathcal {P} \leq \frac {1}{2} (1 + \epsilon). \tag {3}
$$

In principle, it is impossible to measure predictability through statistical tests of the random numbers because they can be made to appear random, unbiased, and independent even if the excess predictability during each trial is nonzero. Extreme examples that could cause nonzero excess predictability include superdeterminism or a powerful and devious adversary with access to the devices, but subtle technical issues can never be entirely ruled out. Greater levels of excess predictability lead to lower statistical confidence in a rejection of local realism. In Fig. 5 we show how different levels of excess predictability change the statistical significance of our results [47] (see Supplemental Material [28] for more details). We can make estimates of the excess predictability in our system. From additional measurements, we observe a bias of  $(1.08 \pm 0.07) \times 10^{-4}$  in the settings reaching the XOR gate from the laser diffusion random source, which includes synchronization electronics as well as the random number generator. If this bias is the only source of predictability in our system, this level of bias would correspond to an excess predictability of approximately  $2 \times 10^{-4}$ . To be conservative we use an excess predictability bound that is 15 times larger,  $\epsilon_{p} = 3 \times 10^{-3}$  (see Supplemental Material [28] for more details). If our experiment had excess predictability equal to  $\epsilon_{p}$ , our  $p$  values would be increased to  $5.9 \times 10^{-3}$ ,  $2.4 \times 10^{-5}$ ,  $2.3 \times 10^{-7}$ , and  $9.2 \times 10^{-6}$  for one, three, five, and seven pulses, respectively [47]. Combining the output of this random number generator with the others should lead to lower bias levels and a lower excess predictability, but even under the paranoid situation where a nearly superdeterministic local realistic system has complete knowledge of the bits from the other random number sources, the adjusted  $p$  values still provide a rejection of local realism with high statistical significance.

Satisfying the spacetime separation constraints in Fig. 3 requires precise measurements of the locations of Alice, Bob, and the source as well as the timing of all events.

![](images/8c7d808c4301527920419eece269dc61f3eb56da42c5c5f4792a81080169455c.jpg)  
(a)

![](images/5bac0dba05e062732f52538d1540a65da002d7f672bc682558d71460af063808.jpg)  
FIG. 4 (color online). (a) The positions of Alice (A), Bob (B), and the source (S) in the building where the experiment was carried out. The insets show a magnified  $(\times 2)$  view of Alice's and Bob's locations. The white dots are the location of the random number generators (RNGs). The larger circle at each location has a radius of  $1\mathrm{m}$  and corresponds to our uncertainty in the spatial position measurements. Alice, Bob, and the source can be located anywhere within the green shaded regions and still have their events be spacelike separated. Boundaries are plotted for aggregates of one, three, five, and seven pulses. Each boundary is computed by keeping the chronology of events fixed but allowing the distance between the three parties to vary independently. In (b) the  $p$  value of each of the individual 15 pulses is shown. Overlaid on the plot are the aggregate pulse combinations used in the contours in (a). The statistical significance of our Bell violation does not appear to depend on the spacelike separation of events. For reference and comparison purposes only, the corresponding number of standard deviations for a given  $p$  value (for a one-sided normal distribution) are shown.

Using a combination of position measurements from a GPS receiver and site surveying, we determine the locations of Alice, Bob, and the source with an uncertainty of  $< 1\mathrm{m}$ . This uncertainty is set by the physical size of the cryostat used to house our detectors and the uncertainty in the GPS coordinates. There are four events that must be spacelike

![](images/6172cd1d345c55535a7ca2d26d69d016b67b93eeabf6f621de0c6d8a08014fe5.jpg)  
FIG. 5 (color online). The  $p$  value for different numbers of aggregate pulses as a function of the excess predictability,  $\epsilon$ , in Alice's and Bob's measurement settings. Larger levels of predictability correspond to a weakening of the assumption that the settings' choices are physically independent of the photon properties Alice and Bob measure. As in Fig. 4(b), the  $p$ -value equivalent confidence levels corresponding to the number of standard deviations of a one-sided normal distribution are shown for reference.

separated: Alice's and Bob's measurement choices must be fixed before any signal emanating from the photon creation event could arrive at their locations, and Alice and Bob must finish their measurements before information from the other party's measurement choice could reach them. Due to the slight asymmetry in the locations of Alice, Bob, and the source, the time difference between Bob finishing his measurement and information possibly arriving about Alice's measurement choice is always shorter than the time differences of the other three events as shown in Fig. 3(b). This time difference serves as a kind of margin; our system can tolerate timing errors as large as this margin and still have all events remain spacelike separated. For one, three, five, and seven aggregate pulses, this corresponds to a margin of  $63.5 \pm 3.7$  ns,  $50.9 \pm 3.7$  ns,  $38.3 \pm 3.7$  ns, and  $25.7 \pm 3.7$  ns, respectively, as shown in Table I. The uncertainty in these timing measurements is dominated by the 1-m positional uncertainty (see Supplemental Material [28] for further details on the timing measurements).

A way to visualize and further quantify the spacelike separation of events is to compute how far Alice, Bob, and the source could move from their measured positions and still be guaranteed to satisfy the locality constraints, assuming that the chronology of all events remains fixed. In Fig. 4(a) Alice, Bob, and the source locations are surrounded by shaded green regions. As long as each party remains anywhere inside the boundaries of these regions, their events are guaranteed to be spacelike separated. There are specific configurations where all three parties can be

outside the boundaries and still be spacelike separated, but here we consider the most conspiratorial case where all parties can collude with one another. The boundaries are overlaid on architectural drawings of the building in which the experiment was performed. Four different boundaries are plotted, corresponding to the Bell test performed with one, three, five, and seven aggregate pulses. Minimizing over the path of each boundary line, the minimum distance that Alice, Bob, and the source are located from their respective boundaries is  $9.2\mathrm{m}$ ,  $7.3\mathrm{m}$ ,  $5.4\mathrm{m}$ , and  $3.5\mathrm{m}$  for aggregates of one pulse, three pulses, five pulses, and seven pulses, respectively. For these pulse configurations we would have had to place our source and detection systems physically in different rooms (or even move outside of the building) to compromise our spacelike separation. Aggregating more than seven pulses leads to boundaries that are less than three meters away from our measured positions. In these cases we are not able to make strong claims about the spacelike separation of our events.

Finally, as shown in Fig. 4(b), we can compute the  $15p$  values for each of the time slots we consider that photons from the source can arrive in every trial. Photons arriving in slots 1 through 11 are spacelike separated, while photons in slots 12 through 15 are not. The photons arriving in these later slots are measured after information from the other party's random number generator could arrive, as shown in Fig. 3(b). It appears that spacelike separation has no discernible effect on the statistical significance of the violation. However, we do see large slot-to-slot fluctuation in the calculated  $p$  values. We suspect that this is due to instability in the applied voltage when the Pockels cell is turned on. In this case photons receive slightly different polarization rotations depending on which slot they arrive in, leading to nonideal measurement settings at Alice and Bob. It is because of this slot-to-slot variation that the aggregate of seven pulses has a computed  $p$  value larger than the five-pulse case. Fixing this instability and using more sophisticated hypothesis test techniques [48-50] will enable us to robustly increase the statistical significance of our violation for the seven-pulse case.

The experiment reported here is a commissioning run of the Bell test machine we eventually plan to use to certify randomness. The ability to include multiple pulses in our Bell test highlights the flexibility of our system. Our Bell test machine is capable of high event rates, making it well suited for generating random numbers required by cryptographic applications [39]. Future work will focus on incorporating our Bell test machine as an additional source of real-time randomness into the National Institute of Standards and Technology's public random number beacon (see Ref. [51]).

It has been 51 years since Bell formulated his test of local realism. During that time his inequality has shaped our understanding of entanglement and quantum correlations, led to the quantum information revolution, and transformed

the study of quantum foundations. Until recently it has not been possible to carry out a complete and statistically significant loophole-free Bell test. Using advances in random number generation, photon source development, and high-efficiency single-photon detectors, we are able to observe a strong violation of a Bell inequality that is loophole free, meaning that we only need to make a minimal set of assumptions. These assumptions are that our measurements of locations and times of events are reliable, that Alice's and Bob's measurement outcomes are fixed at the time taggers, and that during any given trial, the random number generators at Alice and Bob are physically independent of each other and the properties of the photons being measured. It is impossible, even in principle, to eliminate a form of these assumptions in any Bell test. Under these assumptions, if a hidden variable theory is local, it does not agree with our results, and if it agrees with our results, then it is not local.

The data and associated software needed for analysis for this paper are available from NIST [52].

We thank Todd Harvey for assistance with optical fiber installation; Norman Sanford for the use of lab space; Kevin Silverman, Aephraim M. Steinberg, Rupert Ursin, Marissa Giustina, Stephen Jordan, Dietrich Leibfried, and Paul Lett for helpful discussions; Nik Luhrs and Kristina Meier for help with the electronics; Andrew Novick for help with the GPS measurements; Joseph Chapman and Malhar Jere for designing the cultural pseudorandom numbers; and Stephen Jordan, Paul Lett, and Dietrich Leibfried for constructive comments on the manuscript. We thank Conrad Turner Bierhorst for waiting patiently for the computation of  $p$  values. We acknowledge support for this project provided by DARPA (L. K. S., M. S. A., A. E. L., S. D. D., M. J. S., V. B. V., T. G., R. P. M., S. W. N., W. H. F., F. M., M. D. S., J. A. S.) and the NIST Quantum Information Program (L. K. S., M. S. A., A. E. L., S. D. D., M. J. S., V. B. V., T. G., S. G., P. B., J. C. B., A. M., R. P. M., E. K., S. W. N.); NSF Grant No. PHY 12-05870 and MURI Center for Photonic Quantum Information Systems (ARO/ARDA Program DAAD19-03-1-0199) DARPA InPho program and the Office of Naval Research MURI on Fundamental Research on Wavelength-Agile High-Rate Quantum Key Distribution (QKD) in a Marine Environment, Grant No. N00014-13-0627 (B.G.C., M. A.W., D.R.K., P.G.K.); NSERC, CIFAR and Industry Canada (E.M.S., Y.Z., T.J.); NASA (F.M., M.D.S., W.H.F., J.A.S.); European Research Council project AQUMET, European Union Project QUIC (Grant Agreement No. 641122), Spanish MINECO under the Severo Ochoa programme (Grant No. SEV-2015-0522) and projects MAGO (Grant No. FIS2011-23520) and EPEC (Grant No. FIS2014-62181-EXP), Catalan AGAUR 2014 SGR Grants No. 1295 and No. 1623, the European Regional Development Fund (FEDER) Grant No. TEC2013-46168-R, and by Fundació Privada

CELLEX (M. W. M., C. A., W. A., V. P.); and New Brunswick Innovation Foundation (D. R. H.). Part of the research was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration.

E. M.-S. and B. G. C. contributed equally to this work.

†Corresponding author. lks@nist.gov  
$^{\ddagger}$ Corresponding author. saewoo.nam@nist.gov  
[1] J. S. Bell, Epistemol. Lett. 2 (1975).  
[2] P. Holland, Found. Phys. 35, 177 (2005).  
[3] L. de Broglie, J. Phys. Radium 8, 225 (1927).  
[4] D. Bohm, Phys. Rev. 85, 166 (1952).  
[5] D. Bohm, Phys. Rev. 85, 180 (1952).  
[6] A. Einstein, B. Podolosky, and N. Rosen, Phys. Rev. 47, 777 (1935).  
[7] A. Einstein, M. Born, and H. Born, The Born-Einstein Letters: The Correspondence between Max & Hedwig Born and Albert Einstein 1916/1955, 1st ed. (The MacMillan Press, London and Basingstoke, 1971).  
[8] J. S. Bell, Physics 1, 195 (1964).  
[9] J.F. Clauser, M.A. Horne, A. Shimony, and R.A. Holt, Phys. Rev. Lett. 23, 880 (1969).  
[10] J. F. Clauser and M. A. Horne, Phys. Rev. D 10, 526 (1974).  
[11] S. J. Freedman and J. F. Clauser, Phys. Rev. Lett. 28, 938 (1972).  
[12] A. Aspect, P. Grangier, and G. Roger, Phys. Rev. Lett. 47, 460 (1981).  
[13] A. Aspect, P. Grangier, and G. Roger, Phys. Rev. Lett. 49, 91 (1982).  
[14] A. Aspect, J. Dalibard, and G. Roger, Phys. Rev. Lett. 49, 1804 (1982).  
[15] M. Genovese, Phys. Rep. 413, 319 (2005).  
[16] J.-Å. Larsson, J. Phys. A 47, 424003 (2014).  
[17] C. Abellan, W. Amaya, D. Mitrani, V. Pruneri, and M. W. Mitchell, following Letter, Phys. Rev. Lett. 115, 250403 (2015).  
[18] M. W. Mitchell, C. Abellan, and W. Amaya, Phys. Rev. A 91, 012314 (2015).  
[19] P. H. Eberhard, Phys. Rev. A 47, R747 (1993).  
[20] G. Weihs, T. Jennewein, C. Simon, H. Weinfurter, and A. Zeilinger, Phys. Rev. Lett. 81, 5039 (1998).  
[21] M. A. Rowe, D. Kielpinski, V. Meyer, C. A. Sackett, W. M. Itano, C. Monroe, and D. J. Wineland, Nature (London) 409, 791 (2001).  
[22] T. Scheidl, R. Ursin, J. Kofler, S. Ramelow, X.-S. Ma, T. Herbst, L. Ratschbacher, A. Fedrizzi, N.K. Langford, T. Jennewein, and A. Zeilinger, Proc. Natl. Acad. Sci. U.S.A. 107, 19708 (2010).  
[23] M. Giustina, A. Mech, S. Ramelow, B. Wittmann, J. Kofler, J. Beyer, A. Lita, B. Calkins, T. Gerrits, S. W. Nam, R. Ursin, and A. Zeilinger, Nature (London) 497, 227 (2013).  
[24] B.G. Christensen, K.T. McCusker, J.B. Altepeter, B. Calkins, T. Gerrits, A.E. Lita, A. Miller, L.K. Shalm,

Y. Zhang, S. W. Nam, N. Brunner, C. C. W. Lim, N. Gisin, and P. G. Kwiat, Phys. Rev. Lett. 111, 130406 (2013).  
[25] B. Hensen, H. Bernien, A. E. Dreau, A. Reiserer, N. Kalb, M. S. Blok, J. Ruitenberg, R. F. L. Vermeulen, R. N. Schouten, C. Abellán, W. Amaya, V. Pruneri, M. W. Mitchell, M. Markham, D. J. Twitchen, D. Elkouss, S. Wehner, T. H. Taminiau, and R. Hanson, Nature (London) 526, 682 (2015).  
[26] M. Giustina et al., preceding Letter, Phys. Rev. Lett. 115, 250401 (2015).  
[27] P. Bierhorst, J. Phys. A 48, 195302 (2015).  
[28] See Supplemental Material at http://link.aps.org/ supplemental/10.1103/PhysRevLett.115.250402, which includes Refs. [29-37] for details.  
[29] J. Barrett, D. Collins, L. Hardy, A. Kent, and S. Popescu, Phys. Rev. A 66, 042111 (2002).  
[30] J. Kofler and M. Giustina, J.-Å. Larsson, M. W. Mitchell, arXiv:1411.4787.  
[31] A. Fine, Phys. Rev. Lett. 48, 291 (1982).  
[32] Manual: Electrooptical Modulator Pockels Cell Driver, Model PCD dpp, Bergmann Messgeräte Entwicklung KG (2008).  
[33] A. Rukhin, J. Soto, J. Nechvatal, M. Smid, E. Barker, S. Leigh, M. Levenson, M. Vangel, D. Banks, A. Heckert, J. Dray, and S. Vo, A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications, Tech. Rep. 800-22 (National Institute of Standards and Technology, 2010).  
[34] P. L'Ecuyer and R. Simard, ACM Trans. Math. Softw. 33, 22 (2007).  
[35] A. Einstein, Deutsche Physikalische Gesellschaft 18, 318 (1916).  
[36] R. G. Brown (2004), http://csrc.nist.gov/groups/ST/to toolkit/rng/documentation software.html.  
[37] J. Walker, “Ent: A pseudorandom number sequence test program (2008), http://www.fourmilab.ch/random/”.  
[38] J. Shao, Mathematical Statistics, Springer Texts in Statistics, 2nd ed. (Springer, New York, 1998), pp. 126-127.  
[39] S. Pironio, A. Acín, S. Massar, A. B. de la Giroday, D. N. Matsukevich, P. Maunz, S. Olmschenk, D. Hayes, L. Luo, T. A. Manning, and C. Monroe, Nature (London) 464, 1021 (2010).  
[40] R. D. Gill, in Mathematical Statistics and Applications: Festschrift for Constance van Eeden, edited by M. Moore, S. Froda, and C. Leger (Institute of Mathematical Statistics, Beachwood, Ohio, 2003), Vol. 42, pp. 133-154.  
[41] P. B. Dixon, D. Rosenberg, V. Stelmakh, M. E. Grein, R. S. Bennink, E. A. Dauler, A. J. Kerman, R. J. Molnar, and F. N. C. Wong, Phys. Rev. A 90, 043804 (2014).  
[42] L. K. Shalm, K. Garay, J. Palfree, A. L. Migdall, A. U'Ren, and S. W. Nam, Spontaneous parametric downconversion calculator, http://www.spdcalc.org.  
[43] P. G. Evans, R. S. Bennink, W. P. Grice, T. S. Humble, and J. Schaake, Phys. Rev. Lett. 105, 253601 (2010).  
[44] All uncertainties  $U$  and error bars correspond to an estimated standard deviation  $\sigma$  and a coverage factor  $k = 1$  as  $U = k\sigma$ .  
[45] F. Marsili, V. B. Verma, J. A. Stern, S. Harrington, A. E. Lita, T. Gerrits, I. Vayshenker, B. Baek, M. D. Shaw, R. P. Mirin, and S. W. Nam, Nat. Photonics 7, 210 (2013).

[46] D. N. Klyshko, Sov. J. Quantum Electron. 10, 1112 (1980).  
[47] P. Bierhorst, arXiv:1312.2999.  
[48] Y. Zhang, S. Glancy, and E. Knill, Phys. Rev. A 84, 062118 (2011).  
[49] Y. Zhang, S. Glancy, and E. Knill, Phys. Rev. A 88, 052119 (2013).

[50] E. Knill, S. Glancy, S. W. Nam, K. Coakley, and Y. Zhang, Phys. Rev. A 91, 032105 (2015).  
[51] https://beacon.nist.gov.  
[52] L. K. Shalm and S. W. Nam, Data for "Strong loophole-free test of local realism," National Institute of Standards and Technology, DOI: 10.5060/D2JW8BTT (2015).