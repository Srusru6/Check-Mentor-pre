import os
import requests
import concurrent.futures
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

# 1. ç›®æ ‡ä½œè€…é…ç½®
TARGETS = [
    {
        "name": "Feng, Xu",
        "id": "1040303",
        "cn_name": "å†¯æ—­",
        "strict_names": ["Xu, Feng"]
    },
]

# 2. æ•°é‡é™åˆ¶
LIMIT = 10       
MAX_AUTHORS = 10 # è¿‡æ»¤æ‰ä½œè€…æ•°è¶…è¿‡ 10 äººçš„æ–‡ç« 

# 2.5 æœŸåˆŠé™åˆ¶
ALLOWED_JOURNALS = ["PhysRevD", "PhysRevLett", "Nature", "Science"]  # åªæ”¶é›† PRD/PRL/Nature/Science çš„è®ºæ–‡

# 3. ç½‘ç»œè¯·æ±‚é…ç½® (ä¼˜åŒ–ç‰ˆ)
BATCH_SIZE = 50  # æ¯æ¬¡æŠ“ 50 ç¯‡ï¼Œå¤§å¹…æé«˜ä¸‹è½½é€Ÿåº¦
MAX_RETRIES = 2  # å•é¡µå¤±è´¥é‡è¯•æ¬¡æ•°
MAX_PAGES = 30   # æœ€å¤šç¿» 30 é¡µ
TIMEOUT = 8      # é™ä½è¶…æ—¶æ—¶é—´åˆ° 8 ç§’

# 4. è¾“å‡ºè·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = CURRENT_DIR
OUTPUT_PATH = os.path.join(OUTPUT_DIR, "results.txt")

# ================= ğŸ› ï¸ æ ¸å¿ƒä»£ç  =================

class StableFetcher:
    BASE_URL = "https://inspirehep.net/api"
    
    def __init__(self):
        self.session = requests.Session()
        # åº•å±‚ TCP é‡è¯•ç­–ç•¥
        retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json"
        })

    def get_bai_from_record_id(self, name, record_id):
        """è·å– BAI"""
        print(f"   [ğŸ”] è®¿é—®æ¡£æ¡ˆ ID: {record_id} ({name})...")
        try:
            res = self.session.get(f"{self.BASE_URL}/authors/{record_id}", timeout=20)
            if res.status_code == 404: return None
            data = res.json()
            ids = data.get("metadata", {}).get("ids", [])
            bai = next((item.get("value") for item in ids if item.get("schema") == "INSPIRE BAI"), None)
            return bai if bai else f"recid:{record_id}"
        except Exception: return None

    def check_exact_match(self, paper_authors, strict_names):
        """çµæ´»çš„åå­—åŒ¹é…ï¼šæ”¯æŒä¸åŒçš„åå­—é¡ºåºã€ç©ºæ ¼å˜ä½“ç­‰"""
        for author in paper_authors:
            full_name = author.get("full_name", "").strip()
            full_name_lower = full_name.lower()
            
            for strict_name in strict_names:
                strict_name_lower = strict_name.lower()
                
                # 1. ç›´æ¥å®Œå…¨åŒ¹é…
                if full_name_lower == strict_name_lower:
                    return True
                
                # 2. å¤„ç†ç©ºæ ¼å˜ä½“ï¼šç§»é™¤æ‰€æœ‰ç©ºæ ¼åæ¯”è¾ƒ
                # ä¾‹å¦‚ "HuaXing, Zhu" åº”è¯¥åŒ¹é… "Hua Xing, Zhu"
                strict_no_space = strict_name_lower.replace(" ", "")
                full_no_space = full_name_lower.replace(" ", "")
                
                if strict_no_space == full_no_space:
                    return True
                
                # 3. å¤„ç†åå­—é¡ºåºå˜ä½“ï¼ˆéƒ½å«æœ‰é€—å·ï¼‰
                if ',' in strict_name_lower and ',' in full_name_lower:
                    strict_parts = [p.strip() for p in strict_name_lower.split(',')]
                    full_parts = [p.strip() for p in full_name_lower.split(',')]
                    
                    if len(strict_parts) == 2 and len(full_parts) == 2:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åå‘é¡ºåº
                        if strict_parts[0] == full_parts[1] and strict_parts[1] == full_parts[0]:
                            return True
                        
                        # å¤„ç†é¡ºåºåè½¬æ—¶çš„ç©ºæ ¼å˜ä½“
                        strict_parts_no_space = [p.replace(" ", "") for p in strict_parts]
                        full_parts_no_space = [p.replace(" ", "") for p in full_parts]
                        if (strict_parts_no_space[0] == full_parts_no_space[1] and 
                            strict_parts_no_space[1] == full_parts_no_space[0]):
                            return True
                
                # 4. å¦‚æœ strict_name ä¸­æ²¡æœ‰é€—å·ï¼Œå°è¯•ä» full_name ä¸­æå–åŒ¹é…
                if ',' not in strict_name_lower and ',' in full_name_lower:
                    # ä¾‹å¦‚ strict_name="Xu Feng"ï¼Œfull_name="Feng, Xu"
                    parts = strict_name_lower.split()
                    # æ£€æŸ¥ strict_name çš„å„éƒ¨åˆ†æ˜¯å¦éƒ½åœ¨ full_name ä¸­
                    if all(part in full_name_lower for part in parts):
                        return True
                    
                    # ä¹Ÿå¤„ç†ç©ºæ ¼ç§»é™¤åçš„åŒ¹é…
                    parts_no_space = [p.replace(" ", "") for p in parts]
                    for part_no_space in parts_no_space:
                        if part_no_space and part_no_space not in full_no_space:
                            break
                    else:
                        # æ‰€æœ‰éƒ¨åˆ†éƒ½åŒ¹é…
                        if all(p.replace(" ", "") in full_no_space for p in parts):
                            return True
        
        return False

    def fetch_paper_data_stable(self, target_config, identifier, sort_mode):
        """
        æç¨³å¥çš„åˆ†é¡µä¸‹è½½ï¼šå°æ‰¹é‡ + åŸåœ°é‡è¯•
        """
        name = target_config["name"]
        strict_names = target_config["strict_names"]
        tag = "æœ€æ–°" if sort_mode == "mostrecent" else "é«˜å¼•"
        
        results = []
        seen_dois = set()  # æœ¬æ¨¡å¼å†…å»é‡
        page = 1
        total_checked = 0  # ç»Ÿè®¡æ‰€æœ‰æ£€ç´¢çš„æ–‡ç« æ€»æ•°
        
        base_params = {
            "q": f"a {identifier}", 
            "sort": sort_mode,
            "size": BATCH_SIZE
        }
        
        print(f"\n   [â³] {name} [{tag}] å¼€å§‹æ‰«æ (ç›®æ ‡ {LIMIT} ç¯‡)...", flush=True)
        
        # å¾ªç¯ç›´åˆ°å‡‘å¤Ÿæ•°é‡æˆ–ç¿»é¡µè¿‡å¤š
        while len(results) < LIMIT:
            params = base_params.copy()
            params["page"] = page
            
            # --- åŸåœ°é‡è¯•é€»è¾‘ ---
            success = False
            for attempt in range(MAX_RETRIES):
                try:
                    res = self.session.get(f"{self.BASE_URL}/literature", params=params, timeout=TIMEOUT)
                    data = res.json()
                    success = True
                    break # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                except Exception as e:
                    print(f"      [âš ï¸] {name} Page {page} å¤±è´¥ (å°è¯• {attempt+1}/{MAX_RETRIES}): {e}", flush=True)
                    time.sleep(0.3) # ç¼©çŸ­ç­‰å¾…æ—¶é—´
            
            if not success:
                print(f"      [âŒ] {name} Page {page} å½»åº•å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µ", flush=True)
                page += 1
                continue # æ”¾å¼ƒè¿™ä¸€é¡µï¼Œå°è¯•ä¸‹ä¸€é¡µ
            
            # --- æ•°æ®å¤„ç† ---
            hits = data.get("hits", {}).get("hits", [])
            if not hits: 
                break # æ²¡æœ‰æ›´å¤šæ•°æ®äº†
            
            for hit in hits:
                if len(results) >= LIMIT: break
                
                total_checked += 1  # è®¡æ•°æ‰€æœ‰æ£€ç´¢çš„æ–‡ç« 
                
                metadata = hit.get("metadata", {})
                authors_list = metadata.get("authors", [])
                
                # 1. è¿‡æ»¤å¤§å‹åˆä½œç»„
                if len(authors_list) > MAX_AUTHORS: 
                    continue
                
                # 2. ä¸¥æ ¼åå­—åŒ¹é…
                if not self.check_exact_match(authors_list, strict_names):
                    continue
                
                # 3. æœŸåˆŠè¿‡æ»¤ (PRD/PRL/Nature/Science)
                d_list = metadata.get("dois", [])
                doi_val = d_list[0].get("value") if d_list else None
                is_allowed_journal = any(journal in doi_val for journal in ALLOWED_JOURNALS) if doi_val else False
                if not is_allowed_journal:
                    continue
                
                # 4. å»é‡æ£€æŸ¥ï¼ˆä»…æœ¬æ¨¡å¼å†…ï¼‰
                if doi_val in seen_dois:
                    continue
                
                # 5. æå–æ•°æ®
                if doi_val:
                    author_names = [a.get("full_name", "Unknown") for a in authors_list]
                    authors_str = "; ".join(author_names)
                    
                    results.append({
                        "doi": doi_val,
                        "authors": authors_str
                    })
                    seen_dois.add(doi_val)
                    print(f"      âœ“ [{tag}] ç¬¬ {total_checked} ç¯‡: {doi_val} (å·²ä¿å­˜ {len(results)}/{LIMIT})", flush=True)
            
            page += 1
            if page > MAX_PAGES: break # é˜²æ­¢æ­»å¾ªç¯

        print(f"      [â¬‡ï¸] {name} [{tag}] å®Œæˆ! è·å– {len(results)} æ¡", flush=True)
        return sort_mode, results

# ================= â–¶ï¸ è¾…åŠ©å‡½æ•° =================

def get_processed_teachers(results_file):
    """ä» results.txt ä¸­è¯»å–å·²å¤„ç†çš„ä¸­æ–‡å§“ååˆ—è¡¨"""
    processed = set()
    if not os.path.exists(results_file):
        return processed
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç­‰å·åˆ†éš”è¡Œ
                if line and not line.startswith('=') and not line.startswith('10.'):
                    # è¿™åº”è¯¥æ˜¯ä¸­æ–‡å§“å
                    processed.add(line)
    except Exception as e:
        print(f"è¯»å–å·²å¤„ç†æ•™å¸ˆåˆ—è¡¨å¤±è´¥: {e}")
    
    return processed

def get_first_unprocessed_target(targets, processed_names):
    """ä» TARGETS ä¸­è·å–ç¬¬ä¸€ä¸ªæœªå¤„ç†çš„æ•™å¸ˆ"""
    for target in targets:
        cn_name = target.get('cn_name', '')
        if cn_name and cn_name not in processed_names:
            return target
    return None

# ================= â–¶ï¸ ä¸»ç¨‹åº =================

if __name__ == "__main__":
    # è‡ªåŠ¨åˆ›å»ºç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        try:
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            print(f"ğŸ“‚ å·²åˆ›å»ºç›®å½•: {OUTPUT_DIR}")
        except OSError as e:
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {e}")
            exit(1)

    fetcher = StableFetcher()
    
    print(f"ğŸš€ å¯åŠ¨å¢é‡æ¨¡å¼ (Batch={BATCH_SIZE}, Limit={LIMIT})...")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}\n")
    
    # 1. è¯»å–å·²å¤„ç†çš„æ•™å¸ˆåˆ—è¡¨
    processed_teachers = get_processed_teachers(OUTPUT_PATH)
    print(f"ğŸ“‹ å·²å¤„ç†æ•™å¸ˆ: {len(processed_teachers)} äºº")
    if processed_teachers:
        print(f"   {', '.join(processed_teachers)}\n")
    
    # 2. ä» id.txt åŠ è½½ TARGETS
    id_file_path = os.path.join(CURRENT_DIR, "id.txt")
    if os.path.exists(id_file_path):
        try:
            with open(id_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # æ‰§è¡Œ id.txt è·å– TARGETS
                local_vars = {}
                exec(content, {}, local_vars)
                if 'TARGETS' in local_vars:
                    TARGETS = local_vars['TARGETS']
                    print(f"âœ“ ä» id.txt åŠ è½½äº† {len(TARGETS)} ä¸ªç›®æ ‡æ•™å¸ˆ")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ id.txtï¼Œä½¿ç”¨é»˜è®¤ TARGETS: {e}")
    
    # 3. æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœªå¤„ç†çš„æ•™å¸ˆ
    target = get_first_unprocessed_target(TARGETS, processed_teachers)
    
    if not target:
        print("\nâœ… æ‰€æœ‰æ•™å¸ˆéƒ½å·²å¤„ç†å®Œæˆï¼")
        exit(0)
    
    print(f"\nğŸ¯ æœ¬æ¬¡å¤„ç†æ•™å¸ˆ: {target.get('cn_name')} ({target.get('name')})\n")
    print("-" * 60)
    
    # 4. è·å–è¯¥æ•™å¸ˆçš„ BAI
    identifier = fetcher.get_bai_from_record_id(target["name"], target["id"])
    if not identifier:
        print(f"âŒ æ— æ³•è·å–æ•™å¸ˆ {target['name']} çš„æ ‡è¯†ç¬¦")
        exit(1)
    
    # 5. å¹¶å‘ä¸‹è½½è¯¥æ•™å¸ˆçš„æ•°æ®
    print(f"\nâš¡ å¼€å§‹ä¸‹è½½ä¸ç­›é€‰...\n")
    
    final_data = {}
    final_data[target["name"]] = {}
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_map = {}
        f1 = executor.submit(fetcher.fetch_paper_data_stable, target, identifier, "mostrecent")
        future_map[f1] = "mostrecent"
        f2 = executor.submit(fetcher.fetch_paper_data_stable, target, identifier, "mostcited")
        future_map[f2] = "mostcited"
        
        for future in concurrent.futures.as_completed(future_map):
            mode = future_map[future]
            try:
                _, data_list = future.result()
                final_data[target["name"]][mode] = data_list
            except Exception as e:
                print(f"ä»»åŠ¡å¼‚å¸¸: {e}")
                final_data[target["name"]][mode] = []

    # 6. è¿½åŠ å†™å…¥æ–‡ä»¶ï¼ˆæç®€æ ¼å¼ï¼šä¸­æ–‡å + 20ä¸ªDOIï¼‰
    print(f"\n{'='*60}")
    print("ğŸ“ æ­£åœ¨è¿½åŠ å†™å…¥ç»“æœæ–‡ä»¶...")
    print(f"{'='*60}\n")
    
    try:
        # ä½¿ç”¨è¿½åŠ æ¨¡å¼
        with open(OUTPUT_PATH, "a", encoding="utf-8") as f:
            name = target["name"]
            cn_name = target.get("cn_name", name)
            
            # åˆå¹¶æœ€æ–°å’Œé«˜å¼•çš„æ‰€æœ‰ DOI
            all_dois = []
            recents = final_data[name].get("mostrecent", [])
            cited = final_data[name].get("mostcited", [])
            
            for item in recents:
                all_dois.append(item['doi'])
            for item in cited:
                all_dois.append(item['doi'])
            
            # è¾“å‡ºï¼šç­‰å· + ä¸­æ–‡å + ç­‰å· + æœ€å¤š20ä¸ªDOI
            f.write("=" * 50 + "\n")
            f.write(f"{cn_name}\n")
            f.write("=" * 50 + "\n")
            for doi in all_dois[:20]:  # åªå–å‰20ä¸ª
                f.write(f"{doi}\n")
            f.write("\n")  # ç©ºè¡Œåˆ†éš”
        
        print(f"âœ… è¿½åŠ å®Œæˆï¼")
        print(f"ğŸ“‚ å·²ä¸º {cn_name} æ·»åŠ  {min(len(all_dois), 20)} ç¯‡è®ºæ–‡")
        print(f"ğŸ“‚ ç»“æœæ–‡ä»¶: {OUTPUT_PATH}")
        
    except Exception as e:
        print(f"âŒ å†™å…¥å¤±è´¥: {e}")