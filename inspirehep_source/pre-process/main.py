import os
import requests
import concurrent.futures
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

# 1. ç›®æ ‡ä½œè€…é…ç½®
TARGETS = [
    {
        "name": "Feng, Xu",
        "id": "1040303",
        "cn_name": "å†¯æ—­",
        "strict_names": ["Xu, Feng"]
    },
]

# 2. æ•°é‡é™åˆ¶
LIMIT = 10       
MAX_AUTHORS = 10 # è¿‡æ»¤æ‰ä½œè€…æ•°è¶…è¿‡ 10 äººçš„æ–‡ç« 

# 2.5 æœŸåˆŠé™åˆ¶
ALLOWED_JOURNALS = ["PhysRevD", "PhysRevLett", "Nature", "Science"]  # åªæ”¶é›† PRD/PRL/Nature/Science çš„è®ºæ–‡

# 3. ç½‘ç»œè¯·æ±‚é…ç½® (ä¼˜åŒ–ç‰ˆ)
BATCH_SIZE = 50  # æ¯æ¬¡æŠ“ 50 ç¯‡ï¼Œå¤§å¹…æé«˜ä¸‹è½½é€Ÿåº¦
MAX_RETRIES = 2  # å•é¡µå¤±è´¥é‡è¯•æ¬¡æ•°
MAX_PAGES = 30   # æœ€å¤šç¿» 30 é¡µ
TIMEOUT = 8      # é™ä½è¶…æ—¶æ—¶é—´åˆ° 8 ç§’

# 4. è¾“å‡ºè·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = CURRENT_DIR
OUTPUT_PATH = os.path.join(OUTPUT_DIR, "results.txt")
FINISHED_TEACHERS_PATH = os.path.join(OUTPUT_DIR, "finished_teachers.txt")

# ================= ğŸ› ï¸ æ ¸å¿ƒä»£ç  =================

class StableFetcher:
    BASE_URL = "https://inspirehep.net/api"
    
    def __init__(self):
        self.session = requests.Session()
        # åº•å±‚ TCP é‡è¯•ç­–ç•¥
        retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json"
        })

    def get_bai_from_record_id(self, name, record_id):
        """è·å– BAI"""
        print(f"   [ğŸ”] è®¿é—®æ¡£æ¡ˆ ID: {record_id} ({name})...")
        try:
            res = self.session.get(f"{self.BASE_URL}/authors/{record_id}", timeout=20)
            if res.status_code == 404: return None
            data = res.json()
            ids = data.get("metadata", {}).get("ids", [])
            bai = next((item.get("value") for item in ids if item.get("schema") == "INSPIRE BAI"), None)
            return bai if bai else f"recid:{record_id}"
        except Exception: return None

    def check_exact_match(self, paper_authors, strict_names):
        """çµæ´»çš„åå­—åŒ¹é…ï¼šæ”¯æŒä¸åŒçš„åå­—é¡ºåºã€ç©ºæ ¼å˜ä½“ç­‰"""
        for author in paper_authors:
            full_name = author.get("full_name", "").strip()
            full_name_lower = full_name.lower()
            
            for strict_name in strict_names:
                strict_name_lower = strict_name.lower()
                
                # 1. ç›´æ¥å®Œå…¨åŒ¹é…
                if full_name_lower == strict_name_lower:
                    return True
                
                # 2. å¤„ç†ç©ºæ ¼å˜ä½“ï¼šç§»é™¤æ‰€æœ‰ç©ºæ ¼åæ¯”è¾ƒ
                # ä¾‹å¦‚ "HuaXing, Zhu" åº”è¯¥åŒ¹é… "Hua Xing, Zhu"
                strict_no_space = strict_name_lower.replace(" ", "")
                full_no_space = full_name_lower.replace(" ", "")
                
                if strict_no_space == full_no_space:
                    return True
                
                # 3. å¤„ç†åå­—é¡ºåºå˜ä½“ï¼ˆéƒ½å«æœ‰é€—å·ï¼‰
                if ',' in strict_name_lower and ',' in full_name_lower:
                    strict_parts = [p.strip() for p in strict_name_lower.split(',')]
                    full_parts = [p.strip() for p in full_name_lower.split(',')]
                    
                    if len(strict_parts) == 2 and len(full_parts) == 2:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åå‘é¡ºåº
                        if strict_parts[0] == full_parts[1] and strict_parts[1] == full_parts[0]:
                            return True
                        
                        # å¤„ç†é¡ºåºåè½¬æ—¶çš„ç©ºæ ¼å˜ä½“
                        strict_parts_no_space = [p.replace(" ", "") for p in strict_parts]
                        full_parts_no_space = [p.replace(" ", "") for p in full_parts]
                        if (strict_parts_no_space[0] == full_parts_no_space[1] and 
                            strict_parts_no_space[1] == full_parts_no_space[0]):
                            return True
                
                # 4. å¦‚æœ strict_name ä¸­æ²¡æœ‰é€—å·ï¼Œå°è¯•ä» full_name ä¸­æå–åŒ¹é…
                if ',' not in strict_name_lower and ',' in full_name_lower:
                    # ä¾‹å¦‚ strict_name="Xu Feng"ï¼Œfull_name="Feng, Xu"
                    parts = strict_name_lower.split()
                    # æ£€æŸ¥ strict_name çš„å„éƒ¨åˆ†æ˜¯å¦éƒ½åœ¨ full_name ä¸­
                    if all(part in full_name_lower for part in parts):
                        return True
                    
                    # ä¹Ÿå¤„ç†ç©ºæ ¼ç§»é™¤åçš„åŒ¹é…
                    parts_no_space = [p.replace(" ", "") for p in parts]
                    for part_no_space in parts_no_space:
                        if part_no_space and part_no_space not in full_no_space:
                            break
                    else:
                        # æ‰€æœ‰éƒ¨åˆ†éƒ½åŒ¹é…
                        if all(p.replace(" ", "") in full_no_space for p in parts):
                            return True
        
        return False

    def fetch_paper_data_dual(self, target_config, identifier, sort_mode, 
                              target_limit_restricted=10, existing_dois=None):
        """
        åŒæ—¶æ”¶é›†é™åˆ¶æœŸåˆŠå’Œéé™åˆ¶æœŸåˆŠçš„è®ºæ–‡
        
        è¿”å›: (sort_mode, restricted_dois, unrestricted_dois)
        """
        name = target_config["name"]
        strict_names = target_config["strict_names"]
        tag = "æœ€æ–°" if sort_mode == "mostrecent" else "é«˜å¼•"
        
        restricted_results = []  # é™åˆ¶æœŸåˆŠ
        unrestricted_results = []  # éé™åˆ¶æœŸåˆŠ
        seen_dois = set()  # å…¨å±€å»é‡
        
        if existing_dois is None:
            existing_dois = set()
        
        page = 1
        total_checked = 0
        
        base_params = {
            "q": f"a {identifier}",
            "sort": sort_mode,
            "size": BATCH_SIZE
        }
        
        print(f"\n   [â³] {name} [{tag}] å¼€å§‹æ‰«æ (é™åˆ¶æœŸåˆŠç›®æ ‡ {target_limit_restricted} ç¯‡ï¼ŒåŒæ—¶æ”¶é›†æ‰€æœ‰éé™åˆ¶æœŸåˆŠï¼Œæœç´¢ä¸Šé™ {MAX_PAGES} é¡µ)...", flush=True)
        
        # å¾ªç¯ç›´åˆ°è¾¾åˆ°æœç´¢ä¸Šé™ï¼ˆä¸å†æå‰åœæ­¢ï¼‰
        while page <= MAX_PAGES:
            params = base_params.copy()
            params["page"] = page
            
            # åŸåœ°é‡è¯•é€»è¾‘
            success = False
            for attempt in range(MAX_RETRIES):
                try:
                    res = self.session.get(f"{self.BASE_URL}/literature", params=params, timeout=TIMEOUT)
                    data = res.json()
                    success = True
                    break
                except Exception as e:
                    print(f"      [âš ï¸] {name} Page {page} å¤±è´¥ (å°è¯• {attempt+1}/{MAX_RETRIES}): {e}", flush=True)
                    time.sleep(0.3)
            
            if not success:
                print(f"      [âŒ] {name} Page {page} å½»åº•å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µ", flush=True)
                page += 1
                continue
            
            # æ•°æ®å¤„ç†
            hits = data.get("hits", {}).get("hits", [])
            if not hits:
                print(f"      [â„¹ï¸] {name} Page {page} æ— æ›´å¤šæ•°æ®ï¼Œæœç´¢å®Œæˆ", flush=True)
                break
            
            for hit in hits:
                total_checked += 1
                
                metadata = hit.get("metadata", {})
                authors_list = metadata.get("authors", [])
                
                # 1. è¿‡æ»¤å¤§å‹åˆä½œç»„
                if len(authors_list) > MAX_AUTHORS:
                    continue
                
                # 2. ä¸¥æ ¼åå­—åŒ¹é…ï¼ˆç¡®ä¿æ‰€æœ‰è®ºæ–‡éƒ½ç»è¿‡ä¸¥æ ¼å®¡æŸ¥ï¼‰
                if not self.check_exact_match(authors_list, strict_names):
                    continue
                
                # 3. æå– DOI
                d_list = metadata.get("dois", [])
                doi_val = d_list[0].get("value") if d_list else None
                
                if not doi_val:
                    continue
                
                # 4. å»é‡æ£€æŸ¥ï¼ˆå…¨å±€å»é‡ï¼‰
                if doi_val in seen_dois or doi_val in existing_dois:
                    continue
                
                # 5. åˆ¤æ–­æœŸåˆŠç±»å‹
                is_allowed_journal = any(journal in doi_val for journal in ALLOWED_JOURNALS)
                
                # 6. æ·»åŠ åˆ°å¯¹åº”åˆ—è¡¨
                author_names = [a.get("full_name", "Unknown") for a in authors_list]
                authors_str = "; ".join(author_names)
                
                result_item = {
                    "doi": doi_val,
                    "authors": authors_str
                }
                
                seen_dois.add(doi_val)
                
                if is_allowed_journal and len(restricted_results) < target_limit_restricted:
                    restricted_results.append(result_item)
                    print(f"      âœ“ [{tag}] é™åˆ¶æœŸåˆŠ ç¬¬ {total_checked} ç¯‡: {doi_val} (å·²ä¿å­˜ {len(restricted_results)}/{target_limit_restricted})", flush=True)
                elif not is_allowed_journal:
                    unrestricted_results.append(result_item)
                    print(f"      âœ“ [{tag}] éé™åˆ¶æœŸåˆŠ: {doi_val} (å·²ä¿å­˜ {len(unrestricted_results)})", flush=True)
            
            page += 1
        
        print(f"      [â¬‡ï¸] {name} [{tag}] å®Œæˆ! é™åˆ¶æœŸåˆŠ {len(restricted_results)} æ¡ï¼Œéé™åˆ¶æœŸåˆŠ {len(unrestricted_results)} æ¡", flush=True)
        return sort_mode, restricted_results, unrestricted_results


# ================= â–¶ï¸ è¾…åŠ©å‡½æ•° =================

def parse_results_file(results_file):
    """
    è§£æ results.txtï¼Œè¿”å›æ•™å¸ˆä¿¡æ¯å­—å…¸
    è¿”å›æ ¼å¼: {
        "cn_name": {
            "completed": True/False,  # æ˜¯å¦æœ‰æ˜Ÿå·
            "dois": ["doi1", "doi2", ...],
            "line_start": è¡Œå·  # åå­—æ‰€åœ¨è¡Œå·
        }
    }
    """
    teachers_info = {}
    
    if not os.path.exists(results_file):
        return teachers_info
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        current_teacher = None
        line_num = 0
        
        for idx, line in enumerate(lines):
            line_stripped = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œç­‰å·è¡Œ
            if not line_stripped or line_stripped.startswith('='):
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ DOI
            if line_stripped.startswith('10.'):
                if current_teacher:
                    teachers_info[current_teacher]["dois"].append(line_stripped)
            else:
                # è¿™æ˜¯æ•™å¸ˆåå­—
                completed = line_stripped.endswith('+')
                cn_name = line_stripped.rstrip('+').strip()
                
                current_teacher = cn_name
                teachers_info[cn_name] = {
                    "completed": completed,
                    "dois": [],
                    "line_number": idx
                }
    
    except Exception as e:
        print(f"è§£æ results.txt å¤±è´¥: {e}")
    
    return teachers_info

def get_processed_teachers(results_file):
    """ä» results.txt ä¸­è¯»å–å·²å¤„ç†çš„ä¸­æ–‡å§“ååˆ—è¡¨ï¼ˆä¸å«æ˜Ÿå·çš„åŸå§‹åå­—ï¼‰"""
    processed = set()
    if not os.path.exists(results_file):
        return processed
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç­‰å·åˆ†éš”è¡Œæˆ– DOI
                if line and not line.startswith('=') and not line.startswith('10.'):
                    # ç§»é™¤å¯èƒ½çš„æ˜Ÿå·ï¼Œè¿™æ˜¯ä¸­æ–‡å§“å
                    cn_name = line.rstrip('+').strip()
                    processed.add(cn_name)
    except Exception as e:
        print(f"è¯»å–å·²å¤„ç†æ•™å¸ˆåˆ—è¡¨å¤±è´¥: {e}")
    
    return processed

def find_teacher_needing_supplement(results_file):
    """
    æ‰¾åˆ°ç¬¬ä¸€ä¸ªéœ€è¦è¡¥å……è®ºæ–‡çš„æ•™å¸ˆï¼ˆæ²¡æœ‰æ˜Ÿå·çš„æ•™å¸ˆï¼‰
    è¿”å›: (cn_name, existing_dois) æˆ– (None, None)
    """
    teachers_info = parse_results_file(results_file)
    
    for cn_name, info in teachers_info.items():
        if not info["completed"]:
            return cn_name, set(info["dois"])
    
    return None, None

def update_teacher_status_in_file(results_file, cn_name, mark_completed=True):
    """
    åœ¨ results.txt ä¸­ç»™æ•™å¸ˆåå­—æ·»åŠ æˆ–ç§»é™¤è¡¥å……æ ‡è®°ï¼ˆå°¾éƒ¨+ï¼‰
    mark_completed=True: æ·»åŠ +
    mark_completed=False: ç§»é™¤+
    """
    if not os.path.exists(results_file):
        return
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        modified = False
        for idx, line in enumerate(lines):
            line_stripped = line.strip()
            # è·³è¿‡ç©ºè¡Œå’Œç­‰å·
            if not line_stripped or line_stripped.startswith('=') or line_stripped.startswith('10.'):
                continue
            
            # è·å–ä¸å«è¡¥å……æ ‡è®°çš„åå­—
            current_name = line_stripped.rstrip('+').strip()
            
            if current_name == cn_name:
                if mark_completed and not line_stripped.endswith('+'):
                    # æ·»åŠ è¡¥å……æ ‡è®°ï¼ˆæ”¾åœ¨åå­—åï¼‰
                    lines[idx] = current_name + '+\n'
                    modified = True
                elif not mark_completed and line_stripped.endswith('+'):
                    # ç§»é™¤è¡¥å……æ ‡è®°
                    lines[idx] = current_name + '\n'
                    modified = True
                break
        
        if modified:
            with open(results_file, 'w', encoding='utf-8') as f:
                f.writelines(lines)
            status = "å·²å®Œæˆ" if mark_completed else "æœªå®Œæˆ"
            print(f"   âœ“ å·²æ ‡è®° {cn_name} ä¸º {status}")
    
    except Exception as e:
        print(f"æ›´æ–°æ•™å¸ˆçŠ¶æ€å¤±è´¥: {e}")

def get_first_unprocessed_target(targets, processed_names):
    """ä» TARGETS ä¸­è·å–ç¬¬ä¸€ä¸ªæœªå¤„ç†çš„æ•™å¸ˆ"""
    for target in targets:
        cn_name = target.get('cn_name', '')
        if cn_name and cn_name not in processed_names:
            return target
    return None

# ================= â–¶ï¸ ä¸»ç¨‹åº =================

if __name__ == "__main__":
    # è‡ªåŠ¨åˆ›å»ºç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        try:
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            print(f"ğŸ“‚ å·²åˆ›å»ºç›®å½•: {OUTPUT_DIR}")
        except OSError as e:
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {e}")
            exit(1)

    fetcher = StableFetcher()
    
    print(f"ğŸš€ å¯åŠ¨å¢é‡æ¨¡å¼ (Batch={BATCH_SIZE}, Limit={LIMIT})...")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}\n")
    
    # 1. ä» id.txt åŠ è½½ TARGETS
    id_file_path = os.path.join(CURRENT_DIR, "id.txt")
    if os.path.exists(id_file_path):
        try:
            with open(id_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # æ‰§è¡Œ id.txt è·å– TARGETS
                local_vars = {}
                exec(content, {}, local_vars)
                if 'TARGETS' in local_vars:
                    TARGETS = local_vars['TARGETS']
                    print(f"âœ“ ä» id.txt åŠ è½½äº† {len(TARGETS)} ä¸ªç›®æ ‡æ•™å¸ˆ")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ id.txtï¼Œä½¿ç”¨é»˜è®¤ TARGETS: {e}")
    
    # 2. è¯»å–å·²å¤„ç†çš„æ•™å¸ˆåˆ—è¡¨
    processed_teachers = get_processed_teachers(OUTPUT_PATH)
    print(f"ğŸ“‹ å·²å¤„ç†æ•™å¸ˆ: {len(processed_teachers)} äºº")
    if processed_teachers:
        print(f"   {', '.join(processed_teachers)}\n")
    
    # 3. æ‰¾ç¬¬ä¸€ä¸ªæœªå¤„ç†çš„æ–°æ•™å¸ˆ
    target = get_first_unprocessed_target(TARGETS, processed_teachers)
    
    if not target:
        print("\nâœ… æ‰€æœ‰æ•™å¸ˆéƒ½å·²å¤„ç†å®Œæˆï¼")
        exit(0)
    
    print(f"\nğŸ¯ æ–°æ•™å¸ˆ: {target.get('cn_name')} ({target.get('name')})\n")
    print("-" * 60)
    
    # 4. è·å–è¯¥æ•™å¸ˆçš„ BAI
    identifier = fetcher.get_bai_from_record_id(target["name"], target["id"])
    if not identifier:
        print(f"âŒ æ— æ³•è·å–æ•™å¸ˆ {target['name']} çš„æ ‡è¯†ç¬¦")
        exit(1)
    
    # 5. ç¬¬ä¸€é˜¶æ®µï¼šè·å–æœ€æ–°å’Œé«˜å¼•ï¼ˆå„é™åˆ¶æœŸåˆŠ10ç¯‡ + æ‰€æœ‰éé™åˆ¶æœŸåˆŠï¼‰
    print(f"\nâš¡ ç¬¬ä¸€é˜¶æ®µï¼šè·å–é™åˆ¶æœŸåˆŠå’Œéé™åˆ¶æœŸåˆŠè®ºæ–‡...\n")
    
    final_data = {}
    final_data[target["name"]] = {}
    name = target["name"]
    cn_name = target.get("cn_name", name)
    
    # å¹¶å‘è·å– mostrecent å’Œ mostcitedï¼ŒåŒæ—¶æ”¶é›†é™åˆ¶å’Œéé™åˆ¶æœŸåˆŠ
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_map = {}
        f1 = executor.submit(
            fetcher.fetch_paper_data_dual,
            target, identifier, "mostrecent",
            target_limit_restricted=10,
            existing_dois=None
        )
        future_map[f1] = "mostrecent"
        
        f2 = executor.submit(
            fetcher.fetch_paper_data_dual,
            target, identifier, "mostcited",
            target_limit_restricted=10,
            existing_dois=None
        )
        future_map[f2] = "mostcited"
        
        for future in concurrent.futures.as_completed(future_map):
            mode = future_map[future]
            try:
                _, restricted_list, unrestricted_list = future.result()
                final_data[name][mode] = {
                    "restricted": restricted_list,
                    "unrestricted": unrestricted_list
                }
            except Exception as e:
                print(f"ä»»åŠ¡å¼‚å¸¸: {e}")
                final_data[name][mode] = {
                    "restricted": [],
                    "unrestricted": []
                }
    
    # 6. åˆå¹¶å¹¶å»é‡é™åˆ¶æœŸåˆŠ DOI
    restricted_dois = []
    restricted_dois_unique = []
    restricted_seen = set()
    
    unrestricted_dois = []  # æ”¶é›†æ‰€æœ‰éé™åˆ¶æœŸåˆŠ DOI
    unrestricted_seen = set()
    
    # å¤„ç† mostrecent çš„é™åˆ¶æœŸåˆŠ
    recents = final_data[name].get("mostrecent", {})
    for item in recents.get("restricted", []):
        doi = item['doi']
        restricted_dois.append(doi)
        if doi not in restricted_seen:
            restricted_dois_unique.append(doi)
            restricted_seen.add(doi)
    
    # å¤„ç† mostcited çš„é™åˆ¶æœŸåˆŠ
    cited = final_data[name].get("mostcited", {})
    for item in cited.get("restricted", []):
        doi = item['doi']
        restricted_dois.append(doi)
        if doi not in restricted_seen:
            restricted_dois_unique.append(doi)
            restricted_seen.add(doi)
    
    # æ”¶é›†æ‰€æœ‰éé™åˆ¶æœŸåˆŠ DOIï¼ˆä» mostrecent å’Œ mostcitedï¼‰
    for item in recents.get("unrestricted", []):
        doi = item['doi']
        if doi not in unrestricted_seen:
            unrestricted_dois.append(doi)
            unrestricted_seen.add(doi)
    
    for item in cited.get("unrestricted", []):
        doi = item['doi']
        if doi not in unrestricted_seen:
            unrestricted_dois.append(doi)
            unrestricted_seen.add(doi)
    
    # æ£€æŸ¥é™åˆ¶æœŸåˆŠæ˜¯å¦æœ‰é‡å¤
    if len(restricted_dois_unique) < len(restricted_dois):
        print(f"\nâš ï¸ é™åˆ¶æœŸåˆŠå‘ç°é‡å¤ DOIï¼šæ”¶é›† {len(restricted_dois)} ç¯‡ï¼Œå»é‡å {len(restricted_dois_unique)} ç¯‡")
    
    supplement_mode = False
    cited_supplement = []  # åˆå§‹åŒ–è¡¥å……æ•°æ®
    
    # å¦‚æœé™åˆ¶æœŸåˆŠä¸è¶³ 20 ç¯‡ï¼Œå¯åŠ¨è¡¥å……æ¨¡å¼ï¼ˆä»…åœ¨é«˜å¼•é™åˆ¶æœŸåˆŠä¸­è¡¥å……ï¼‰
    if len(restricted_dois_unique) < 20:
        supplement_mode = True
        existing_restricted_dois = set(restricted_dois_unique)
        needed = 20 - len(restricted_dois_unique)
        
        print(f"\nâš ï¸ é™åˆ¶æœŸåˆŠä»…è·å– {len(restricted_dois_unique)} ç¯‡ï¼Œä¸è¶³ 20 ç¯‡")
        print(f"ğŸ”„ å¯åŠ¨è¡¥å……æ¨¡å¼ï¼šä»é«˜å¼•é™åˆ¶æœŸåˆŠè¡¥å…… {needed} ç¯‡...\n")
        
        # ç¬¬äºŒé˜¶æ®µï¼šä»…ä» mostcited çš„é™åˆ¶æœŸåˆŠè¡¥å……
        _, cited_restricted, _ = fetcher.fetch_paper_data_dual(
            target, identifier, "mostcited",
            target_limit_restricted=needed,
            existing_dois=existing_restricted_dois
        )
        cited_supplement = cited_restricted
        
        # è¿½åŠ åˆ°é™åˆ¶æœŸåˆŠæ•°æ®
        restricted_dois_unique.extend([item['doi'] for item in cited_supplement])
    
    # 7. å†™å…¥æ–‡ä»¶
    print(f"\n{'='*60}")
    print("ğŸ“ æ­£åœ¨å†™å…¥ç»“æœæ–‡ä»¶...")
    print(f"{'='*60}\n")
    
    try:
        with open(OUTPUT_PATH, "a", encoding="utf-8") as f:
            # ç¬¬ä¸€æ¡ç›®ï¼šé™åˆ¶æœŸåˆŠ DOIï¼ˆä¸å¸¦åŠ å·ï¼‰
            f.write("=" * 50 + "\n")
            f.write(f"{cn_name}\n")
            f.write("=" * 50 + "\n")
            
            # è¾“å‡ºé™åˆ¶æœŸåˆŠçš„ DOIï¼Œæœ€å¤š 20 ç¯‡
            output_restricted = restricted_dois_unique[:20]
            for doi in output_restricted:
                f.write(f"{doi}\n")
            f.write("\n")
            
            print(f"âœ… ç¬¬ä¸€æ¡ç›®å®Œæˆï¼")
            print(f"ğŸ“‚ é™åˆ¶æœŸåˆŠï¼š{cn_name} æ·»åŠ  {len(output_restricted)} ç¯‡è®ºæ–‡")
            
            if supplement_mode:
                print(f"ğŸ“‚ è¡¥å……æ¨¡å¼å·²ä½¿ç”¨ï¼šä»é«˜å¼•é™åˆ¶æœŸåˆŠè¡¥å…… {len(cited_supplement)} ç¯‡")
            
            # ç¬¬äºŒæ¡ç›®ï¼šéé™åˆ¶æœŸåˆŠ DOIï¼ˆåå­—ååŠ +ï¼‰
            if unrestricted_dois:
                f.write("=" * 50 + "\n")
                f.write(f"{cn_name}+\n")
                f.write("=" * 50 + "\n")
                
                for doi in unrestricted_dois:
                    f.write(f"{doi}\n")
                f.write("\n")
                
                print(f"ğŸ“‚ ç¬¬äºŒæ¡ç›®å®Œæˆï¼")
                print(f"ğŸ“‚ éé™åˆ¶æœŸåˆŠï¼š{cn_name}+ æ·»åŠ  {len(unrestricted_dois)} ç¯‡è®ºæ–‡")
        
        print(f"ğŸ“‚ ç»“æœæ–‡ä»¶: {OUTPUT_PATH}")
        
        # è¿½åŠ æ•™å¸ˆååˆ° finished_teachers.txt
        try:
            with open(FINISHED_TEACHERS_PATH, "a", encoding="utf-8") as f:
                f.write(f"{cn_name}\n")
            print(f"âœ… å·²å°† {cn_name} è¿½åŠ åˆ° finished_teachers.txt")
        except Exception as e:
            print(f"âš ï¸ å†™å…¥ finished_teachers.txt å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"âŒ å†™å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
