import os
import requests
import concurrent.futures
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================

# 1. ç›®æ ‡ä½œè€…é…ç½®
TARGETS = [
    {
        "name": "Feng, Xu",
        "id": "1040303",
        "cn_name": "å†¯æ—­",
        "strict_names": ["Xu, Feng"]
    },
]

# 2. æ•°é‡é™åˆ¶
LIMIT = 10       
MAX_AUTHORS = 10 # è¿‡æ»¤æ‰ä½œè€…æ•°è¶…è¿‡ 10 äººçš„æ–‡ç« 

# 2.5 æœŸåˆŠé™åˆ¶
ALLOWED_JOURNALS = ["PhysRevD", "PhysRevLett", "Nature", "Science"]  # åªæ”¶é›† PRD/PRL/Nature/Science çš„è®ºæ–‡

# 3. ç½‘ç»œè¯·æ±‚é…ç½® (ä¼˜åŒ–ç‰ˆ)
BATCH_SIZE = 50  # æ¯æ¬¡æŠ“ 50 ç¯‡ï¼Œå¤§å¹…æé«˜ä¸‹è½½é€Ÿåº¦
MAX_RETRIES = 2  # å•é¡µå¤±è´¥é‡è¯•æ¬¡æ•°
MAX_PAGES = 30   # æœ€å¤šç¿» 30 é¡µ
TIMEOUT = 8      # é™ä½è¶…æ—¶æ—¶é—´åˆ° 8 ç§’

# 4. è¾“å‡ºè·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = CURRENT_DIR
OUTPUT_PATH = os.path.join(OUTPUT_DIR, "results.txt")

# ================= ğŸ› ï¸ æ ¸å¿ƒä»£ç  =================

class StableFetcher:
    BASE_URL = "https://inspirehep.net/api"
    
    def __init__(self):
        self.session = requests.Session()
        # åº•å±‚ TCP é‡è¯•ç­–ç•¥
        retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=retries))
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "application/json"
        })

    def get_bai_from_record_id(self, name, record_id):
        """è·å– BAI"""
        print(f"   [ğŸ”] è®¿é—®æ¡£æ¡ˆ ID: {record_id} ({name})...")
        try:
            res = self.session.get(f"{self.BASE_URL}/authors/{record_id}", timeout=20)
            if res.status_code == 404: return None
            data = res.json()
            ids = data.get("metadata", {}).get("ids", [])
            bai = next((item.get("value") for item in ids if item.get("schema") == "INSPIRE BAI"), None)
            return bai if bai else f"recid:{record_id}"
        except Exception: return None

    def check_exact_match(self, paper_authors, strict_names):
        """çµæ´»çš„åå­—åŒ¹é…ï¼šæ”¯æŒä¸åŒçš„åå­—é¡ºåºã€ç©ºæ ¼å˜ä½“ç­‰"""
        for author in paper_authors:
            full_name = author.get("full_name", "").strip()
            full_name_lower = full_name.lower()
            
            for strict_name in strict_names:
                strict_name_lower = strict_name.lower()
                
                # 1. ç›´æ¥å®Œå…¨åŒ¹é…
                if full_name_lower == strict_name_lower:
                    return True
                
                # 2. å¤„ç†ç©ºæ ¼å˜ä½“ï¼šç§»é™¤æ‰€æœ‰ç©ºæ ¼åæ¯”è¾ƒ
                # ä¾‹å¦‚ "HuaXing, Zhu" åº”è¯¥åŒ¹é… "Hua Xing, Zhu"
                strict_no_space = strict_name_lower.replace(" ", "")
                full_no_space = full_name_lower.replace(" ", "")
                
                if strict_no_space == full_no_space:
                    return True
                
                # 3. å¤„ç†åå­—é¡ºåºå˜ä½“ï¼ˆéƒ½å«æœ‰é€—å·ï¼‰
                if ',' in strict_name_lower and ',' in full_name_lower:
                    strict_parts = [p.strip() for p in strict_name_lower.split(',')]
                    full_parts = [p.strip() for p in full_name_lower.split(',')]
                    
                    if len(strict_parts) == 2 and len(full_parts) == 2:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯åå‘é¡ºåº
                        if strict_parts[0] == full_parts[1] and strict_parts[1] == full_parts[0]:
                            return True
                        
                        # å¤„ç†é¡ºåºåè½¬æ—¶çš„ç©ºæ ¼å˜ä½“
                        strict_parts_no_space = [p.replace(" ", "") for p in strict_parts]
                        full_parts_no_space = [p.replace(" ", "") for p in full_parts]
                        if (strict_parts_no_space[0] == full_parts_no_space[1] and 
                            strict_parts_no_space[1] == full_parts_no_space[0]):
                            return True
                
                # 4. å¦‚æœ strict_name ä¸­æ²¡æœ‰é€—å·ï¼Œå°è¯•ä» full_name ä¸­æå–åŒ¹é…
                if ',' not in strict_name_lower and ',' in full_name_lower:
                    # ä¾‹å¦‚ strict_name="Xu Feng"ï¼Œfull_name="Feng, Xu"
                    parts = strict_name_lower.split()
                    # æ£€æŸ¥ strict_name çš„å„éƒ¨åˆ†æ˜¯å¦éƒ½åœ¨ full_name ä¸­
                    if all(part in full_name_lower for part in parts):
                        return True
                    
                    # ä¹Ÿå¤„ç†ç©ºæ ¼ç§»é™¤åçš„åŒ¹é…
                    parts_no_space = [p.replace(" ", "") for p in parts]
                    for part_no_space in parts_no_space:
                        if part_no_space and part_no_space not in full_no_space:
                            break
                    else:
                        # æ‰€æœ‰éƒ¨åˆ†éƒ½åŒ¹é…
                        if all(p.replace(" ", "") in full_no_space for p in parts):
                            return True
        
        return False

    def fetch_paper_data_stable(self, target_config, identifier, sort_mode, 
                                 allow_all_journals=False, existing_dois=None, target_limit=None):
        """
        æç¨³å¥çš„åˆ†é¡µä¸‹è½½ï¼šå°æ‰¹é‡ + åŸåœ°é‡è¯•
        
        å‚æ•°:
            target_config: ç›®æ ‡é…ç½®å­—å…¸
            identifier: ä½œè€…æ ‡è¯†ç¬¦
            sort_mode: æ’åºæ¨¡å¼ (mostrecent/mostcited)
            allow_all_journals: æ˜¯å¦å…è®¸æ‰€æœ‰æœŸåˆŠï¼ˆè¡¥å……æ¨¡å¼ç”¨ï¼Œé»˜è®¤ Falseï¼‰
            existing_dois: å·²æœ‰ DOI é›†åˆï¼Œç”¨äºå»é‡ï¼ˆé»˜è®¤ Noneï¼‰
            target_limit: ç›®æ ‡æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨LIMITï¼‰
        """
        name = target_config["name"]
        strict_names = target_config["strict_names"]
        tag = "æœ€æ–°" if sort_mode == "mostrecent" else "é«˜å¼•"
        
        results = []
        seen_dois = set()  # æœ¬æ¨¡å¼å†…å»é‡
        if existing_dois is None:
            existing_dois = set()  # é˜²æ­¢ None
        
        if target_limit is None:
            target_limit = LIMIT
        
        page = 1
        total_checked = 0  # ç»Ÿè®¡æ‰€æœ‰æ£€ç´¢çš„æ–‡ç« æ€»æ•°
        
        base_params = {
            "q": f"a {identifier}", 
            "sort": sort_mode,
            "size": BATCH_SIZE
        }
        
        mode_tag = "[è¡¥å……]" if allow_all_journals else ""
        print(f"\n   [â³] {name} [{tag}]{mode_tag} å¼€å§‹æ‰«æ (ç›®æ ‡ {target_limit} ç¯‡)...", flush=True)
        
        # å¾ªç¯ç›´åˆ°å‡‘å¤Ÿæ•°é‡æˆ–ç¿»é¡µè¿‡å¤š
        while len(results) < target_limit:
            params = base_params.copy()
            params["page"] = page
            
            # --- åŸåœ°é‡è¯•é€»è¾‘ ---
            success = False
            for attempt in range(MAX_RETRIES):
                try:
                    res = self.session.get(f"{self.BASE_URL}/literature", params=params, timeout=TIMEOUT)
                    data = res.json()
                    success = True
                    break # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
                except Exception as e:
                    print(f"      [âš ï¸] {name} Page {page} å¤±è´¥ (å°è¯• {attempt+1}/{MAX_RETRIES}): {e}", flush=True)
                    time.sleep(0.3) # ç¼©çŸ­ç­‰å¾…æ—¶é—´
            
            if not success:
                print(f"      [âŒ] {name} Page {page} å½»åº•å¤±è´¥ï¼Œè·³è¿‡æ­¤é¡µ", flush=True)
                page += 1
                continue # æ”¾å¼ƒè¿™ä¸€é¡µï¼Œå°è¯•ä¸‹ä¸€é¡µ
            
            # --- æ•°æ®å¤„ç† ---
            hits = data.get("hits", {}).get("hits", [])
            if not hits: 
                break # æ²¡æœ‰æ›´å¤šæ•°æ®äº†
            
            for hit in hits:
                if len(results) >= target_limit: break
                
                total_checked += 1  # è®¡æ•°æ‰€æœ‰æ£€ç´¢çš„æ–‡ç« 
                
                metadata = hit.get("metadata", {})
                authors_list = metadata.get("authors", [])
                
                # 1. è¿‡æ»¤å¤§å‹åˆä½œç»„
                if len(authors_list) > MAX_AUTHORS: 
                    continue
                
                # 2. ä¸¥æ ¼åå­—åŒ¹é…
                if not self.check_exact_match(authors_list, strict_names):
                    continue
                
                # 3. æå– DOI
                d_list = metadata.get("dois", [])
                doi_val = d_list[0].get("value") if d_list else None
                
                if not doi_val:
                    continue
                
                # 4. æœŸåˆŠè¿‡æ»¤ (ä»…åœ¨éè¡¥å……æ¨¡å¼ä¸‹)
                if not allow_all_journals:
                    is_allowed_journal = any(journal in doi_val for journal in ALLOWED_JOURNALS)
                    if not is_allowed_journal:
                        continue
                
                # 5. å»é‡æ£€æŸ¥ï¼ˆæœ¬æ¨¡å¼å†… + å·²æœ‰ DOIï¼‰
                if doi_val in seen_dois or doi_val in existing_dois:
                    continue
                
                # 6. æ·»åŠ ç»“æœ
                author_names = [a.get("full_name", "Unknown") for a in authors_list]
                authors_str = "; ".join(author_names)
                
                results.append({
                    "doi": doi_val,
                    "authors": authors_str
                })
                seen_dois.add(doi_val)
                print(f"      âœ“ [{tag}]{mode_tag} ç¬¬ {total_checked} ç¯‡: {doi_val} (å·²ä¿å­˜ {len(results)}/{target_limit})", flush=True)
            
            page += 1
            if page > MAX_PAGES: break # é˜²æ­¢æ­»å¾ªç¯

        print(f"      [â¬‡ï¸] {name} [{tag}]{mode_tag} å®Œæˆ! è·å– {len(results)} æ¡", flush=True)
        return sort_mode, results

# ================= â–¶ï¸ è¾…åŠ©å‡½æ•° =================

def parse_results_file(results_file):
    """
    è§£æ results.txtï¼Œè¿”å›æ•™å¸ˆä¿¡æ¯å­—å…¸
    è¿”å›æ ¼å¼: {
        "cn_name": {
            "completed": True/False,  # æ˜¯å¦æœ‰æ˜Ÿå·
            "dois": ["doi1", "doi2", ...],
            "line_start": è¡Œå·  # åå­—æ‰€åœ¨è¡Œå·
        }
    }
    """
    teachers_info = {}
    
    if not os.path.exists(results_file):
        return teachers_info
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        current_teacher = None
        line_num = 0
        
        for idx, line in enumerate(lines):
            line_stripped = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œç­‰å·è¡Œ
            if not line_stripped or line_stripped.startswith('='):
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ DOI
            if line_stripped.startswith('10.'):
                if current_teacher:
                    teachers_info[current_teacher]["dois"].append(line_stripped)
            else:
                # è¿™æ˜¯æ•™å¸ˆåå­—
                completed = line_stripped.endswith('+')
                cn_name = line_stripped.rstrip('+').strip()
                
                current_teacher = cn_name
                teachers_info[cn_name] = {
                    "completed": completed,
                    "dois": [],
                    "line_number": idx
                }
    
    except Exception as e:
        print(f"è§£æ results.txt å¤±è´¥: {e}")
    
    return teachers_info

def get_processed_teachers(results_file):
    """ä» results.txt ä¸­è¯»å–å·²å¤„ç†çš„ä¸­æ–‡å§“ååˆ—è¡¨ï¼ˆä¸å«æ˜Ÿå·çš„åŸå§‹åå­—ï¼‰"""
    processed = set()
    if not os.path.exists(results_file):
        return processed
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç­‰å·åˆ†éš”è¡Œæˆ– DOI
                if line and not line.startswith('=') and not line.startswith('10.'):
                    # ç§»é™¤å¯èƒ½çš„æ˜Ÿå·ï¼Œè¿™æ˜¯ä¸­æ–‡å§“å
                    cn_name = line.rstrip('+').strip()
                    processed.add(cn_name)
    except Exception as e:
        print(f"è¯»å–å·²å¤„ç†æ•™å¸ˆåˆ—è¡¨å¤±è´¥: {e}")
    
    return processed

def find_teacher_needing_supplement(results_file):
    """
    æ‰¾åˆ°ç¬¬ä¸€ä¸ªéœ€è¦è¡¥å……è®ºæ–‡çš„æ•™å¸ˆï¼ˆæ²¡æœ‰æ˜Ÿå·çš„æ•™å¸ˆï¼‰
    è¿”å›: (cn_name, existing_dois) æˆ– (None, None)
    """
    teachers_info = parse_results_file(results_file)
    
    for cn_name, info in teachers_info.items():
        if not info["completed"]:
            return cn_name, set(info["dois"])
    
    return None, None

def update_teacher_status_in_file(results_file, cn_name, mark_completed=True):
    """
    åœ¨ results.txt ä¸­ç»™æ•™å¸ˆåå­—æ·»åŠ æˆ–ç§»é™¤è¡¥å……æ ‡è®°ï¼ˆå°¾éƒ¨+ï¼‰
    mark_completed=True: æ·»åŠ +
    mark_completed=False: ç§»é™¤+
    """
    if not os.path.exists(results_file):
        return
    
    try:
        with open(results_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        modified = False
        for idx, line in enumerate(lines):
            line_stripped = line.strip()
            # è·³è¿‡ç©ºè¡Œå’Œç­‰å·
            if not line_stripped or line_stripped.startswith('=') or line_stripped.startswith('10.'):
                continue
            
            # è·å–ä¸å«è¡¥å……æ ‡è®°çš„åå­—
            current_name = line_stripped.rstrip('+').strip()
            
            if current_name == cn_name:
                if mark_completed and not line_stripped.endswith('+'):
                    # æ·»åŠ è¡¥å……æ ‡è®°ï¼ˆæ”¾åœ¨åå­—åï¼‰
                    lines[idx] = current_name + '+\n'
                    modified = True
                elif not mark_completed and line_stripped.endswith('+'):
                    # ç§»é™¤è¡¥å……æ ‡è®°
                    lines[idx] = current_name + '\n'
                    modified = True
                break
        
        if modified:
            with open(results_file, 'w', encoding='utf-8') as f:
                f.writelines(lines)
            status = "å·²å®Œæˆ" if mark_completed else "æœªå®Œæˆ"
            print(f"   âœ“ å·²æ ‡è®° {cn_name} ä¸º {status}")
    
    except Exception as e:
        print(f"æ›´æ–°æ•™å¸ˆçŠ¶æ€å¤±è´¥: {e}")

def get_first_unprocessed_target(targets, processed_names):
    """ä» TARGETS ä¸­è·å–ç¬¬ä¸€ä¸ªæœªå¤„ç†çš„æ•™å¸ˆ"""
    for target in targets:
        cn_name = target.get('cn_name', '')
        if cn_name and cn_name not in processed_names:
            return target
    return None

# ================= â–¶ï¸ ä¸»ç¨‹åº =================

if __name__ == "__main__":
    # è‡ªåŠ¨åˆ›å»ºç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        try:
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            print(f"ğŸ“‚ å·²åˆ›å»ºç›®å½•: {OUTPUT_DIR}")
        except OSError as e:
            print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {e}")
            exit(1)

    fetcher = StableFetcher()
    
    print(f"ğŸš€ å¯åŠ¨å¢é‡æ¨¡å¼ (Batch={BATCH_SIZE}, Limit={LIMIT})...")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}\n")
    
    # 1. ä» id.txt åŠ è½½ TARGETS
    id_file_path = os.path.join(CURRENT_DIR, "id.txt")
    if os.path.exists(id_file_path):
        try:
            with open(id_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # æ‰§è¡Œ id.txt è·å– TARGETS
                local_vars = {}
                exec(content, {}, local_vars)
                if 'TARGETS' in local_vars:
                    TARGETS = local_vars['TARGETS']
                    print(f"âœ“ ä» id.txt åŠ è½½äº† {len(TARGETS)} ä¸ªç›®æ ‡æ•™å¸ˆ")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½ id.txtï¼Œä½¿ç”¨é»˜è®¤ TARGETS: {e}")
    
    # 2. è¯»å–å·²å¤„ç†çš„æ•™å¸ˆåˆ—è¡¨
    processed_teachers = get_processed_teachers(OUTPUT_PATH)
    print(f"ğŸ“‹ å·²å¤„ç†æ•™å¸ˆ: {len(processed_teachers)} äºº")
    if processed_teachers:
        print(f"   {', '.join(processed_teachers)}\n")
    
    # 3. æ‰¾ç¬¬ä¸€ä¸ªæœªå¤„ç†çš„æ–°æ•™å¸ˆ
    target = get_first_unprocessed_target(TARGETS, processed_teachers)
    
    if not target:
        print("\nâœ… æ‰€æœ‰æ•™å¸ˆéƒ½å·²å¤„ç†å®Œæˆï¼")
        exit(0)
    
    print(f"\nğŸ¯ æ–°æ•™å¸ˆ: {target.get('cn_name')} ({target.get('name')})\n")
    print("-" * 60)
    
    # 4. è·å–è¯¥æ•™å¸ˆçš„ BAI
    identifier = fetcher.get_bai_from_record_id(target["name"], target["id"])
    if not identifier:
        print(f"âŒ æ— æ³•è·å–æ•™å¸ˆ {target['name']} çš„æ ‡è¯†ç¬¦")
        exit(1)
    
    # 5. ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨æœŸåˆŠé™åˆ¶ä¸‹è½½ï¼ˆæœ€æ–°å’Œé«˜å¼•å„10ç¯‡ï¼‰
    print(f"\nâš¡ ç¬¬ä¸€é˜¶æ®µï¼šé™åˆ¶æœŸåˆŠä¸‹è½½ï¼ˆæœ€æ–°10ç¯‡ + é«˜å¼•10ç¯‡ï¼‰...\n")
    
    final_data = {}
    final_data[target["name"]] = {}
    name = target["name"]
    cn_name = target.get("cn_name", name)
    
    # å¹¶å‘è·å– mostrecent å’Œ mostcitedï¼Œå„10ç¯‡
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        future_map = {}
        f1 = executor.submit(
            fetcher.fetch_paper_data_stable,
            target, identifier, "mostrecent",
            allow_all_journals=False,
            existing_dois=None,
            target_limit=10  # æœ€æ–°10ç¯‡
        )
        future_map[f1] = "mostrecent"
        
        f2 = executor.submit(
            fetcher.fetch_paper_data_stable,
            target, identifier, "mostcited",
            allow_all_journals=False,
            existing_dois=None,
            target_limit=10  # é«˜å¼•10ç¯‡
        )
        future_map[f2] = "mostcited"
        
        for future in concurrent.futures.as_completed(future_map):
            mode = future_map[future]
            try:
                _, data_list = future.result()
                final_data[name][mode] = data_list
            except Exception as e:
                print(f"ä»»åŠ¡å¼‚å¸¸: {e}")
                final_data[name][mode] = []
    
    # 6. åˆå¹¶ç¬¬ä¸€é˜¶æ®µçš„ DOI å¹¶è¿›è¡Œå»é‡
    all_dois = []
    all_dois_unique = []  # å»é‡åçš„ DOI
    seen_dois_set = set()
    
    recents = final_data[name].get("mostrecent", [])
    cited = final_data[name].get("mostcited", [])
    
    for item in recents:
        doi = item['doi']
        all_dois.append(doi)
        if doi not in seen_dois_set:
            all_dois_unique.append(doi)
            seen_dois_set.add(doi)
    
    for item in cited:
        doi = item['doi']
        all_dois.append(doi)
        if doi not in seen_dois_set:
            all_dois_unique.append(doi)
            seen_dois_set.add(doi)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤
    if len(all_dois_unique) < len(all_dois):
        print(f"\nâš ï¸ ç¬¬ä¸€é˜¶æ®µå‘ç°é‡å¤ DOIï¼šæ”¶é›† {len(all_dois)} ç¯‡ï¼Œå»é‡å {len(all_dois_unique)} ç¯‡")
    
    supplement_mode = False
    cited_supplement = []  # åˆå§‹åŒ–è¡¥å……æ•°æ®
    
    # å¦‚æœå»é‡åä¸è¶³ 20 ç¯‡ï¼Œå¯åŠ¨è¡¥å……æ¨¡å¼
    if len(all_dois_unique) < 20:
        supplement_mode = True
        existing_dois = set(all_dois_unique)
        needed = 20 - len(all_dois_unique)
        
        print(f"\nâš ï¸ å»é‡åä»…è·å– {len(all_dois_unique)} ç¯‡ï¼Œä¸è¶³ 20 ç¯‡")
        print(f"ğŸ”„ å¯åŠ¨è¡¥å……æ¨¡å¼ï¼šæ— æœŸåˆŠé™åˆ¶ï¼Œä»…ä»é«˜å¼•è¡¥å…… {needed} ç¯‡...\n")
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ— æœŸåˆŠé™åˆ¶ï¼Œä»…ä» mostcited è¡¥å……
        _, cited_supplement = fetcher.fetch_paper_data_stable(
            target, identifier, "mostcited",
            allow_all_journals=True,
            existing_dois=existing_dois,
            target_limit=needed
        )
        
        # è¿½åŠ åˆ° mostcited æ•°æ®
        final_data[name]["mostcited"].extend(cited_supplement)
        
        # é‡æ–°åˆå¹¶æ‰€æœ‰ DOI
        all_dois = []
        recents = final_data[name].get("mostrecent", [])
        cited = final_data[name].get("mostcited", [])
        
        for item in recents:
            all_dois.append(item['doi'])
        for item in cited:
            all_dois.append(item['doi'])
    
    # 7. å†™å…¥æ–‡ä»¶
    print(f"\n{'='*60}")
    print("ğŸ“ æ­£åœ¨å†™å…¥ç»“æœæ–‡ä»¶...")
    print(f"{'='*60}\n")
    
    try:
        with open(OUTPUT_PATH, "a", encoding="utf-8") as f:
            if supplement_mode:
                # è¡¥å……æ¨¡å¼ï¼šåˆ†ä¸¤ä¸ªæ¡ç›®å†™å…¥
                
                # ç¬¬ä¸€æ¡ç›®ï¼šç¬¬ä¸€é˜¶æ®µçš„ç»“æœï¼ˆä¸å¸¦æ˜Ÿå·ï¼Œå»é‡åï¼‰
                phase1_dois_unique = []
                phase1_seen = set()
                
                recents = final_data[name].get("mostrecent", [])
                cited_phase1 = final_data[name].get("mostcited", [])
                
                # åªå–ç¬¬ä¸€é˜¶æ®µçš„æ•°æ®ï¼ˆéœ€è¦åŒºåˆ†ç¬¬ä¸€é˜¶æ®µå’Œè¡¥å……é˜¶æ®µï¼‰
                # é€šè¿‡é•¿åº¦åˆ¤æ–­ï¼šmostrecent éƒ½æ˜¯ç¬¬ä¸€é˜¶æ®µï¼Œmostcited å¯èƒ½æ··åˆ
                for item in recents:
                    doi = item['doi']
                    if doi not in phase1_seen:
                        phase1_dois_unique.append(doi)
                        phase1_seen.add(doi)
                
                # mostcited ä¸­ï¼Œå‰ len(all_dois_before_supplement) æ˜¯ç¬¬ä¸€é˜¶æ®µ
                all_dois_before_supplement_count = len(all_dois_unique)
                recents_count = len(recents)
                cited_phase1_count = all_dois_before_supplement_count - recents_count
                
                for i, item in enumerate(cited_phase1[:cited_phase1_count]):
                    doi = item['doi']
                    if doi not in phase1_seen:
                        phase1_dois_unique.append(doi)
                        phase1_seen.add(doi)
                
                # å†™å…¥ç¬¬ä¸€é˜¶æ®µï¼ˆä¸å¸¦æ ‡è®°ï¼Œå»é‡åï¼‰
                f.write("=" * 50 + "\n")
                f.write(f"{cn_name}\n")
                f.write("=" * 50 + "\n")
                for doi in phase1_dois_unique:
                    f.write(f"{doi}\n")
                f.write("\n")
                
                # ç¬¬äºŒæ¡ç›®ï¼šè¡¥å……é˜¶æ®µçš„ç»“æœï¼ˆåå­—ååŠ +ï¼‰
                supplement_dois = [item['doi'] for item in cited_supplement]
                
                f.write("=" * 50 + "\n")
                f.write(f"{cn_name}+\n")
                f.write("=" * 50 + "\n")
                for doi in supplement_dois:
                    f.write(f"{doi}\n")
                f.write("\n")
                
                print(f"âœ… æ·»åŠ å®Œæˆï¼")
                print(f"ğŸ“‚ ç¬¬ä¸€é˜¶æ®µï¼š{cn_name} æ·»åŠ  {len(phase1_dois_unique)} ç¯‡ï¼ˆæœŸåˆŠé™åˆ¶ï¼Œå·²å»é‡ï¼‰")
                print(f"ğŸ“‚ ç¬¬äºŒé˜¶æ®µï¼š{cn_name}+ æ·»åŠ  {len(supplement_dois)} ç¯‡ï¼ˆè¡¥å……æ¨¡å¼ï¼‰")
                
            else:
                # æ— è¡¥å……æ¨¡å¼ï¼šåªå†™ä¸€ä¸ªæ¡ç›®ï¼ˆä¸å¸¦æ˜Ÿå·ï¼Œå»é‡åï¼‰
                f.write("=" * 50 + "\n")
                f.write(f"{cn_name}\n")
                f.write("=" * 50 + "\n")
                
                # è¾“å‡ºå»é‡åçš„ DOI
                for doi in all_dois_unique[:20]:  # åªå–å‰20ä¸ª
                    f.write(f"{doi}\n")
                f.write("\n")
                
                print(f"âœ… æ·»åŠ å®Œæˆï¼")
                print(f"ğŸ“‚ å·²ä¸º {cn_name} æ·»åŠ  {len(all_dois_unique)} ç¯‡è®ºæ–‡ï¼ˆæœŸåˆŠé™åˆ¶ï¼Œå·²å»é‡ï¼‰")
        
        print(f"ğŸ“‚ ç»“æœæ–‡ä»¶: {OUTPUT_PATH}")
        
    except Exception as e:
        print(f"âŒ å†™å…¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
