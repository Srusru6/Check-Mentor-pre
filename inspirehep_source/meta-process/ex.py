import argparse
import json
import os
import sys
import time
import uuid
import shutil
import zipfile
import hashlib
import random
import requests
import re
from urllib.parse import unquote
import configparser
import concurrent.futures
import threading
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Optional, Any, Tuple
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from datetime import datetime

# ==========================================
# 1. Configuration & Utils
# ==========================================

PROJ_ROOT = Path(__file__).resolve().parents[2]
CONFIG_PATH = PROJ_ROOT / 'config.ini'
PROGRESS_FILE = PROJ_ROOT / 'inspirehep_source/meta-process/processed.txt'
PROGRESS_LOCK = threading.Lock()

# Import generic downloader
sys.path.append(str(PROJ_ROOT / 'DOI_source'))
try:
    import download as doi_downloader
except ImportError:
    doi_downloader = None
    print("Warning: Could not import doi_downloader from DOI_source")

def load_config():
    config = configparser.ConfigParser()
    if CONFIG_PATH.exists():
        config.read(CONFIG_PATH, encoding='utf-8')
    return config

def load_progress() -> set[str]:
    if not PROGRESS_FILE.exists():
        return set()
    try:
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            return set(line.strip() for line in f if line.strip())
    except Exception:
        return set()

def save_progress(record: str):
    with PROGRESS_LOCK:
        try:
            with open(PROGRESS_FILE, 'a', encoding='utf-8') as f:
                f.write(f"{record}\n")
        except Exception as e:
            print(f"Failed to save progress: {e}")

def get_sampling_cfg(config) -> Tuple[Optional[int], Optional[int]]:
    try:
        if config.has_section('download'):
            n_raw = config.get('download', 'sample_threshold', fallback='').strip()
            m_raw = config.get('download', 'sample_size', fallback='').strip()
            n_val = int(n_raw) if n_raw.isdigit() else None
            m_val = int(m_raw) if m_raw.isdigit() else None
            if (n_val is not None and n_val <= 0) or (m_val is not None and m_val <= 0):
                return None, None
            return n_val, m_val
    except Exception:
        pass
    return None, None

def get_worker_cfg(config) -> Tuple[int, int]:
    wm, wr = 4, 6
    try:
        if config.has_section('download'):
            wmr = config.get('download', 'workers_main', fallback='').strip()
            wrr = config.get('download', 'workers_related', fallback='').strip()
            if wmr.isdigit(): wm = max(1, int(wmr))
            if wrr.isdigit(): wr = max(1, int(wrr))
    except Exception:
        pass
    return wm, wr

def get_limit_cfg(config) -> Tuple[int, int]:
    lr, lc = 10, 100
    try:
        # Prioritize [limits] section if it exists
        if config.has_section('limits'):
            r_raw = config.get('limits', 'limit_ref', fallback='').strip()
            c_raw = config.get('limits', 'limit_cited', fallback='').strip()
            if r_raw.isdigit(): lr = int(r_raw)
            if c_raw.isdigit(): lc = int(c_raw)
        # Fallback to [download] section
        elif config.has_section('download'):
            r_raw = config.get('download', 'top_n_ref', fallback='').strip()
            c_raw = config.get('download', 'top_n_cited', fallback='').strip()
            if r_raw.isdigit(): lr = int(r_raw)
            if c_raw.isdigit(): lc = int(c_raw)
    except Exception:
        pass
    return lr, lc

def get_young_author_years(config) -> int:
    val_env = os.getenv("YOUNG_AUTHOR_YEARS")
    if val_env and val_env.isdigit():
        return int(val_env)
    try:
        if config.has_section("metadata") and config.has_option("metadata", "young_author_years"):
            years = config.get("metadata", "young_author_years").strip()
            if years.isdigit():
                return int(years)
    except Exception:
        pass
    return 5

def ensure_dir(path: Path):
    path.mkdir(parents=True, exist_ok=True)

def _sanitize_dir_name(name: str) -> str:
    return (
        name.replace('/', '_').replace('\\', '_').replace(':', '_')
        .replace('?', '_').replace('*', '_').replace('"', "'")
        .replace('<', '_').replace('>', '_').replace('|', '_')
    )

def parse_year_month(pub_date: Any) -> Dict[str, int]:
    year_month: Dict[str, int] = {}
    if pub_date is None:
        return year_month
    try:
        if isinstance(pub_date, int):
            if 1000 <= pub_date <= 9999:
                year_month["year"] = pub_date
                return year_month
        if isinstance(pub_date, str) and pub_date.strip():
            txt = pub_date.strip()
            for fmt in ("%Y-%m-%d", "%Y-%m", "%Y"):
                try:
                    dt = datetime.strptime(txt, fmt)
                    year_month["year"] = dt.year
                    if fmt in ("%Y-%m-%d", "%Y-%m"):
                        year_month["month"] = dt.month
                    return year_month
                except ValueError:
                    continue
    except Exception:
        pass
    return year_month

# ==========================================
# 2. OpenAlex Utils
# ==========================================

OPENALEX_BASE = "https://api.openalex.org"

def normalize_doi(doi: str) -> str:
    if not doi: return ""
    doi = unquote(doi).strip()
    doi = re.sub(r"^(https?://(dx\.)?doi\.org/|doi:)", "", doi, flags=re.I)
    return doi.strip().strip(' .;')

def get_openalex_id_for_doi(doi: str) -> Optional[str]:
    doi = normalize_doi(doi)
    if not doi: return None
    try:
        url = f"{OPENALEX_BASE}/works/doi:{requests.utils.quote(doi, safe='')}"
        r = requests.get(url, headers={'User-Agent': 'CheckMentor/1.0'}, timeout=30)
        if r.status_code == 404: return None
        r.raise_for_status()
        data = r.json()
        oid = data.get("id")
        return oid.rsplit("/", 1)[-1] if oid else None
    except Exception:
        return None

def get_citing_dois_openalex(doi: str, limit: int = 100) -> List[str]:
    oid = get_openalex_id_for_doi(doi)
    if not oid: return []
    
    url = f"{OPENALEX_BASE}/works"
    params = {
        "filter": f"cites:{oid}",
        "select": "doi",
        "per_page": min(limit, 200),
        "sort": "publication_year:desc",
    }
    
    dois = set()
    try:
        while len(dois) < limit:
            r = requests.get(url, params=params, headers={'User-Agent': 'CheckMentor/1.0'}, timeout=60)
            r.raise_for_status()
            data = r.json()
            results = data.get("results", [])
            if not results: break
            
            for item in results:
                d = item.get("doi")
                if d:
                    nd = normalize_doi(d)
                    if nd: dois.add(nd)
            
            if len(dois) >= limit: break
            
            cursor = (data.get("meta") or {}).get("next_cursor")
            if not cursor: break
            params["cursor"] = cursor
            
    except Exception as e:
        print(f"OpenAlex error: {e}")
        
    return list(dois)[:limit]

# ==========================================
# 3. InspireHEP Client
# ==========================================

class InspireHEPClient:
    BASE_URL = "https://inspirehep.net/api"
    
    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            "Accept": "application/json",
            "User-Agent": "CheckMentor-InspireHEP-Client/1.0"
        })
        retries = Retry(
            total=5, 
            backoff_factor=1.0, 
            status_forcelist=(429, 500, 502, 503, 504),
            allowed_methods=("GET", "HEAD")
        )
        adapter = HTTPAdapter(pool_connections=64, pool_maxsize=64, max_retries=retries)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def _request(self, url: str, params: Optional[Dict] = None, stream: bool = False) -> requests.Response:
        # Random sleep to distribute requests and avoid hitting rate limits
        time.sleep(random.uniform(0.5, 1.5))
        response = self.session.get(url, params=params, timeout=self.timeout, stream=stream)
        response.raise_for_status()
        return response

    def search_literature(self, query: str, size: int = 10, page: int = 1) -> Dict:
        url = f"{self.BASE_URL}/literature"
        params = {"q": query, "size": size, "page": page}
        return self._request(url, params=params).json()

    def find_record_by_doi(self, doi: str) -> Optional[Dict]:
        query = f"doi:{doi}"
        results = self.search_literature(query, size=1)
        hits = results.get("hits", {}).get("hits", [])
        return hits[0] if hits else None
    
    def get_record(self, record_id: str) -> Dict:
        url = f"{self.BASE_URL}/literature/{record_id}"
        return self._request(url).json()

    def get_citations(self, record_id: str, size: int = 50, page: int = 1) -> Dict:
        query = f"refersto:recid:{record_id}"
        return self.search_literature(query, size=size, page=page)
    
    def get_metadata(self, record_id: str) -> Dict:
        record = self.get_record(record_id)
        metadata = record.get("metadata", {})
        return {
            "record_id": record_id,
            "title": metadata.get("titles", [{}])[0].get("title", "N/A"),
            "authors": [author.get("full_name", "N/A") for author in metadata.get("authors", [])],
            "abstract": metadata.get("abstracts", [{}])[0].get("value", "N/A"),
            "publication_date": metadata.get("preprint_date") or metadata.get("publication_info", [{}])[0].get("year", "N/A"),
            "arxiv_id": metadata.get("arxiv_eprints", [{}])[0].get("value", "N/A"),
            "doi": metadata.get("dois", [{}])[0].get("value", "N/A"),
            "citations": metadata.get("citation_count", 0),
            "keywords": [kw.get("value", "") for kw in metadata.get("keywords", [])],
            "inspire_url": f"https://inspirehep.net/literature/{record_id}",
        }
    
    def download_file(self, url: str, output_path: str) -> None:
        response = self._request(url, stream=True)
        with open(output_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk: f.write(chunk)

# ==========================================
# 3. Downloader Logic (Migrated)
# ==========================================

AUTHOR_FIRST_YEAR_CACHE: Dict[str, Optional[int]] = {}
CACHE_LOCK = threading.Lock()

def extract_year_from_inspire_metadata(md: Dict[str, Any]) -> Optional[int]:
    preprint = md.get("preprint_date")
    ym = parse_year_month(preprint)
    if "year" in ym: return ym["year"]
    pub_infos = md.get("publication_info", []) or []
    if pub_infos and isinstance(pub_infos, list):
        year = pub_infos[0].get("year")
        if isinstance(year, (int, str)): return int(year)
    earliest = md.get("earliest_date") or md.get("earliestdate")
    ym2 = parse_year_month(earliest)
    if "year" in ym2: return ym2["year"]
    return None

def get_author_first_year(client: InspireHEPClient, author_name: str) -> Optional[int]:
    with CACHE_LOCK:
        if author_name in AUTHOR_FIRST_YEAR_CACHE:
            return AUTHOR_FIRST_YEAR_CACHE[author_name]
    
    min_year: Optional[int] = None
    try:
        results = client.search_literature(f"author:\"{author_name}\"", size=25)
        hits = results.get("hits", {}).get("hits", [])
        for h in hits:
            y = extract_year_from_inspire_metadata(h.get("metadata", {}))
            if y is not None:
                if min_year is None or y < min_year: min_year = y
        if min_year is None:
            results2 = client.search_literature(f"find a {author_name}", size=25)
            hits2 = results2.get("hits", {}).get("hits", [])
            for h in hits2:
                y = extract_year_from_inspire_metadata(h.get("metadata", {}))
                if y is not None:
                    if min_year is None or y < min_year: min_year = y
    except Exception:
        min_year = None
    
    with CACHE_LOCK:
        AUTHOR_FIRST_YEAR_CACHE[author_name] = min_year
    return min_year

def compute_young_author_fields(client: InspireHEPClient, authors: List[str], pub_year: Optional[int], years_window: int) -> Dict[str, Any]:
    result = {"young_scholar_index": -1, "young_authors": [], "author_first_years": {}, "young_author_years_window": years_window}
    if not authors or pub_year is None: return result
    young_any = False
    for name in authors:
        fy = get_author_first_year(client, name)
        if fy is not None:
            result["author_first_years"][name] = fy
            if 0 <= (pub_year - fy) <= years_window:
                young_any = True
                result["young_authors"].append(name)
    result["young_scholar_index"] = 1 if young_any else -1
    return result

def is_prl_or_prd(info_or_list: Any) -> bool:
    if not info_or_list: return False
    if isinstance(info_or_list, dict):
        infos = [info_or_list]
    elif isinstance(info_or_list, list):
        infos = info_or_list
    else:
        return False
        
    for info in infos:
        if not isinstance(info, dict): continue
        title = info.get('journal_title', '').lower().replace('.', '')
        if 'phys rev lett' in title or 'physical review letters' in title:
            return True
        if 'phys rev d' in title or 'physical review d' in title:
            return True
    return False

def fetch_related_ids(client: InspireHEPClient, record_id: str, kind: str, limit: int, doi: Optional[str] = None) -> List[str]:
    ids: List[str] = []
    try:
        if kind == "citations" and doi:
            return get_citing_dois_openalex(doi, limit)

        if kind == "references":
            record = client.get_record(record_id)
            refs = (record.get('metadata', {}) or {}).get('references', []) or []
            for ref in refs:
                # Filter by Journal (PRL/PRD)
                if not is_prl_or_prd(ref.get('publication_info')):
                    continue

                rec = ref.get('record') if isinstance(ref, dict) else None
                if isinstance(rec, dict):
                    ref_url = rec.get('$ref') or rec.get('$REF') or rec.get('url')
                    if isinstance(ref_url, str) and '/literature/' in ref_url:
                        try:
                            rid = ref_url.rstrip('/').split('/')[-1]
                            if rid: ids.append(str(rid))
                        except Exception: pass
                    cn = rec.get('control_number') or rec.get('controlNumber')
                    if cn: ids.append(str(cn))
                if len(ids) >= limit: break
        else:
            # Fetch citations
            data = client.get_citations(record_id, size=limit)
            hits = (data.get('hits', {}) or {}).get('hits', []) or []
            for h in hits:
                rid = h.get('id') or (h.get('metadata', {}) or {}).get('control_number')
                if rid: ids.append(str(rid))
                if len(ids) >= limit: break
    except Exception as e:
        print(f"âš ï¸ èŽ·å–{kind}å¤±è´¥: {e}")
    return ids[:limit]

def _pdf_url_from_inspire_metadata_raw(md_raw: Dict[str, Any]) -> Optional[str]:
    docs = md_raw.get("documents", []) or []
    for doc in docs:
        key = (doc.get("key") or "").lower()
        url = doc.get("url") or doc.get("source")
        if key.endswith(".pdf") and url: return url
        if url and str(url).lower().endswith('.pdf'): return url
    eprints = md_raw.get("arxiv_eprints", []) or []
    if eprints:
        arxiv_id = eprints[0].get("value")
        if arxiv_id: return f"https://arxiv.org/pdf/{arxiv_id}.pdf"
    return None

def dir_name_from_metadata(meta_norm: Dict[str, Any]) -> str:
    doi = meta_norm.get('doi')
    if doi: return _sanitize_dir_name(doi)
    aid = meta_norm.get('arxiv_id')
    if aid: return _sanitize_dir_name(aid)
    rid = meta_norm.get('record_id') or 'unknown'
    return _sanitize_dir_name(str(rid))

def to_items_metadata(record_meta: Dict[str, Any], role: Optional[str] = None) -> Dict[str, Any]:
    item: Dict[str, Any] = {
        "title": record_meta.get("title", ""),
        "doi": record_meta.get("doi", ""),
        "authors": record_meta.get("authors", []),
    }
    published = parse_year_month(record_meta.get("publication_date"))
    if published: item["published"] = published
    item["record_id"] = record_meta.get("record_id")
    item["inspire_url"] = record_meta.get("inspire_url")
    item["citations_count"] = record_meta.get("citations", 0)
    if role: item["role"] = role
    return item

def process_one_by_doi(client: InspireHEPClient, doi: str, out_dir: Path, download: bool = True, years_window: int = 5) -> Dict[str, Any]:
    ensure_dir(out_dir)
    
    base_name = _sanitize_dir_name(doi)
    pdf_path = out_dir / f"{base_name}.pdf"
    meta_path = out_dir / f"{base_name}_metadata.json"

    # Early check: if metadata exists and (PDF exists or not downloading), skip API
    if meta_path.exists():
        try:
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta_norm = json.load(f)
            
            if not download or pdf_path.exists():
                print(f"  [Skipped] {doi} (Files exist)")
                item = to_items_metadata(meta_norm)
                pub_year = item.get("published", {}).get("year")
                ya = compute_young_author_fields(client, item.get("authors", []), pub_year, years_window)
                item.update(ya)
                return item
        except Exception:
            pass

    hit = client.find_record_by_doi(doi)
    if not hit: raise ValueError(f"æœªæ‰¾åˆ° DOI={doi} çš„è®°å½•")
    rid = str(hit.get('id'))
    md_raw = hit.get('metadata', {}) or {}
    meta_norm = client.get_metadata(rid)
    item = to_items_metadata(meta_norm)
    
    pdf_filename = f"{_sanitize_dir_name(doi)}.pdf"
    pdf_path = out_dir / pdf_filename
    
    if download:
        if pdf_path.exists():
            print(f"  [Skipped Download] {pdf_filename} exists")
        else:
            pdf_url = _pdf_url_from_inspire_metadata_raw(md_raw)
            if pdf_url:
                try:
                    client.download_file(pdf_url, str(pdf_path))
                except Exception as e:
                    print(f"è­¦å‘Š: æ— æ³•ä¸‹è½½ PDF ({pdf_url}): {e}")
    
    meta_path = out_dir / f"{_sanitize_dir_name(doi)}_metadata.json"
    if pdf_path.exists():
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta_norm, f, ensure_ascii=False, indent=2)
    
    pub_year = item.get("published", {}).get("year")
    ya = compute_young_author_fields(client, item.get("authors", []), pub_year, years_window)
    item.update(ya)
    return item

def process_one_by_record_id_using_url(client: InspireHEPClient, record_id: str, out_dir: Path, download: bool = True, years_window: int = 5) -> Dict[str, Any]:
    ensure_dir(out_dir)
    rec = client.get_record(record_id)
    md_raw = rec.get('metadata', {}) or {}
    meta_norm = client.get_metadata(record_id)
    item = to_items_metadata(meta_norm)
    
    base_name = dir_name_from_metadata(meta_norm)
    pdf_path = out_dir / f"{base_name}.pdf"
    meta_path = out_dir / f"{base_name}_metadata.json"
    
    if download:
        if pdf_path.exists():
            print(f"  [Skipped Download] {base_name}.pdf exists")
        else:
            pdf_url = _pdf_url_from_inspire_metadata_raw(md_raw)
            if pdf_url:
                try:
                    client.download_file(pdf_url, str(pdf_path))
                except Exception as e:
                    print(f"è­¦å‘Š: å…³è”æ–‡çŒ® PDF ä¸‹è½½å¤±è´¥ ({pdf_url}): {e}")
    
    if pdf_path.exists() and not meta_path.exists():
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(meta_norm, f, ensure_ascii=False, indent=2)
            
    pub_year = item.get("published", {}).get("year")
    ya = compute_young_author_fields(client, item.get("authors", []), pub_year, years_window)
    item.update(ya)
    return item

# ==========================================
# 4. PDF2MD Logic (Migrated)
# ==========================================

def _build_header(token: str) -> dict:
    return {"Content-Type": "application/json", "Authorization": f"Bearer {token}"}

def _file_signature(path: Path, sample_bytes: int = 4 * 1024 * 1024) -> str:
    try:
        size = path.stat().st_size
        h = hashlib.sha1()
        with open(path, 'rb') as f:
            chunk = f.read(sample_bytes)
            h.update(chunk)
        return f"{size}-" + h.hexdigest()
    except Exception:
        return f"0-err-{path.name}"

def _is_converted_rel(output_dir: Path, rel_pdf: Path) -> bool:
    return (output_dir / rel_pdf.with_suffix('.md')).exists() or (output_dir / rel_pdf.with_suffix('.zip')).exists()

def _gather_candidates_and_duplicates(file_dir: Path, output_dir: Path) -> tuple[dict[str, list[str]], dict[str, str]]:
    rel_pdfs = [p.relative_to(file_dir) for p in file_dir.rglob('*.pdf')]
    groups: dict[str, list[Path]] = defaultdict(list)
    for rel in rel_pdfs:
        groups[_file_signature(file_dir / rel)].append(rel)
    
    uniques: dict[str, list[str]] = {}
    duplicates_map: dict[str, str] = {}
    for sig, rel_list in groups.items():
        primary_rel = next((r for r in rel_list if _is_converted_rel(output_dir, r)), None)
        if not primary_rel:
            primary_rel = next((r for r in rel_list if not _is_converted_rel(output_dir, r)), rel_list[0])
        
        for rel in rel_list:
            if rel != primary_rel:
                duplicates_map[str(rel)] = str(primary_rel)
        
        if not _is_converted_rel(output_dir, primary_rel):
            uniques.setdefault(primary_rel.stem, []).append(str(primary_rel))
            
    return uniques, duplicates_map

def batch_upload(file_names: list[str], unique_dict: dict[str, list[str]], file_dir: Path, header: dict) -> str | None:
    url = "https://mineru.org.cn/api/v4/file-urls/batch"
    data = {
        "is_ocr": False, "enable_formula": True, "enable_table": True,
        "model_version": "vlm", "language": None, "is_chem": False,
        "files": [{"name": f"{fn}.pdf", "data_id": str(uuid.uuid4())} for fn in file_names]
    }
    try:
        res = requests.post(url, headers=header, json=data)
        if res.status_code == 200 and res.json().get("code") == 0:
            batch_id = res.json()["data"]["batch_id"]
            urls = res.json()["data"]["file_urls"]
            
            def upload_one(idx_url):
                idx, u = idx_url
                try:
                    with open(file_dir / unique_dict[file_names[idx]][0], 'rb') as f:
                        if requests.put(u, data=f).status_code == 200:
                            print(f"Uploaded: {file_names[idx]}")
                except Exception as e:
                    print(f"Upload failed for {file_names[idx]}: {e}")

            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                list(executor.map(upload_one, enumerate(urls)))
                
            return batch_id
    except Exception as e:
        print(e)
    return None

def batch_retrieve(batch_id: str, unique_dict: dict[str, list[str]], file_dir: Path, output_dir: Path, header: dict):
    url = f"https://mineru.org.cn/api/v4/extract-results/batch/{batch_id}"
    done_files = set()
    start_time = time.time()
    MAX_WAIT_SECONDS = 600  # 10 minutes timeout for batch processing
    
    def download_one(item):
        fname = item.get("file_name", "")
        zip_url = item.get("full_zip_url")
        if zip_url:
            stem = Path(fname).stem
            path = unique_dict.get(stem, [stem])[0]
            out_path = output_dir / Path(path).with_suffix(".zip")
            out_path.parent.mkdir(parents=True, exist_ok=True)
            try:
                with requests.get(zip_url, stream=True, timeout=60) as r, open(out_path, 'wb') as f:
                    shutil.copyfileobj(r.raw, f)
                print(f"Downloaded: {out_path}")
            except Exception as e:
                print(f"Download failed for {fname}: {e}")

    while True:
        if time.time() - start_time > MAX_WAIT_SECONDS:
            print(f"Timeout waiting for batch {batch_id}")
            break

        try:
            res = requests.get(url, headers=header, timeout=30)
        except requests.RequestException as e:
            print(f"Polling error: {e}")
            time.sleep(5)
            continue

        if res.status_code != 200:
            print(f"Batch status check failed: {res.status_code}")
            break
            
        result = res.json().get("data", {}).get("extract_result", [])
        
        all_finished = True
        new_done = []
        
        for item in result:
            state = item.get("state")
            fname = item.get("file_name", "")
            
            if state == "done":
                if fname not in done_files:
                    done_files.add(fname)
                    new_done.append(item)
            elif state in ["failed", "error", "cancelled"]:
                if fname not in done_files:
                    print(f"File failed processing: {fname}, state: {state}")
                    done_files.add(fname)
            else:
                all_finished = False
        
        if new_done:
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                list(executor.map(download_one, new_done))

        if result and all_finished:
            print("All files processed (done or failed)")
            break
        
        print(f"Waiting for batch processing... ({len(done_files)}/{len(result)})", end='\r')
        time.sleep(5)
    print("")

def process_zip_file(zip_path: Path):
    parent = zip_path.parent
    stem = zip_path.stem
    img_out = parent / "images"
    
    # Determine target filename stem from metadata if available
    target_stem = stem
    meta_path = parent / f"{stem}_metadata.json"
    if meta_path.exists():
        try:
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta = json.load(f)
                title = meta.get('title')
                if title:
                    sanitized_title = _sanitize_dir_name(title)
                    # Truncate to avoid filesystem limits
                    if len(sanitized_title) > 150:
                        sanitized_title = sanitized_title[:150]
                    target_stem = sanitized_title
        except Exception:
            pass

    try:
        with zipfile.ZipFile(zip_path, 'r') as zf:
            for info in zf.infolist():
                if info.is_dir(): continue
                fn = info.filename.strip('/')
                if fn == 'full.md':
                    with zf.open(info) as src, open(parent / f"{target_stem}.md", 'wb') as dst:
                        shutil.copyfileobj(src, dst)
                elif fn == 'full.json':
                    with zf.open(info) as src, open(parent / f"{target_stem}.json", 'wb') as dst:
                        shutil.copyfileobj(src, dst)
                elif fn.startswith('images/'):
                    target = img_out / fn.replace('images/', '', 1)
                    target.parent.mkdir(parents=True, exist_ok=True)
                    with zf.open(info) as src, open(target, 'wb') as dst:
                        shutil.copyfileobj(src, dst)
    except Exception as e:
        print(f"Failed to process zip {zip_path}: {e}")

def replicate_files(file_dir: Path, output_dir: Path):
    for p in file_dir.rglob("*.pdf"):
        rel = p.relative_to(file_dir)
        md = output_dir / rel.with_suffix(".md")
        js = output_dir / rel.with_suffix(".json")
        if md.exists():
            # Logic to replicate if needed, simplified here as we process in place mostly
            pass

def _replicate_duplicates(duplicates_map: dict[str, str], output_dir: Path):
    for dup, pri in duplicates_map.items():
        pri_md = output_dir / Path(pri).with_suffix('.md')
        dup_md = output_dir / Path(dup).with_suffix('.md')
        if pri_md.exists():
            dup_md.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy(pri_md, dup_md)

def convert_pdfs_to_md(teacher: str, pdf_root: Path, md_root: Path, token: str):
    file_dir = pdf_root / teacher
    output_dir = md_root / teacher
    uniques, duplicates_map = _gather_candidates_and_duplicates(file_dir, output_dir)
    
    if not uniques:
        print("No new PDFs to convert.")
        return

    header = _build_header(token)
    batch_id = batch_upload(list(uniques.keys()), uniques, file_dir, header)
    if batch_id:
        print(f"Batch ID: {batch_id}")
        batch_retrieve(batch_id, uniques, file_dir, output_dir, header)
        for zip_file in output_dir.rglob("*.zip"):
            process_zip_file(zip_file)
        _replicate_duplicates(duplicates_map, output_dir)

# ==========================================
# 5. Main Logic
# ==========================================

def parse_papers_data(file_path: Path) -> Dict[str, List[str]]:
    data = {}
    current_teacher = None
    current_dois = []
    if not file_path.exists():
        print(f"File not found: {file_path}")
        return {}
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line.startswith('ðŸ‘¤'):
                if current_teacher: data[current_teacher] = current_dois
                current_teacher = line.replace('ðŸ‘¤', '').strip()
                current_dois = []
            elif line.startswith('DOI:'):
                doi = line.split(':', 1)[1].strip()
                if doi: current_dois.append(doi)
        if current_teacher: data[current_teacher] = current_dois
    return data

def process_doi_task(args_tuple):
    client, doi, main_dir, ref_dir, cited_dir, sample_size, years_window, workers_related, limit_ref, limit_cited, teacher = args_tuple
    print(f"  Downloading Main DOI: {doi}")
    try:
        item = process_one_by_doi(client, doi, main_dir, download=True, years_window=years_window)
        item['role'] = 'main'
        
        rid = item.get('record_id')
        if not rid: 
            save_progress(f"{teacher}|{doi}")
            return item

        # Parallel fetch of IDs
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as fetch_executor:
            # future_ref_ids = fetch_executor.submit(fetch_related_ids, client, rid, 'references', limit_ref)
            future_cited_ids = fetch_executor.submit(fetch_related_ids, client, rid, 'citations', limit_cited, doi)
            
            ref_ids = [] # future_ref_ids.result()
            cited_ids = future_cited_ids.result()

        # Sampling
        if sample_size and len(ref_ids) > 0:
            ref_ids = random.sample(ref_ids, min(len(ref_ids), sample_size))
        if sample_size and len(cited_ids) > 0:
            cited_ids = random.sample(cited_ids, min(len(cited_ids), sample_size))

        # Prepare download tasks
        download_tasks = []
        for r_id in ref_ids:
            download_tasks.append({'id': r_id, 'dir': ref_dir, 'role': 'reference'})
        for c_id in cited_ids:
            download_tasks.append({'id': c_id, 'dir': cited_dir, 'role': 'citation'})

        def download_related(task):
            tid = task['id']
            tdir = task['dir']
            trole = task['role']
            print(f"    Downloading {trole.capitalize()}: {tid}")
            try:
                if '/' in tid:
                    # Try InspireHEP first
                    try:
                        t_item = process_one_by_doi(client, tid, tdir, download=True, years_window=years_window)
                        t_item['role'] = trole
                    except Exception as e_inspire:
                        # Fallback to generic downloader if available
                        if doi_downloader:
                            print(f"    InspireHEP failed for {tid}, trying generic downloader...")
                            # Use doi_downloader to download PDF
                            # Note: doi_downloader saves to its own structure, we might need to adapt or move file
                            # For simplicity, we call download_and_process_doi but we need to handle the output path
                            # Or better, use DownloadFileByUrl directly if we can get a URL
                            
                            # Let's try to use doi_downloader's logic to get PDF URL and download to our tdir
                            official_title = doi_downloader.get_official_title_from_doi(tid)
                            if official_title:
                                # Try to get PDF URL
                                pdf_url = doi_downloader.get_pdf_from_publisher(tid)
                                if not pdf_url:
                                    pdf_url = doi_downloader.get_pdf_from_unpaywall(tid, None)
                                if not pdf_url:
                                    try:
                                        pdf_url = doi_downloader.GetDownloadUrl(tid)
                                    except Exception: pass
                                
                                if pdf_url:
                                    base_name = _sanitize_dir_name(tid)
                                    pdf_path = tdir / f"{base_name}.pdf"
                                    client.download_file(pdf_url, str(pdf_path))
                                    
                                    # Create minimal metadata
                                    meta_path = tdir / f"{base_name}_metadata.json"
                                    meta_simple = {
                                        "doi": tid,
                                        "title": official_title,
                                        "authors": [], # We could fetch from CrossRef if needed
                                        "role": trole
                                    }
                                    # Try to enrich with CrossRef
                                    try:
                                        cr_meta = doi_downloader.get_crossref_metadata(tid)
                                        if cr_meta:
                                            meta_simple["title"] = cr_meta.get('title', [official_title])[0]
                                            meta_simple["authors"] = doi_downloader._extract_authors_from_meta(cr_meta)
                                            y, m = doi_downloader._extract_pub_year_month(cr_meta)
                                            if y: meta_simple["publication_date"] = f"{y}-{m}" if m else str(y)
                                    except Exception: pass
                                    
                                    with open(meta_path, 'w', encoding='utf-8') as f:
                                        json.dump(meta_simple, f, ensure_ascii=False, indent=2)
                                else:
                                    print(f"    Generic download failed (no URL) for {tid}")
                            else:
                                print(f"    Generic download failed (no title) for {tid}")
                        else:
                            raise e_inspire

                else:
                    t_item = process_one_by_record_id_using_url(client, tid, tdir, download=True, years_window=years_window)
                    t_item['role'] = trole
            except Exception as e:
                print(f"    Failed {trole.capitalize()} {tid}: {e}")

        # Execute downloads in parallel
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers_related) as executor:
            list(executor.map(download_related, download_tasks))
            
        save_progress(f"{teacher}|{doi}")
        return item

    except Exception as e:
        print(f"  Failed Main DOI {doi}: {e}")
        return None

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--papers-data', default=str(PROJ_ROOT / 'inspirehep_source/pre-process/papers_data.txt'))
    parser.add_argument('--token', default=os.getenv('MINERU_TOKEN'))
    args = parser.parse_args()

    if not args.token:
        print("Warning: MINERU_TOKEN is missing. PDF to MD conversion will be skipped.")
    
    config = load_config()
    sample_threshold, sample_size = get_sampling_cfg(config)
    years_window = get_young_author_years(config)
    workers_main, workers_related = get_worker_cfg(config)
    limit_ref, limit_cited = get_limit_cfg(config)
    
    papers_data = parse_papers_data(Path(args.papers_data))
    processed_records = load_progress()
    
    data_root = PROJ_ROOT / 'data'
    
    for teacher, dois in papers_data.items():
        print(f"\nProcessing Teacher: {teacher}")
        teacher_dir = data_root / teacher
        main_dir = teacher_dir / 'main'
        ref_dir = teacher_dir / 'ref1'
        cited_dir = teacher_dir / 'cited'
        
        # Use a new client per thread if needed, but requests.Session is thread-safe.
        # However, to avoid sharing connection pool limits too aggressively, we can pass the same client.
        client = InspireHEPClient()

        # Prepare tasks
        tasks = []
        for doi in dois:
            if f"{teacher}|{doi}" in processed_records:
                print(f"  [Skipped] {doi} (Already processed)")
                continue
            tasks.append((client, doi, main_dir, ref_dir, cited_dir, sample_size, years_window, workers_related, limit_ref, limit_cited, teacher))
        
        if tasks:
            with concurrent.futures.ThreadPoolExecutor(max_workers=workers_main) as executor:
                results = list(executor.map(process_doi_task, tasks))
        else:
            print(f"  All DOIs for {teacher} are already processed.")

        # 3. Convert to MD (Batch process for the teacher)
        if args.token:
            print(f"  Converting PDFs to MD for {teacher}...")
            convert_pdfs_to_md(teacher, data_root, data_root, args.token)
        else:
            print(f"  Skipping PDF to MD conversion for {teacher} (no token).")
        
        # 4. Generate Metadata JSON
        all_items = []
        for subdir in [main_dir, ref_dir, cited_dir]:
            if not subdir.exists(): continue
            for meta_file in subdir.glob('*_metadata.json'):
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta = json.load(f)
                        if subdir.name == 'main': meta['role'] = 'main'
                        elif subdir.name == 'ref1': meta['role'] = 'reference'
                        elif subdir.name == 'cited': meta['role'] = 'citation'
                        
                        item = {
                            "title": meta.get("title"),
                            "doi": meta.get("doi"),
                            "authors": meta.get("authors"),
                            "published": parse_year_month(meta.get("publication_date")),
                            "role": meta.get("role"),
                            "record_id": meta.get("record_id"),
                            "inspire_url": meta.get("inspire_url"),
                            "citations_count": meta.get("citations", 0)
                        }
                        pub_year = item.get("published", {}).get("year")
                        ya = compute_young_author_fields(client, item.get("authors", []), pub_year, years_window)
                        item.update(ya)
                        all_items.append(item)
                except Exception: pass
        
        with open(teacher_dir / 'metadata_items.json', 'w', encoding='utf-8') as f:
            json.dump({"items": all_items}, f, ensure_ascii=False, indent=2)
        print(f"  Metadata saved to {teacher_dir / 'metadata_items.json'}")

if __name__ == '__main__':
    main()
